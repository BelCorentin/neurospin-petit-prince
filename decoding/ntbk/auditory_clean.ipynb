{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c94f2ac-eafd-4ef3-b240-048cf7e432ae",
   "metadata": {},
   "source": [
    "# Clean version of the auditory debug ntbk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a4220",
   "metadata": {},
   "source": [
    "# Testing updated JRA match list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765c85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import logging\n",
    "import mne_bids\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from jra_utils import approx_match_samples\n",
    "\n",
    "from pathlib import Path\n",
    "# Set the logger level to WARNING to reduce verbosity\n",
    "logger = logging.getLogger('mne')\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "#path = Path(\"/home/co/data/neuralset/LPP_copy/pallierlisten2023/download\")\n",
    "path = Path(\"/media/co/T7/workspace-LPP/data/MEG/LPP/PallierListen2023/download\")\n",
    "\n",
    "def testing(subject, run_id, n_missing=5, abs_tol=10):\n",
    "    task = 'listen'\n",
    "    bids_path = mne_bids.BIDSPath(\n",
    "        subject=subject,\n",
    "        session=\"01\",\n",
    "        task=task,\n",
    "        datatype=\"meg\",\n",
    "        root=path,\n",
    "        run=run_id,\n",
    "    )\n",
    "\n",
    "    raw = mne_bids.read_raw_bids(bids_path)\n",
    "    # triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "    # triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "\n",
    "    triggers = mne.find_stim_steps(raw, stim_channel=\"STI008\")\n",
    "    # Offsets\n",
    "    triggers = triggers[triggers[:, 2] == 0]\n",
    "\n",
    "    # Generate event_file path\n",
    "    event_file = path / f\"sub-{bids_path.subject}\"\n",
    "    event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "    event_file = event_file / \"meg\"\n",
    "    event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "    event_file += f\"_ses-{bids_path.session}\"\n",
    "    event_file += f\"_task-{bids_path.task}\"\n",
    "    event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "    assert Path(event_file).exists()\n",
    "\n",
    "    meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "\n",
    "    meta[\"word\"] = meta[\"trial_type\"].apply(\n",
    "            lambda x: eval(x)[\"word\"] if type(eval(x)) == dict else np.nan)\n",
    "\n",
    "    # Remove the empty words:\n",
    "\n",
    "    meta.loc[meta['word'] == ' ', 'word'] = None\n",
    "\n",
    "    # Drop the rows containing NaN values in the text column\n",
    "    meta = meta.dropna(subset=['word'])\n",
    "\n",
    "    meta['start'] = meta.onset\n",
    "\n",
    "    words = meta\n",
    "\n",
    "    events_df = meta\n",
    "    # Triggers from the MEG: :, 2 is the trigger value: 1 is a word, 128 is the start / end\n",
    "    # word_triggers = triggers[triggers[:, 2] > 1]\n",
    "    word_triggers = triggers\n",
    "    try:\n",
    "        i, j = approx_match_samples((words.start * 1000).tolist()[:], word_triggers[:, 0], abs_tol=abs_tol, max_missing=n_missing)\n",
    "        return f\"Worked for {subject} {run_id}, {len(i) / words.start.shape[0]}\"\n",
    "    except Exception as e:\n",
    "        # When it doesn't work, print the index where it broke, and create a new file from it\n",
    "        # To add to the tests\n",
    "        print(f\"Failed for {subject} {run_id}\")\n",
    "        print(f\"Error message: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80049e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 21 06 with 5 missing and 30 abs_tol\n",
      "Failed for 21 06\n",
      "Error message: Failed to match after indices [1001, 1000] (values {[s.last_value for s in seqs]}, {first_match=})\n",
      "(follows:\n",
      " [   0.  190.  540.  600.  950. 1050. 2020. 2180. 2580. 4970.]\n",
      " [   0  141  332  683  743 1093 1193 2163 2324 2725](before:\n",
      " [-3370. -2980. -2850. -1780. -1680.  -970.  -850.  -780.  -600.  -140.]\n",
      " [-3383 -3343 -2951 -2821 -1644 -1544  -832  -712  -642  -461]\n",
      "Failed for 21 06\n",
      "argument of type 'NoneType' is not iterable\n",
      "Testing for 21 07 with 5 missing and 10 abs_tol\n",
      "Worked for 21 07, 1.0\n",
      "Testing for 21 08 with 5 missing and 30 abs_tol\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m abs_tol, max_missing \u001b[38;5;241m=\u001b[39m TOL_MISSING_DICT\u001b[38;5;241m.\u001b[39mget((\u001b[38;5;28mint\u001b[39m(subject), run), (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_missing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m missing and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabs_tol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m abs_tol\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtesting\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_missing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabs_tol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mabs_tol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorked\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "Cell \u001b[0;32mIn [1], line 32\u001b[0m, in \u001b[0;36mtesting\u001b[0;34m(subject, run_id, n_missing, abs_tol)\u001b[0m\n\u001b[1;32m     28\u001b[0m raw \u001b[38;5;241m=\u001b[39m mne_bids\u001b[38;5;241m.\u001b[39mread_raw_bids(bids_path)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m triggers \u001b[38;5;241m=\u001b[39m \u001b[43mmne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_stim_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstim_channel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSTI008\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Offsets\u001b[39;00m\n\u001b[1;32m     34\u001b[0m triggers \u001b[38;5;241m=\u001b[39m triggers[triggers[:, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/meg/lib/python3.9/site-packages/mne/event.py:430\u001b[0m, in \u001b[0;36mfind_stim_steps\u001b[0;34m(raw, pad_start, pad_stop, merge, stim_channel)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(picks) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo stim channel found to extract event triggers.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 430\u001b[0m data, _ \u001b[38;5;241m=\u001b[39m \u001b[43mraw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(data \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    432\u001b[0m     warn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrigger channel contains negative values, using absolute value.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/meg/lib/python3.9/site-packages/mne/io/base.py:747\u001b[0m, in \u001b[0;36mBaseRaw.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;124;03m\"\"\"Get raw data and times.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \n\u001b[1;32m    714\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/meg/lib/python3.9/site-packages/mne/io/base.py:754\u001b[0m, in \u001b[0;36mBaseRaw._getitem\u001b[0;34m(self, item, return_times)\u001b[0m\n\u001b[1;32m    752\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[sel, start:stop]\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 754\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_segment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mprojector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_projector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_times:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;66;03m# Rather than compute the entire thing just compute the subset\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;66;03m# times = self.times[start:stop]\u001b[39;00m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;66;03m# stop can be None here so don't use it directly\u001b[39;00m\n\u001b[1;32m    761\u001b[0m     times \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(start, start \u001b[38;5;241m+\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[0;32m<decorator-gen-209>:12\u001b[0m, in \u001b[0;36m_read_segment\u001b[0;34m(self, start, stop, sel, data_buffer, projector, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/meg/lib/python3.9/site-packages/mne/io/base.py:390\u001b[0m, in \u001b[0;36mBaseRaw._read_segment\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# reindex back to original file\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     orig_idx \u001b[38;5;241m=\u001b[39m _convert_slice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_picks[fi][need_idx])\n\u001b[0;32m--> 390\u001b[0m     \u001b[43m_ReadSegmentFileProtector\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_segment_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis_sl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstop_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n_read\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/meg/lib/python3.9/site-packages/mne/io/base.py:2131\u001b[0m, in \u001b[0;36m_ReadSegmentFileProtector._read_segment_file\u001b[0;34m(self, data, idx, fi, start, stop, cals, mult)\u001b[0m\n\u001b[1;32m   2130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_segment_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, idx, fi, start, stop, cals, mult):\n\u001b[0;32m-> 2131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__raw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_segment_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/meg/lib/python3.9/site-packages/mne/io/fiff/raw.py:368\u001b[0m, in \u001b[0;36mRaw._read_segment_file\u001b[0;34m(self, data, idx, fi, start, stop, cals, mult)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# only read data if it exists\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     one \u001b[38;5;241m=\u001b[39m \u001b[43mread_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnsamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnchan\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mrlims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfirst_pick\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_pick\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m         one\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m (picksamp, nchan)\n",
      "File \u001b[0;32m~/miniconda3/envs/meg/lib/python3.9/site-packages/mne/io/tag.py:476\u001b[0m, in \u001b[0;36mread_tag\u001b[0;34m(fid, pos, shape, rlims)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    475\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnimplemented tag data type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m tag\u001b[38;5;241m.\u001b[39mtype)\n\u001b[0;32m--> 476\u001b[0m         tag\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrlims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;241m!=\u001b[39m FIFF\u001b[38;5;241m.\u001b[39mFIFFV_NEXT_SEQ:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# f.seek(tag.next,0)\u001b[39;00m\n\u001b[1;32m    479\u001b[0m     fid\u001b[38;5;241m.\u001b[39mseek(tag\u001b[38;5;241m.\u001b[39mnext, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# XXX : fix? pb when tag.next < 0\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/meg/lib/python3.9/site-packages/mne/io/tag.py:265\u001b[0m, in \u001b[0;36m_read_simple\u001b[0;34m(fid, tag, shape, rlims, dtype)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_simple\u001b[39m(fid, tag, shape, rlims, dtype):\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;124;03m\"\"\"Read simple datatypes from tag (typically used with partial).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_frombuffer_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrlims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrlims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/meg/lib/python3.9/site-packages/mne/io/tag.py:99\u001b[0m, in \u001b[0;36m_frombuffer_rows\u001b[0;34m(fid, tag_size, dtype, shape, rlims)\u001b[0m\n\u001b[1;32m     97\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(start_skip, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Do the reading\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(\u001b[43mfid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Move the pointer ahead to the end of the tag\u001b[39;00m\n\u001b[1;32m    101\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(end_pos)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from jra_utils import TOL_MISSING_DICT\n",
    "\n",
    "total_subjects = 24\n",
    "worked = 0\n",
    "n_missing = 5\n",
    "for subject in range(21,total_subjects+1):\n",
    "    for run in range(6,10):\n",
    "        run_id = '0' + str(run)\n",
    "        subject = str(subject)\n",
    "\n",
    "        # Get the TOL AND MISSING\n",
    "        abs_tol, max_missing = TOL_MISSING_DICT.get((int(subject), run), (10, 5))\n",
    "        print(f\"Testing for {subject} {run_id} with {max_missing} missing and {abs_tol} abs_tol\")\n",
    "        result = testing(subject, run_id, n_missing=max_missing, abs_tol=abs_tol)\n",
    "        try:\n",
    "            if \"Worked\" in result:\n",
    "                print(f\"{result}\")\n",
    "                worked += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for {subject} {run_id}\")\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "print(f\"Worked for {worked} out of {total_subjects * 9} subjects: {worked / total_subjects * 100 * 9} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4a2a4",
   "metadata": {},
   "source": [
    "# Testing all the dysfunctional ones\n",
    "\n",
    "They all work (21 & 23 are fked upÂ§)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29248e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 9 6 with 5 missing and 30 abs_tol\n",
      "Worked for 9 08, 1.0\n",
      "Testing for 10 6 with 5 missing and 30 abs_tol\n",
      "Worked for 10 08, 1.0\n",
      "Testing for 12 5 with 5 missing and 30 abs_tol\n",
      "Worked for 12 08, 1.0\n",
      "Testing for 13 3 with 5 missing and 30 abs_tol\n",
      "Worked for 13 08, 1.0\n",
      "Testing for 13 7 with 5 missing and 30 abs_tol\n",
      "Worked for 13 08, 1.0\n",
      "Testing for 14 9 with 5 missing and 30 abs_tol\n",
      "Worked for 14 08, 1.0\n",
      "Testing for 21 6 with 5 missing and 30 abs_tol\n",
      "Failed for 21 08\n",
      "Error message: Failed to match after indices [770, 770] (values {[s.last_value for s in seqs]}, {first_match=})\n",
      "(follows:\n",
      " [   0. 3870. 4070. 4690. 4940. 5040. 5350. 6450. 6590. 6750.]\n",
      " [   0 2289 2489 2699 3170 3290 3360 3580 3880 4060](before:\n",
      " [-4180. -3710. -3150. -2160. -1970. -1410. -1270.  -600.  -490.  -150.]\n",
      " [-4180 -3710 -3150 -2160 -1970 -1410 -1270  -600  -490  -150]\n",
      "None\n",
      "Testing for 21 8 with 5 missing and 30 abs_tol\n",
      "Failed for 21 08\n",
      "Error message: Failed to match after indices [770, 770] (values {[s.last_value for s in seqs]}, {first_match=})\n",
      "(follows:\n",
      " [   0. 3870. 4070. 4690. 4940. 5040. 5350. 6450. 6590. 6750.]\n",
      " [   0 2289 2489 2699 3170 3290 3360 3580 3880 4060](before:\n",
      " [-4180. -3710. -3150. -2160. -1970. -1410. -1270.  -600.  -490.  -150.]\n",
      " [-4180 -3710 -3150 -2160 -1970 -1410 -1270  -600  -490  -150]\n",
      "None\n",
      "Testing for 22 4 with 5 missing and 30 abs_tol\n",
      "Worked for 22 08, 1.0\n",
      "Testing for 33 2 with 5 missing and 30 abs_tol\n",
      "Worked for 33 08, 1.0\n",
      "Testing for 39 5 with 5 missing and 30 abs_tol\n",
      "Worked for 39 08, 1.0\n",
      "Testing for 40 2 with 5 missing and 30 abs_tol\n",
      "Worked for 40 08, 1.0\n",
      "Testing for 41 1 with 5 missing and 30 abs_tol\n",
      "Worked for 41 08, 1.0\n",
      "Testing for 43 4 with 5 missing and 30 abs_tol\n",
      "Worked for 43 08, 1.0\n",
      "Testing for 43 5 with 5 missing and 30 abs_tol\n",
      "Worked for 43 08, 1.0\n",
      "Testing for 44 9 with 5 missing and 30 abs_tol\n",
      "Worked for 44 08, 1.0\n",
      "Testing for 24 2 with 20 missing and 10 abs_tol\n",
      "Worked for 24 08, 1.0\n"
     ]
    }
   ],
   "source": [
    "for subject, run in TOL_MISSING_DICT:\n",
    "    abs_tol, max_missing = TOL_MISSING_DICT[(subject, run)]\n",
    "    print(f\"Testing for {subject} {run} with {max_missing} missing and {abs_tol} abs_tol\")\n",
    "    rst = testing(str(subject), str(run_id), n_missing=max_missing, abs_tol=abs_tol)\n",
    "    print(rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f4f5f",
   "metadata": {},
   "source": [
    "# Plotting JRA match list results along recorded audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff34c59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /media/co/T7/workspace-LPP/data/MEG/LPP/PallierListen2023/download/sub-5/ses-01/meg/sub-5_ses-01_task-listen_run-01_meg.fif...\n",
      "    Read a total of 13 projection items:\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "        generated with dossp-2.1 (1 x 306)  idle\n",
      "    Range : 25000 ... 642999 =     25.000 ...   642.999 secs\n",
      "Ready.\n",
      "Reading events from /media/co/T7/workspace-LPP/data/MEG/LPP/PallierListen2023/download/sub-5/ses-01/meg/sub-5_ses-01_task-listen_run-01_events.tsv.\n",
      "Reading channel info from /media/co/T7/workspace-LPP/data/MEG/LPP/PallierListen2023/download/sub-5/ses-01/meg/sub-5_ses-01_task-listen_run-01_channels.tsv.\n",
      "Using 4 HPI coils: 293 307 314 321 Hz\n",
      "Not fully anonymizing info - keeping his_id, sex, and hand info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-8a3135e2af72>:22: RuntimeWarning: This file contains raw Internal Active Shielding data. It may be distorted. Elekta recommends it be run through MaxFilter to produce reliable results. Consider closing the file and running MaxFilter on the data.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "<ipython-input-8-8a3135e2af72>:22: RuntimeWarning: The unit for channel(s) STI001, STI002, STI003, STI004, STI005, STI006, STI007, STI008, STI009, STI010, STI011, STI012, STI013, STI014, STI015, STI016, STI101, STI201, STI301 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "import mne_bids\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "run_id = '01'\n",
    "subject = '5'\n",
    "\n",
    "path = Path(\"/media/co/T7/workspace-LPP/data/MEG/LPP/PallierListen2023/download\")\n",
    "\n",
    "task = 'listen'\n",
    "bids_path = mne_bids.BIDSPath(\n",
    "    subject=subject,\n",
    "    session=\"01\",\n",
    "    task=task,\n",
    "    datatype=\"meg\",\n",
    "    root=path,\n",
    "    run=run_id,\n",
    ")\n",
    "\n",
    "raw = mne_bids.read_raw_bids(bids_path)\n",
    "\n",
    "triggers = mne.find_stim_steps(raw, stim_channel=\"STI008\")\n",
    "# Offsets\n",
    "triggers = triggers[triggers[:, 2] == 0]\n",
    "\n",
    "# Generate event_file path\n",
    "event_file = path / f\"sub-{bids_path.subject}\"\n",
    "event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "event_file = event_file / \"meg\"\n",
    "event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "event_file += f\"_ses-{bids_path.session}\"\n",
    "event_file += f\"_task-{bids_path.task}\"\n",
    "event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "assert Path(event_file).exists()\n",
    "\n",
    "meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "\n",
    "meta[\"word\"] = meta[\"trial_type\"].apply(\n",
    "        lambda x: eval(x)[\"word\"] if type(eval(x)) == dict else np.nan)\n",
    "\n",
    "# Remove the empty words:\n",
    "\n",
    "meta.loc[meta['word'] == ' ', 'word'] = None\n",
    "\n",
    "# Drop the rows containing NaN values in the text column\n",
    "meta = meta.dropna(subset=['word'])\n",
    "\n",
    "meta['start'] = meta.onset\n",
    "\n",
    "words = meta\n",
    "\n",
    "events_df = meta\n",
    "\n",
    "word_triggers = triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67761360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Removing projector <Projection | generated with dossp-2.1, active : False, n_channels : 306>\n",
      "Matching until 638360 (610200.0 on second seq, offset=28160.0\n",
      "0\n",
      "28160.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "\n",
    "import jra_utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seq0 = word_triggers[:, 0]\n",
    "seq1 = (words.start * 1000).tolist()\n",
    "\n",
    "data_stim = raw.copy().pick_channels(['STI008']).get_data()\n",
    "data_stim = data_stim / 10\n",
    "to_add = np.zeros(raw.first_samp)\n",
    "data_stim = np.concatenate([to_add, data_stim[0]])\n",
    "\n",
    "wav = raw.pick_channels(['MISC005']).get_data()[0]\n",
    "matches = utils.plot_matches(seq0, seq1, wav)\n",
    "\n",
    "plt.plot(data_stim)\n",
    "# Label this last plot as the MEG data\n",
    "plt.legend(['Mne Processed STI008', 'Ground truth', 'Raw STI008 MEG data'])\n",
    "\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b86ae7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "data_wav = raw.pick_channels(['MISC004']).get_data()\n",
    "plt.plot(data_wav[0])\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf1e6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wav)\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9946446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wav = raw.pick_channels(['MISC004']).get_data()\n",
    "br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac7c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to a 1D array\n",
    "audio_data = data_wav.flatten()\n",
    "\n",
    "# Assuming the audio is sampled at 1000 Hz\n",
    "sample_rate = 1000\n",
    "time = np.linspace(0, len(audio_data) / sample_rate, len(audio_data))\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(time, audio_data)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Waveform')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5990ae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels marked as bad:\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "data_wav = raw.pick_channels(['MISC005']).plot()\n",
    "# raw.pick_types(stim=True, misc=True).plot(block=True, start=0, duration=30., precompute='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "\n",
    "plot_matches(seq0, seq1)\n",
    "plt.plot(data_stim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put into a csv file both arrays that were used for matching\n",
    "np.save(f'saves/word_triggers_{subject}_{run_id}.npy', word_triggers[:, 0])\n",
    "np.save(f'saves/words_start_{subject}_{run_id}.npy', (words.start * 1000).tolist())\n",
    "# save datastim\n",
    "np.save(f'saves/data_stim_{subject}_{run_id}.npy', data_stim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw data\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "\n",
    "raw.pick_types(stim=True, misc=True).plot(block=True, start=0, duration=10., precompute='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4bf700",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.first_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot((words.start * 1000).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(word_triggers[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83506340-7204-42b5-814b-f4152f3aba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triggers(subject, run_id):\n",
    "    task = 'listen'\n",
    "    bids_path = mne_bids.BIDSPath(\n",
    "        subject=subject,\n",
    "        session=\"01\",\n",
    "        task=task,\n",
    "        datatype=\"meg\",\n",
    "        root=path,\n",
    "        run=run_id,\n",
    "    )\n",
    "\n",
    "    raw = mne_bids.read_raw_bids(bids_path)\n",
    "    # triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "    triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "\n",
    "    # Generate event_file path\n",
    "    event_file = path / f\"sub-{bids_path.subject}\"\n",
    "    event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "    event_file = event_file / \"meg\"\n",
    "    event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "    event_file += f\"_ses-{bids_path.session}\"\n",
    "    event_file += f\"_task-{bids_path.task}\"\n",
    "    event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "    assert Path(event_file).exists()\n",
    "\n",
    "    meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "\n",
    "    meta[\"word\"] = meta[\"trial_type\"].apply(\n",
    "            lambda x: eval(x)[\"word\"] if type(eval(x)) == dict else np.nan)\n",
    "\n",
    "    # Remove the empty words:\n",
    "\n",
    "    meta.loc[meta['word'] == ' ', 'word'] = None\n",
    "\n",
    "    # Drop the rows containing NaN values in the text column\n",
    "    meta = meta.dropna(subset=['word'])\n",
    "\n",
    "    meta['start'] = meta.onset\n",
    "\n",
    "    words = meta\n",
    "\n",
    "    events_df = meta\n",
    "    return triggers[triggers[:, 2] > 1], meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58602e-31fa-4ced-afc9-08965438a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import logging\n",
    "import mne_bids\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from jr_utils import Align, resample_safe\n",
    "from utils import match_list\n",
    "\n",
    "from pathlib import Path\n",
    "# Set the logger level to WARNING to reduce verbosity\n",
    "logger = logging.getLogger('mne')\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# General epoching function\n",
    "alignment = True\n",
    "\n",
    "run_id = '01'\n",
    "subject = '4'\n",
    "\n",
    "#path = Path(\"/home/co/data/neuralset/LPP_copy/pallierlisten2023/download\")\n",
    "path = Path(\"/media/co/T7/workspace-LPP/data/MEG/LPP/PallierListen2023/download\")\n",
    "\n",
    "def testing(subject, run_id):\n",
    "    task = 'listen'\n",
    "    bids_path = mne_bids.BIDSPath(\n",
    "        subject=subject,\n",
    "        session=\"01\",\n",
    "        task=task,\n",
    "        datatype=\"meg\",\n",
    "        root=path,\n",
    "        run=run_id,\n",
    "    )\n",
    "\n",
    "    raw = mne_bids.read_raw_bids(bids_path)\n",
    "    # triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "    triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "\n",
    "    # Generate event_file path\n",
    "    event_file = path / f\"sub-{bids_path.subject}\"\n",
    "    event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "    event_file = event_file / \"meg\"\n",
    "    event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "    event_file += f\"_ses-{bids_path.session}\"\n",
    "    event_file += f\"_task-{bids_path.task}\"\n",
    "    event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "    assert Path(event_file).exists()\n",
    "\n",
    "    meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "\n",
    "    meta[\"word\"] = meta[\"trial_type\"].apply(\n",
    "            lambda x: eval(x)[\"word\"] if type(eval(x)) == dict else np.nan)\n",
    "\n",
    "    # Remove the empty words:\n",
    "\n",
    "    meta.loc[meta['word'] == ' ', 'word'] = None\n",
    "\n",
    "    # Drop the rows containing NaN values in the text column\n",
    "    meta = meta.dropna(subset=['word'])\n",
    "\n",
    "    meta['start'] = meta.onset\n",
    "\n",
    "    words = meta\n",
    "\n",
    "    events_df = meta\n",
    "    # Triggers from the MEG: :, 2 is the trigger value: 1 is a word, 128 is the start / end\n",
    "    word_triggers = triggers[triggers[:, 2] > 1]\n",
    "    try:\n",
    "        i, j = approx_match_samples((words.start * 1000).tolist()[:], word_triggers[:, 0], tol=50, max_missing=3)\n",
    "        return f\"Worked for {subject} {run_id}, {len(i) / words.start.shape[0]}\"\n",
    "    except Exception as e:\n",
    "        # When it doesn't work, print the index where it broke, and create a new file from it\n",
    "        # To add to the tests\n",
    "        print(f\"Failed for {subject} {run_id}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "\n",
    "for subject in range(5,6):\n",
    "    for run in range(1,10):\n",
    "        run_id = '0' + str(run)\n",
    "        subject = str(subject)\n",
    "        print(testing(subject, run_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0671febf-419a-407f-abea-6522675baa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "for subject in range(5,6):\n",
    "    for run in range(1,10):\n",
    "        run_id = '0' + str(run)\n",
    "        subject = str(subject)\n",
    "        print(testing(subject, run_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10970062-2161-4898-90bb-6a0c369f3f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7ef91-89b1-45bd-b6d2-2e99f8e15156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "for subject in range(21,25):\n",
    "    for run in range(6,9):\n",
    "        run_id = '0' + str(run)\n",
    "        subject = str(subject)\n",
    "        print(testing(subject, run_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54dc4f-29fc-4a2a-8369-8393dc32a071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i, j = approx_match_samples((words.start * 1000).tolist()[:], word_triggers[:, 0], tol=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe8d73-c900-4681-bfb9-0923e22b2ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diff((words.start * 1000))[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9a3d7e-9526-451a-b9b9-45dd07225bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.to_csv(, \"test.csv\")\n",
    "words_times = words.start * 1000\n",
    "np.savetxt('words_times.csv', words_times, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05694637-2185-4ee9-9f1e-ec34b43fec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import mne\n",
    "    import logging\n",
    "    import mne_bids\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.io import wavfile\n",
    "\n",
    "    from jr_utils import Align, resample_safe\n",
    "    from utils import match_list\n",
    "\n",
    "    from pathlib import Path\n",
    "    # Set the logger level to WARNING to reduce verbosity\n",
    "    logger = logging.getLogger('mne')\n",
    "    logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "    # General epoching function\n",
    "    alignment = True\n",
    "\n",
    "    run_id = '01'\n",
    "    subject = '3'\n",
    "\n",
    "    path = Path(\"/home/co/data/neuralset/LPP_copy/pallierlisten2023/download\")\n",
    "\n",
    "    task = 'listen'\n",
    "    bids_path = mne_bids.BIDSPath(\n",
    "        subject=subject,\n",
    "        session=\"01\",\n",
    "        task=task,\n",
    "        datatype=\"meg\",\n",
    "        root=path,\n",
    "        run=run_id,\n",
    "    )\n",
    "\n",
    "    raw = mne_bids.read_raw_bids(bids_path)\n",
    "    # triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "    triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "\n",
    "    # Generate event_file path\n",
    "    event_file = path / f\"sub-{bids_path.subject}\"\n",
    "    event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "    event_file = event_file / \"meg\"\n",
    "    event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "    event_file += f\"_ses-{bids_path.session}\"\n",
    "    event_file += f\"_task-{bids_path.task}\"\n",
    "    event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "    assert Path(event_file).exists()\n",
    "\n",
    "    meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "\n",
    "    meta[\"word\"] = meta[\"trial_type\"].apply(\n",
    "            lambda x: eval(x)[\"word\"] if type(eval(x)) == dict else np.nan)\n",
    "\n",
    "    # Remove the empty words:\n",
    "\n",
    "    meta.loc[meta['word'] == ' ', 'word'] = None\n",
    "\n",
    "    # Drop the rows containing NaN values in the text column\n",
    "    meta = meta.dropna(subset=['word'])\n",
    "\n",
    "    meta['start'] = meta.onset\n",
    "\n",
    "    words = meta\n",
    "\n",
    "    events_df = meta\n",
    "    # Triggers from the MEG: :, 2 is the trigger value: 1 is a word, 128 is the start / end\n",
    "    word_triggers = triggers[triggers[:, 2] > 1]\n",
    "\n",
    "    # Testing the JR alignment\n",
    "\n",
    "    if alignment:\n",
    "        starts = mne.find_events(raw, output='step', shortest_event=1)[:, 0]\n",
    "        meg_times = np.copy(raw.times)\n",
    "        meg_triggers = np.zeros_like(meg_times)\n",
    "        meg_triggers[starts - raw.first_samp] = 1\n",
    "\n",
    "        # read wav\n",
    "        path_wav = path / 'sourcedata/stimuli/audio/ch1-3.wav'\n",
    "        wav_freq, wav = wavfile.read(path_wav)\n",
    "        wav = wav[:, 1] / wav.max()\n",
    "        # -- decim to get down to meg frequency\n",
    "        wav_triggers = resample_safe(wav, int(len(wav)/wav_freq*raw.info['sfreq']))\n",
    "\n",
    "        # -- get events\n",
    "        wav_times = np.cumsum(np.ones(len(wav_triggers)))/wav_freq\n",
    "        # fit alignment\n",
    "        align = Align(freq=raw.info['sfreq'], decim=100)\n",
    "        inds = np.where(np.diff(wav_triggers) > 0)[0] + 1\n",
    "        wav_spikes = 0 * wav_triggers\n",
    "        wav_spikes[inds] = 1\n",
    "        n = min(len(wav_spikes), len(meg_triggers))\n",
    "        align.fit(wav_spikes[:n], meg_triggers[:n])\n",
    "        out = align.predict(wav_spikes)\n",
    "\n",
    "    # Now that we have the out object, which represents the shifted triggers in the corrected frequency space\n",
    "    # We can finally realign it with the metdata starts, after diving by sfreq.\n",
    "\n",
    "    # Transform the current out object to make it like the triggers one, in order to be able \n",
    "    # To match on the timing diffs\n",
    "\n",
    "    # Handle the fact that the triggers are multiple values eg. [0 0 0 1 1 1 0 0 0 0 0 0 1 1 1  0 0 0]\n",
    "    #unique_vals, counts = np.unique(wav_triggers, return_counts=True)\n",
    "    #(array([0.        , 0.91555528]), array([602472,  11045]))\n",
    "\n",
    "    trigger_value = np.unique(out)[1]\n",
    "    aligned_trigger_indices = np.where(np.diff(out) == trigger_value)[0] + 1\n",
    "\n",
    "\n",
    "    # Matching triggers and metadata\n",
    "    decimals = 2\n",
    "    triggers_delta = np.round(np.diff(word_triggers[:, 0] / raw.info[\"sfreq\"]), decimals=decimals)  # type: ignore\n",
    "    events_delta = np.round(np.diff(words.start.values), decimals=decimals)  # type: ignore\n",
    "    i, j = match_list(triggers_delta, events_delta)\n",
    "\n",
    "    print(f\"Found {len(i)/len(words)} of the words in the triggers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbc2f3-5b97-4497-b2d2-038585319af5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Grid testing\n",
    "\n",
    "Testing the timings & nb of triggers in the original TextGrid file that was used to generate metadata / triggers in the wav file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16d48c-dcd8-4e47-ae98-9b8123f0a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textgrids as tg\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "files = [f'/home/co/tmp/testing_code/{i}.TextGrid' for i in range(1,4)]\n",
    "\n",
    "all_starts = []\n",
    "grid_starts = []\n",
    "grid_word_count = []\n",
    "\n",
    "for file in files:\n",
    "    textgrid = tg.TextGrid(file)\n",
    "    words = [interval.text for interval in (textgrid['text words']) if interval.text != \"\"]\n",
    "    starts = [interval.xmin for interval in (textgrid['text words']) if interval.text != \"\"]\n",
    "    all_starts.append(starts)\n",
    "    grid_word_count.append(len(words))\n",
    "    \n",
    "    first_pause = textgrid['text words'][0].xmax\n",
    "    grid_starts.append(first_pause)\n",
    "    \n",
    "grid_length = [starts[-1] - starts[0] for starts in all_starts]\n",
    "\n",
    "for i in range(len(grid_length)):\n",
    "    print(f'For run {i+1}')\n",
    "    print(f'Word count: {grid_word_count[i]}')\n",
    "    print(f'Grids word length from Praat textgrids: {grid_length[i]} ') \n",
    "    print(f'Grid start from audio start to first word: {grid_starts[i]} ') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffe7d3-856a-4f31-a9b9-eb281ad4001b",
   "metadata": {},
   "source": [
    "# Wav File testing\n",
    "\n",
    "Now testing the amount of triggers in the wav file, as well as its real length (as used in the MEG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba3d83-c873-43dd-b7d6-2de3b7187e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CHAPTER_PATHS = [\n",
    "    \"ch1-3.wav\",\n",
    "    \"ch4-6.wav\",\n",
    "    \"ch7-9.wav\",\n",
    "    \"ch10-12.wav\",\n",
    "    \"ch13-14.wav\",\n",
    "    \"ch15-19.wav\",\n",
    "    \"ch20-22.wav\",\n",
    "    \"ch23-25.wav\",\n",
    "    \"ch26-27.wav\",\n",
    "]\n",
    "\n",
    "audio_word_trigger_length = []\n",
    "audio_trigger_length = []\n",
    "\n",
    "for i, chapter in enumerate(CHAPTER_PATHS):\n",
    "    wav_file = f'/home/co/data/neuralset/LPP_copy/PallierListen2023/download/sourcedata/stimuli/audio/{chapter}'\n",
    "    duration = get_wav_duration(wav_file)\n",
    "    fs, data = wavfile.read(wav_file)\n",
    "\n",
    "    # Getting the channel for the triggers encoded in the 2nd part of the wav\n",
    "    data = data[:,1]\n",
    "    data = np.round(data / 30000)\n",
    "    diff = np.diff(data)\n",
    "    audio_triggers = np.where(diff == 1)[0]\n",
    "    word_total_length =  (audio_triggers[-1] - audio_triggers[0] )/ fs\n",
    "    \n",
    "    audio_word_trigger_length.append(word_total_length)\n",
    "    audio_trigger_length.append(duration)\n",
    "    print(f'For run: {i+1}')\n",
    "    print(f'Word count:' , len(audio_triggers))\n",
    "    print(\"Length of word triggers in the audio file : \", word_total_length)\n",
    "    print(\"TOTAL Length of the audio file : \", duration)\n",
    "    print(f'Wav start from audio start to first word: {audio_triggers[0] / fs} ') \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72997aad-14cf-4a12-9ace-837bce58de48",
   "metadata": {},
   "source": [
    "# Metadata testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df564b5d-e83e-4e0f-b3d5-82569fb3e437",
   "metadata": {},
   "source": [
    "There is an inherent problem that I could fix with the metadata. In the textgrid, there is a weird jump with du (it is always followed by an empty event \"\").\n",
    "\n",
    "I added it to the metadata, but it is not present in the triggers. It means a difference of ~40 words that are missed when matching on metadata with triggers. \n",
    "\n",
    "I fixed it by removing empty words when reading the event file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638bbd6-cd8f-4e46-b3a9-dc99cd8f597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/co/data/LPP_MEG_auditory/sub-1/ses-01/meg/sub-1_ses-01_task-listen_run-\"\n",
    "\n",
    "for run in range(1,10):\n",
    "    file = path + f'0{run}_events.tsv'\n",
    "    meta = pd.read_csv(file, sep='\\t')\n",
    "    meta['start'] = meta.onset\n",
    "    \n",
    "    # Trying to remove the additional empty word\n",
    "    \n",
    "    meta_triggers = meta.start\n",
    "    print(f'For run {run}')\n",
    "    print(f'Word count: {meta.shape[0]}')\n",
    "    print(f'Meta length from metadata file: {(meta_triggers.iloc[-1] - meta_triggers.iloc[0])} ') \n",
    "    print(f'Metadata start from audio start to first word: {meta_triggers[0]} ') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72912ad-4fe1-4f9e-8c49-4405bcce1836",
   "metadata": {},
   "source": [
    "# Triggers testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d39b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two events in STI001\n",
    "# Start of sound, first trigger STI008613.5172789115646\n",
    "# Verify buffer over/under run\n",
    "# Instller JACK driver audio PC MEG?\n",
    "\n",
    "# Timing photodiode theorique PallierRead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a4a43-36a3-4652-a1ae-a34c6cd77998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import logging\n",
    "import mne_bids\n",
    "# Set the logger level to WARNING to reduce verbosity\n",
    "logger = logging.getLogger('mne')\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "subject = '2'\n",
    "\n",
    "all_triggers = []  # All the trigger timings\n",
    "all_word_triggers = []  # All the word triggers\n",
    "trigger_starts = []  # The time of between first trigger (beginning of audio) and first word\n",
    "triggers_word_count = []  # The total number of word triggers for each run\n",
    "all_word_trigger_length = []  # The total length from the first word trigger to the last one\n",
    "all_trigger_length = []  # The total length from the first trigger to the last one (the whole audio file)\n",
    "\n",
    "for run in range(1,10):\n",
    "    run_id = '0'+ str(run)\n",
    "    # Audio data\n",
    "    \n",
    "    audio_data = get_audio_timestamps(subject, run_id)\n",
    "    \n",
    "    # Triggers words\n",
    "    f, triggers = get_triggers(subject, run_id)\n",
    "    all_triggers.append(triggers)\n",
    "    \n",
    "    word_triggers = triggers[triggers[:, 2] > 1][:,0]\n",
    "    all_word_triggers.append(word_triggers)\n",
    "    \n",
    "    first_diff = (triggers[1][0] - triggers[0][0]) / f\n",
    "    trigger_starts.append(first_diff)\n",
    "    \n",
    "    triggers_word_count.append(word_triggers.shape[0])\n",
    "    \n",
    "    word_trigger_length = (word_triggers[-1] - word_triggers[0]) / f\n",
    "    all_word_trigger_length.append(word_trigger_length)\n",
    "    \n",
    "    total_length = triggers[-1][0] - triggers[0][0]\n",
    "    all_trigger_length.append(total_length)\n",
    "        \n",
    "for i in range(len(all_word_trigger_length)):\n",
    "    print(f'For run {i}')\n",
    "    print(f'Number of word triggers: {triggers_word_count[i]} ') \n",
    "    print(f'Trigger start from audio start to first word: {trigger_starts[i]} ') \n",
    "    print(f'Total length of words triggers: {all_word_trigger_length[i]}')\n",
    "    print(f'Total length from first triger to the last: {all_trigger_length[i] / f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb3e5d-8ab6-4974-a2f9-25f07443e0b8",
   "metadata": {},
   "source": [
    "# Comparaisons\n",
    "\n",
    "Let's compare the difference between the trigger length recorded vs theoritical, between runs / subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab511d3-6cd6-46ad-9a99-cfced73ad5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_diff = [all_word_trigger_length[run] - audio_word_trigger_length[run] for run in range(9)]\n",
    "\n",
    "plt.plot(timing_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aab9ce-27c3-4a8f-be14-c9334bae90cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '01'\n",
    "subject = '3'\n",
    "\n",
    "path = Path(\"/media/co/T7/workspace-LPP/data/MEG/LPP/PallierListen2023/download\")\n",
    "\n",
    "task = 'listen'\n",
    "bids_path = mne_bids.BIDSPath(\n",
    "    subject=subject,\n",
    "    session=\"01\",\n",
    "    task=task,\n",
    "    datatype=\"meg\",\n",
    "    root=path,\n",
    "    run=run_id,\n",
    ")\n",
    "\n",
    "raw = mne_bids.read_raw_bids(bids_path)\n",
    "# triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "\n",
    "# Generate event_file path\n",
    "event_file = path / f\"sub-{bids_path.subject}\"\n",
    "event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "event_file = event_file / \"meg\"\n",
    "event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "event_file += f\"_ses-{bids_path.session}\"\n",
    "event_file += f\"_task-{bids_path.task}\"\n",
    "event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "assert Path(event_file).exists()\n",
    "\n",
    "meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "\n",
    "meta[\"word\"] = meta[\"trial_type\"].apply(\n",
    "        lambda x: eval(x)[\"word\"] if type(eval(x)) == dict else np.nan)\n",
    "\n",
    "# Remove the empty words:\n",
    "\n",
    "meta.loc[meta['word'] == ' ', 'word'] = None\n",
    "\n",
    "# Drop the rows containing NaN values in the text column\n",
    "meta = meta.dropna(subset=['word'])\n",
    "\n",
    "meta['start'] = meta.onset\n",
    "\n",
    "words = meta\n",
    "\n",
    "events_df = meta\n",
    "# Triggers from the MEG: :, 2 is the trigger value: 1 is a word, 128 is the start / end\n",
    "word_triggers = triggers[triggers[:, 2] > 1]\n",
    "\n",
    "\n",
    "# Matching triggers and metadata\n",
    "\n",
    "\n",
    "decimals = 2\n",
    "triggers_delta = np.round(np.diff(word_triggers[:, 0] / raw.info[\"sfreq\"]), decimals=decimals)  # type: ignore\n",
    "events_delta = np.round(np.diff(words.start.values), decimals=decimals)  # type: ignore\n",
    "i, j = match_list(triggers_delta, events_delta)\n",
    "\n",
    "print(f\"Found {len(i)/len(words)} of the words in the triggers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4249d0-b62f-4dc9-befd-18006eeaa4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a complete array to understand whats going on before / after match list\n",
    "\n",
    "current_i = -1\n",
    "current_j = -1\n",
    "\n",
    "list_events = []\n",
    "for i_, j_ in zip(i, j):  # Make sure that 'i' and 'j' iterables are defined before this line\n",
    "    for m_i in range(current_i + 1, i_):\n",
    "        list_events.append({\"data\": \"trigger\", \"trigger_ind\": m_i, \"trigger_time\": word_triggers[m_i][0]/ raw.info['sfreq']})\n",
    "        \n",
    "    for m_j in range(current_j + 1, j_):\n",
    "        list_events.append({\"data\": \"word\", \"word\": words.word.iloc[m_j], \"word_time\": words.onset.iloc[m_j], \"word_duration\": words.duration.iloc[m_j]})\n",
    "    list_events.append({\"data\": \"match\", \"trigger_ind\": i_, \"word_ind\":words.index[j_], \"word\": words.word.iloc[j_], \"trigger_time\": word_triggers[i_][0] / raw.info['sfreq'], \"word_time\": words.onset.iloc[j_], \"word_duration\": words.duration.iloc[j_]})\n",
    "    current_i = i_\n",
    "    current_j = j_\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a8d33-e1f3-4bb1-b0a0-46ce25e71122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events = pd.DataFrame(list_events)\n",
    "df_events = df_events.loc[:, sorted(df_events.columns)]\n",
    "df_events.to_csv('./df_events.csv', sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456fde1-44d5-4e95-97da-644406caed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 5 words for which we know it is the end of a sentence,and confirm their timings !\n",
    "\n",
    "targets = []\n",
    "\n",
    "audio_start = 4724  # Find the audio start in the wav file extracted, from the wave form\n",
    "\n",
    "timings = [1601, 1611, 1621, 1622, 1628, 1629, 1630]  # Gravement, (chez) moi: 605328,  ajouta, droit: 610390: pas aller bien \n",
    "# Le petit prince remarqua gravement: word_ind 1601\n",
    "\n",
    "for timing in timings:\n",
    "    target = df_events.loc[df_events.word_ind == timing].trigger_time.values[0]\n",
    "    targets.append(target*raw.info['sfreq'])\n",
    "\n",
    "diff_moi = (df_events.loc[df_events.word_ind == 1611].trigger_time.values[0] * 1000 )- 605328\n",
    "diff_droit =( df_events.loc[df_events.word_ind == 1622].trigger_time.values[0] * 1000) - 610390"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d85fb2-abab-42c3-ae80-3db522ccd355",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951f20c-3af9-4c0c-84d4-b7ce91a339bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from Levenshtein import editops\n",
    "import string\n",
    "\n",
    "def decod_xy(X, y):\n",
    "    \"\"\"\n",
    "    Simple decoding function to evaluate how much\n",
    "    information can be infered from X to predict y.\n",
    "    Training a RidgeCV and then calculating the\n",
    "    Pearson R between predicted and test y\n",
    "    Args:\n",
    "        - X: np.Array\n",
    "        - y: np.Array\n",
    "    Returns:\n",
    "        - np.Array\n",
    "    \"\"\"\n",
    "    assert len(X) == len(y)\n",
    "    # define data\n",
    "    model = make_pipeline(StandardScaler(), RidgeCV(alphas=np.logspace(-3, 8, 10)))\n",
    "    cv = KFold(5, shuffle=True, random_state=0)\n",
    "\n",
    "    # fit predict\n",
    "    n, n_chans, n_times = X.shape\n",
    "    if y.ndim == 1:\n",
    "        y = np.asarray(y).reshape(y.shape[0], 1)\n",
    "    R = np.zeros((n_times, y.shape[1]))\n",
    "\n",
    "    for t in range(n_times):\n",
    "        print(\".\", end=\"\")\n",
    "        rs = []\n",
    "        # y_pred = cross_val_predict(model, X[:, :, t], y, cv=cv)\n",
    "        for train, test in cv.split(X):\n",
    "            model.fit(X[train, :, t], y[train])\n",
    "            y_pred = model.predict(X[test, :, t])\n",
    "            r = correlate(y[test], y_pred)\n",
    "            rs.append(r)\n",
    "        R[t] = np.mean(rs)\n",
    "        # R[t] = correlate(y, y_pred)\n",
    "\n",
    "    return R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a379ca-3f6c-43d6-96a6-f10142aac954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import mne_bids\n",
    "import mne\n",
    "\n",
    "import pandas as pd\n",
    "from utils import match_list\n",
    "\n",
    "\n",
    "#path = '/home/co/data/LPP_MEG_auditory'\n",
    "path = Path(\"/media/co/T7/workspace-LPP/data/MEG/LPP/PallierListen2023/download\")\n",
    "\n",
    "\n",
    "def get_trigger_diff_array(subject):\n",
    "    run_id = '01'\n",
    "    path = Path(\"/media/co/T7/workspace-LPP/data/MEG/LPP/PallierListen2023/download\")\n",
    "\n",
    "    task = 'listen'\n",
    "    bids_path = mne_bids.BIDSPath(\n",
    "        subject=subject,\n",
    "        session=\"01\",\n",
    "        task=task,\n",
    "        datatype=\"meg\",\n",
    "        root=path,\n",
    "        run=run_id,\n",
    "    )\n",
    "\n",
    "    raw = mne_bids.read_raw_bids(bids_path)\n",
    "    # triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "    triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "\n",
    "    # Generate event_file path\n",
    "    event_file = path / f\"sub-{bids_path.subject}\"\n",
    "    event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "    event_file = event_file / \"meg\"\n",
    "    event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "    event_file += f\"_ses-{bids_path.session}\"\n",
    "    event_file += f\"_task-{bids_path.task}\"\n",
    "    event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "    assert Path(event_file).exists()\n",
    "\n",
    "    meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "\n",
    "    meta[\"word\"] = meta[\"trial_type\"].apply(\n",
    "            lambda x: eval(x)[\"word\"] if type(eval(x)) == dict else np.nan)\n",
    "\n",
    "    # Trying to remove the empty words:\n",
    "    \n",
    "    meta.loc[meta['word'] == ' ', 'word'] = None\n",
    "\n",
    "    # Drop the rows containing NaN values in the text column\n",
    "    meta = meta.dropna(subset=['word'])\n",
    "\n",
    "    meta['start'] = meta.onset\n",
    "\n",
    "    words = meta\n",
    "\n",
    "    events_df = meta\n",
    "    # Triggers from the MEG: :, 2 is the trigger value: 1 is a word, 128 is the start / end\n",
    "    word_triggers = triggers[triggers[:, 2] > 1]\n",
    "\n",
    "\n",
    "    # Matching triggers and metadata\n",
    "\n",
    "\n",
    "    decimals = 2\n",
    "    triggers_delta = np.round(np.diff(word_triggers[:, 0] / raw.info[\"sfreq\"]), decimals=decimals)  # type: ignore\n",
    "    events_delta = np.round(np.diff(words.start.values), decimals=decimals)  # type: ignore\n",
    "    i, j = match_list(triggers_delta, events_delta)\n",
    "\n",
    "    print(f\"Found {len(i)/len(words)} of the words in the triggers\")\n",
    "\n",
    "    true_indices = words.iloc[j].index\n",
    "\n",
    "    # Find the missing words df\n",
    "    opposite_indices = events_df.index.difference(true_indices)\n",
    "    opposite_events_df = events_df.loc[opposite_indices]\n",
    "\n",
    "\n",
    "    events_df = events_df.loc[true_indices].copy()\n",
    "\n",
    "    events_df.loc[true_indices, \"start\"] = (\n",
    "        word_triggers[i, 0] / raw.info[\"sfreq\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Match on text the events_df df with the initial metadata, to have info how it shifts\n",
    "\n",
    "    metadata_words = meta\n",
    "    matched_words = events_df\n",
    "\n",
    "    # Match list between the two events arrays\n",
    "    idx2, idx = match_list((metadata_words.word.values), (matched_words.word.values))\n",
    "\n",
    "    # Return the diff arrays\n",
    "    diff_array = metadata_words.start.values[idx2] - matched_words.start.values[idx]\n",
    "    return diff_array, len(i)/len(words)\n",
    "\n",
    "def get_triggers(subject, run_id):\n",
    "    task = 'listen'\n",
    "    bids_path = mne_bids.BIDSPath(\n",
    "        subject=subject,\n",
    "        session=\"01\",\n",
    "        task=task,\n",
    "        datatype=\"meg\",\n",
    "        root=path,\n",
    "        run=run_id,\n",
    "    )\n",
    "    raw = mne_bids.read_raw_bids(bids_path)\n",
    "    triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "    sfreq = raw.info['sfreq']\n",
    "    \n",
    "    return sfreq, triggers\n",
    "\n",
    "def get_audio_timestamps(subject, run_id):\n",
    "    task = 'listen'\n",
    "    bids_path = mne_bids.BIDSPath(\n",
    "        subject=subject,\n",
    "        session=\"01\",\n",
    "        task=task,\n",
    "        datatype=\"meg\",\n",
    "        root=path,\n",
    "        run=run_id,\n",
    "    )\n",
    "    raw = mne_bids.read_raw_bids(bids_path)\n",
    "    audio_data = raw.get_data(picks=[\"MISC004\"])[0,:]\n",
    "\n",
    "    sfreq = raw.info['sfreq']\n",
    "    \n",
    "    return sfreq, audio_data\n",
    "\n",
    "def get_raw(subject, run_id):\n",
    "    task = 'listen'\n",
    "    bids_path = mne_bids.BIDSPath(\n",
    "        subject=subject,\n",
    "        session=\"01\",\n",
    "        task=task,\n",
    "        datatype=\"meg\",\n",
    "        root=path,\n",
    "        run=run_id,\n",
    "    )\n",
    "    raw = mne_bids.read_raw_bids(bids_path)\n",
    "    return raw\n",
    "\n",
    "\n",
    "import wave\n",
    "\n",
    "def get_wav_duration(filename):\n",
    "    with wave.open(filename, 'rb') as wav_file:\n",
    "        # Get the total number of frames in the WAV file\n",
    "        frames = wav_file.getnframes()\n",
    "        \n",
    "        # Get the frame rate (number of frames per second)\n",
    "        frame_rate = wav_file.getframerate()\n",
    "        \n",
    "        # Calculate the duration in seconds\n",
    "        duration = frames / float(frame_rate)\n",
    "        \n",
    "        return duration\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_timing_diff(all_results, target):\n",
    "    num_subjects = len(all_results)\n",
    "    num_runs = 9\n",
    "\n",
    "    # Create an empty list to store the timing differences for all subjects and runs\n",
    "    timing_diffs = [[] for _ in range(num_subjects)]\n",
    "\n",
    "    # Loop over all subjects and runs\n",
    "    for subject_idx, subject_results in enumerate(all_results):\n",
    "        for run in range(1, num_runs + 1):\n",
    "            \n",
    "            if target == 'word':\n",
    "                # Extract the word trigger length and audio word trigger length for the current run\n",
    "                word_trigger_length = subject_results['all_word_trigger_length'][run - 1]\n",
    "                audio_word_trigger_length_ = audio_word_trigger_length[run - 1]\n",
    "\n",
    "                # Calculate the timing difference\n",
    "                timing_diff_ = word_trigger_length - audio_word_trigger_length_\n",
    "\n",
    "            if target == 'audio':\n",
    "                audio_trigger_length_meg = subject_results['all_trigger_length'][run - 1]\n",
    "                audio_trigger_length_ = audio_trigger_length[run - 1]\n",
    "\n",
    "                # Calculate the timing difference\n",
    "                timing_diff_ = (audio_trigger_length_meg / 1000) - audio_trigger_length_\n",
    "                \n",
    "                \n",
    "            timing_diffs[subject_idx].append(timing_diff_)\n",
    "            \n",
    "            \n",
    "    # Loop over all subjects and plot the timing differences for each run\n",
    "    for subject_idx, timing_diff in enumerate(timing_diffs):\n",
    "        plt.plot(range(1, num_runs + 1), timing_diff, label=f'Subject {subject_idx + 1}')\n",
    "\n",
    "    # Add a legend and labels to the plot\n",
    "    plt.legend()\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel(f'Timing difference for {target}(s)')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406256c9-a684-4d50-8881-c6a1a62b042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import logging\n",
    "import mne_bids\n",
    "# Set the logger level to WARNING to reduce verbosity\n",
    "logger = logging.getLogger('mne')\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for subject in range(1,20):\n",
    "    subject = str(subject)\n",
    "    all_triggers = []  # All the trigger timings\n",
    "    all_word_triggers = []  # All the word triggers\n",
    "    trigger_starts = []  # The time of between first trigger (beginning of audio) and first word\n",
    "    triggers_word_count = []  # The total number of word triggers for each run\n",
    "    all_word_trigger_length = []  # The total length from the first word trigger to the last one\n",
    "    all_trigger_length = []  # The total length from the first trigger to the last one (the whole audio file)\n",
    "\n",
    "    subject_results = {}  # Create an empty dictionary for the subject results\n",
    "    \n",
    "    for run in range(1,10):\n",
    "        run_id = '0'+ str(run)\n",
    "\n",
    "        # Triggers words\n",
    "        f, triggers = get_triggers(subject, run_id)\n",
    "        all_triggers.append(triggers)\n",
    "\n",
    "        word_triggers = triggers[triggers[:, 2] > 1][:,0]\n",
    "        all_word_triggers.append(word_triggers)\n",
    "\n",
    "        first_diff = (triggers[1][0] - triggers[0][0]) / f\n",
    "        trigger_starts.append(first_diff)\n",
    "\n",
    "        triggers_word_count.append(word_triggers.shape[0])\n",
    "\n",
    "        word_trigger_length = (word_triggers[-1] - word_triggers[0]) / f\n",
    "        all_word_trigger_length.append(word_trigger_length)\n",
    "\n",
    "        total_length = triggers[-1][0] - triggers[0][0]\n",
    "        all_trigger_length.append(total_length)\n",
    "\n",
    "    # Add the results to the subject dictionary\n",
    "    subject_results['all_triggers'] = all_triggers\n",
    "    subject_results['all_word_triggers'] = all_word_triggers\n",
    "    subject_results['trigger_starts'] = trigger_starts\n",
    "    subject_results['triggers_word_count'] = triggers_word_count\n",
    "    subject_results['all_word_trigger_length'] = all_word_trigger_length\n",
    "    subject_results['all_trigger_length'] = all_trigger_length\n",
    "\n",
    "    all_results.append(subject_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba68e28b-d473-4099-aad6-07774a5eea76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
