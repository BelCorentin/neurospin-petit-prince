{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw.annotations + order of things test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import logging\n",
    "import mne_bids\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from utils import match_list\n",
    "\n",
    "# Set the logger level to WARNING to reduce verbosity\n",
    "logger = logging.getLogger('mne')\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "#path = Path(\"/home/co/data/neuralset/LPP_copy/pallierlisten2023/download\")\n",
    "path = Path(\"/media/co/T7/workspace-LPP/data/MEG/LPP/PallierRead2023/download\")\n",
    "\n",
    "subject = '30'\n",
    "run = '01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'read'\n",
    "bids_path = mne_bids.BIDSPath(\n",
    "    subject=subject,\n",
    "    session=\"01\",\n",
    "    task=task,\n",
    "    datatype=\"meg\",\n",
    "    root=path,\n",
    "    run=run,\n",
    ")\n",
    "\n",
    "raw = mne_bids.read_raw_bids(bids_path)\n",
    "\n",
    "# extract annotations\n",
    "events = []\n",
    "for (\n",
    "    annot\n",
    ") in (\n",
    "    raw.annotations\n",
    "):\n",
    "    description = annot.pop(\"description\")\n",
    "    if \"BAD_ACQ_SKIP\" in description:\n",
    "        continue\n",
    "    event = eval(description)\n",
    "    event[\"condition\"] = \"sentence\"\n",
    "    event[\"type\"] = event.pop(\"kind\").capitalize()\n",
    "    event[\"start\"] = annot[\"onset\"]\n",
    "    event[\"duration\"] = annot[\"duration\"]\n",
    "    event[\"stop\"] = annot[\"onset\"] + annot[\"duration\"]\n",
    "    event[\"language\"] = \"french\"\n",
    "    events.append(event)\n",
    "\n",
    "# The size of raw.annotations impacts the creation of the events_df: smaller than the number of events\n",
    "events_df = pd.DataFrame(events).rename(columns=dict(word=\"text\"))\n",
    "\n",
    "# Read the TSV file into the 'words' DataFrame\n",
    "eventsfile = '/media/co/T7/workspace-LPP/data/MEG/LPP/PallierRead2023/download/sub-1/ses-01/meg/sub-1_ses-01_task-read_run-01_events.tsv'\n",
    "words = pd.read_csv(eventsfile, sep=\"\\t\")\n",
    "\n",
    "\n",
    "# Create the 'events_df' DataFrame\n",
    "events = []\n",
    "for _, row in words.iterrows():\n",
    "    description = row[\"trial_type\"]\n",
    "    if \"BAD_ACQ_SKIP\" in description:\n",
    "        continue\n",
    "    event = eval(description)\n",
    "    event[\"condition\"] = \"sentence\"\n",
    "    event[\"type\"] = event.pop(\"kind\").capitalize()\n",
    "    event[\"start\"] = row[\"onset\"]\n",
    "    event[\"duration\"] = row[\"duration\"]\n",
    "    event[\"stop\"] = row[\"onset\"] + row[\"duration\"]\n",
    "    event[\"language\"] = \"french\"\n",
    "    event[\"text\"] = row[\"word\"]\n",
    "    events.append(event)\n",
    "\n",
    "events_df2 = pd.DataFrame(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the grounth truth for words that were used in the STIM setup\n",
    "correct_words_df = pd.read_csv(self._get_word_info_path(), delimiter=\"\\t\")\n",
    "\n",
    "# In order to match with the events_df, we need the clean words from correct_words_df\n",
    "correct_words_df.trial_type = correct_words_df.trial_type.apply(\n",
    "    lambda x: eval(x)[\"word\"]\n",
    ")\n",
    "correct_words_match = correct_words_df[\"trial_type\"].values.astype(str)\n",
    "rows_events, rows_metadata = match_list(\n",
    "    events_df[\"text\"].values.astype(str),\n",
    "    correct_words_match,\n",
    ")\n",
    "assert len(rows_events) / len(events_df) > 0.95, (\n",
    "    error_msg_prefix\n",
    "    + f\"only {len(rows_events) / len(events_df)} of the words were found in the metadata\"\n",
    ")\n",
    "\n",
    "events_idx, metadata_idx = (\n",
    "    events_df.index[rows_events],\n",
    "    correct_words_df.index[rows_metadata],\n",
    ")\n",
    "events_df.loc[events_idx, \"text\"] = correct_words_df.loc[metadata_idx, \"word\"]\n",
    "events_df.loc[events_idx, \"clean_text\"] = correct_words_df.loc[\n",
    "    metadata_idx, \"trial_type\"\n",
    "].values.astype(str)\n",
    "\n",
    "# TODO: this hack doesnt work as in read, the j and avais have been merged\n",
    "# It is thus needed to think about how to find again this information\n",
    "\n",
    "# Small data augmentation because some columns dont exist in the read metadata\n",
    "# metadata_listen = pd.read_csv(self.path / \"sourcedata/task-listen_run-{self.run}_extra_info.tsv\")\n",
    "# # Add to metadata the missing columns from the listen metadata: n_closing, is_last_word, pos, content_word\n",
    "# metadata = metadata.merge(metadata_listen[[\"word\", \"n_closing\", \"is_last_word\", \"pos\", \"content_word\"]], on=\"word\")\n",
    "\n",
    "word_triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "words = events_df.loc[events_df.type == \"Word\"]\n",
    "words[\"wlength\"] = words.text.apply(len)\n",
    "if word_triggers[:, 2].max() > 2048:\n",
    "    word_triggers[:, 2] = (\n",
    "        word_triggers[:, 2] - 2048\n",
    "    )  # HACK because of a bug in the word_triggers for 2 subjects that have particularly high word_triggers\n",
    "\n",
    "# Matching the triggers wlength (with hyphens, dashes etc..) with the CORRECT metadata\n",
    "i, j = match_list(word_triggers[:, 2], words.wlength)\n",
    "\n",
    "assert len(i) / len(word_triggers) > 0.9, (\n",
    "    error_msg_prefix\n",
    "    + f\"only {len(i)/len(word_triggers)} of the words were found in the word_triggers\"\n",
    ")\n",
    "matched_word_indices = words.iloc[j].index\n",
    "\n",
    "# Create new type of events: missed words that were not found in the triggers\n",
    "events_df[\"unaligned_start\"] = events_df[\"start\"]\n",
    "missed_words = words[~words.index.isin(matched_word_indices)].copy()\n",
    "missed_words[\"type\"] = \"MissedWord\"\n",
    "\n",
    "events_df.loc[matched_word_indices, \"start\"] = (\n",
    "    word_triggers[i, 0] / raw.info[\"sfreq\"]\n",
    ")\n",
    "\n",
    "# Drop the word events that were not found in the triggers\n",
    "false_indices = words[~words.index.isin(matched_word_indices)].index\n",
    "events_df.loc[false_indices, \"start\"] = np.nan\n",
    "events_df = events_df.dropna(subset=[\"start\"])\n",
    "\n",
    "# Add the missed words to the events_df\n",
    "events_df = pd.concat([events_df, missed_words])\n",
    "\n",
    "# Match the events with the metadata\n",
    "metadata = pd.read_csv(self._get_seq_id_path())\n",
    "\n",
    "# Match with the metadata df that contains syntactic info, in order to append them later\n",
    "# Match it with the CLEAN text, as it is the one that is present in the extra_info\n",
    "rows_events, rows_metadata = match_list(\n",
    "    [str(word) for word in events_df[\"clean_text\"].values],\n",
    "    [str(word) for word in metadata[\"word\"].values],\n",
    ")\n",
    "\n",
    "assert len(rows_events) / len(events_df) > 0.95, (\n",
    "    error_msg_prefix\n",
    "    + f\"only {len(rows_events) / len(events_df)} of the words were found in the metadata\"\n",
    ")\n",
    "events_idx, metadata_idx = (\n",
    "    events_df.index[rows_events],\n",
    "    metadata.index[rows_metadata],\n",
    ")\n",
    "\n",
    "# Adding the information about sequence_id and n_closing\n",
    "events_df[\"word\"] = events_df[\"text\"]\n",
    "# for col in [\"sequence_id\", \"n_closing\", \"is_last_word\", \"pos\"]:\n",
    "for col in [\"sequence_id\"]:\n",
    "    events_df.loc[events_idx, col] = metadata.loc[metadata_idx, col]\n",
    "\n",
    "# Add sentence / constituent info\n",
    "events_df = _enrich_metadata(events_df)\n",
    "\n",
    "# add train/test/val splits\n",
    "events_df = set_sentence_split(events_df)  # TODO\n",
    "\n",
    "# add raw event\n",
    "uri = f\"method:_load_raw?timeline={self.timeline}\"\n",
    "meg = {\"filepath\": uri, \"type\": \"Meg\", \"start\": 0}\n",
    "events_df = pd.concat([pd.DataFrame([meg]), events_df])\n",
    "\n",
    "# sort by start\n",
    "events_df = events_df.sort_values(by=\"start\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the visual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import logging\n",
    "import mne_bids\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the logger level to WARNING to reduce verbosity\n",
    "logger = logging.getLogger('mne')\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "#path = Path(\"/home/co/data/neuralset/LPP_copy/pallierlisten2023/download\")\n",
    "path = Path(\"/media/co/T7/workspace-LPP/data/MEG/LPP/PallierRead2023/download\")\n",
    "\n",
    "def testing(subject, run_id):\n",
    "    task = 'read'\n",
    "    bids_path = mne_bids.BIDSPath(\n",
    "        subject=subject,\n",
    "        session=\"01\",\n",
    "        task=task,\n",
    "        datatype=\"meg\",\n",
    "        root=path,\n",
    "        run=run_id,\n",
    "    )\n",
    "\n",
    "    raw = mne_bids.read_raw_bids(bids_path)\n",
    "    # triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "    triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "\n",
    "    # Generate event_file path\n",
    "    event_file = path / f\"sub-{bids_path.subject}\"\n",
    "    event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "    event_file = event_file / \"meg\"\n",
    "    event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "    event_file += f\"_ses-{bids_path.session}\"\n",
    "    event_file += f\"_task-{bids_path.task}\"\n",
    "    event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "    assert Path(event_file).exists()\n",
    "\n",
    "    meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "\n",
    "    meta[\"word\"] = meta[\"trial_type\"].apply(\n",
    "            lambda x: eval(x)[\"word\"] if type(eval(x)) == dict else np.nan)\n",
    "\n",
    "    # Remove the empty words:\n",
    "\n",
    "    meta.loc[meta['word'] == ' ', 'word'] = None\n",
    "\n",
    "    # Drop the rows containing NaN values in the text column\n",
    "    meta = meta.dropna(subset=['word'])\n",
    "\n",
    "    meta['start'] = meta.onset\n",
    "\n",
    "    # return meta\n",
    "    # Get the length of the meta file\n",
    "    total_time_meta = np.array(meta.onset)[-1] - np.array(meta.onset)[0]\n",
    "\n",
    "    # Length of triggers\n",
    "    total_time_triggers = triggers[-1][0] - triggers[0][0]\n",
    "\n",
    "    return total_time_meta, total_time_triggers, (len(triggers) / len(meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"2\"\n",
    "run = '01'\n",
    "\n",
    "# meta = testing(subject, run)\n",
    "total_time_meta, total_time_triggers, perc = testing(subject, run)\n",
    "shift = total_time_meta - (total_time_triggers / 1000)\n",
    "shift, perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_meta, total_time_triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"40\"\n",
    "run = '01'\n",
    "\n",
    "task = 'read'\n",
    "bids_path = mne_bids.BIDSPath(\n",
    "    subject=subject,\n",
    "    session=\"01\",\n",
    "    task=task,\n",
    "    datatype=\"meg\",\n",
    "    root=path,\n",
    "    run=run,\n",
    ")\n",
    "\n",
    "raw = mne_bids.read_raw_bids(bids_path)\n",
    "# triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "\n",
    "# Generate event_file path\n",
    "event_file = path / f\"sub-{bids_path.subject}\"\n",
    "event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "event_file = event_file / \"meg\"\n",
    "event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "event_file += f\"_ses-{bids_path.session}\"\n",
    "event_file += f\"_task-{bids_path.task}\"\n",
    "event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "assert Path(event_file).exists()\n",
    "\n",
    "meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "\n",
    "meta[\"word\"] = meta[\"trial_type\"].apply(\n",
    "        lambda x: eval(x)[\"word\"] if type(eval(x)) == dict else np.nan)\n",
    "\n",
    "# Remove the empty words:\n",
    "\n",
    "meta.loc[meta['word'] == ' ', 'word'] = None\n",
    "\n",
    "# Drop the rows containing NaN values in the text column\n",
    "meta = meta.dropna(subset=['word'])\n",
    "\n",
    "meta['start'] = meta.onset\n",
    "\n",
    "# return meta\n",
    "# Get the length of the meta file\n",
    "total_time_meta = np.array(meta.onset)[-1] - np.array(meta.onset)[0]\n",
    "\n",
    "# Length of triggers\n",
    "total_time_triggers = triggers[-1][0] - triggers[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.diff(triggers[:,0]))\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw data\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(raw.copy().pick_channels(['STI101']).get_data()[0] )\n",
    "# Plot meta wlenght\n",
    "meta['wlength'] = meta['word'].apply(len)\n",
    "# plt.plot(meta.wlength, 'r')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import match_list\n",
    "\n",
    "import mne_bids\n",
    "import mne\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "path = Path(\"/media/co/T7/workspace-LPP/data/MEG/LPP/PallierRead2023/download\")\n",
    "\n",
    "\n",
    "subject = \"30\"\n",
    "run = '01'\n",
    "\n",
    "def get_annot_trigg(subject, run):\n",
    "    task = 'read'\n",
    "    bids_path = mne_bids.BIDSPath(\n",
    "        subject=subject,\n",
    "        session=\"01\",\n",
    "        task=task,\n",
    "        datatype=\"meg\",\n",
    "        root=path,\n",
    "        run=run,\n",
    "    )\n",
    "\n",
    "    raw = mne_bids.read_raw_bids(bids_path)\n",
    "\n",
    "    all_triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "\n",
    "    # Triggers are simpler for this modality: no need to get the step function / offsets\n",
    "    word_triggers = all_triggers\n",
    "    if word_triggers[:, 2].max() > 2048:\n",
    "        word_triggers[:, 2] = (\n",
    "            word_triggers[:, 2] - 2048\n",
    "        ) \n",
    "\n",
    "\n",
    "    eventsile = '/media/co/T7/workspace-LPP/data/MEG/LPP/PallierRead2023/download/sub-1/ses-01/meg/sub-1_ses-01_task-read_run-01_events.tsv'\n",
    "    words = pd.read_csv(eventsile, sep=\"\\t\")\n",
    "    # file = \"/home/co/code/LPP_experiment/formatting/v2/run1_v2_0.25_0.5.tsv\"\n",
    "    # words = pd.read_csv(file, sep=\"\\t\")\n",
    "    # words['wlength'] = words['word'].apply(len)\n",
    "    # i, j = match_list(word_triggers[:, 2], words.wlength)\n",
    "    return len(raw.annotations), len(word_triggers), words.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne \n",
    "\n",
    "mne.set_log_level('ERROR')\n",
    "for subject in range(1, 41):\n",
    "    for run in range(1, 2):\n",
    "        print(f\"Subject {subject}, run {run}\")\n",
    "        print(get_annot_trigg(str(subject), '0' + str(run)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_io = mne.io.read_raw(bids_path, allow_maxshield=True, preload=True)\n",
    "raw_io.annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw, raw.first_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7*60 + 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.annotations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw triggers\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "raw.pick_types(meg=False, eeg=False, stim=True).plot(start=0, duration=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.find_events(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(i) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_triggers[~np.isin(np.arange(word_triggers.shape[0]), i)][:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Neuralset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "for annot in raw.annotations:\n",
    "    description = annot.pop(\"description\")\n",
    "    if \"BAD_ACQ_SKIP\" in description:\n",
    "        continue\n",
    "    event = eval(description)\n",
    "    event[\"condition\"] = \"sentence\"\n",
    "    event[\"type\"] = event.pop(\"kind\").capitalize()\n",
    "    event[\"start\"] = annot[\"onset\"]\n",
    "    event[\"duration\"] = annot[\"duration\"]\n",
    "    event[\"stop\"] = annot[\"onset\"] + annot[\"duration\"]\n",
    "    event[\"language\"] = \"french\"\n",
    "    events.append(event)\n",
    "\n",
    "events_df = pd.DataFrame(events).rename(columns=dict(word=\"text\"))\n",
    "\n",
    "# Remove empty words that were included in the metadata files...\n",
    "events_df.loc[events_df[\"text\"] == \" \", \"text\"] = None\n",
    "# Drop the rows containing NaN values in the text column\n",
    "events_df = events_df.dropna(subset=[\"text\"])\n",
    "events_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Match the events with the metadata\n",
    "metadata = pd.read_csv('/media/co/T7/workspace-LPP/data/MEG/LPP/PallierRead2023/download/sourcedata/task-read_run-01_extra_info.tsv')\n",
    "\n",
    "# TODO: this hack doesnt work as in read, the j and avais have been merged\n",
    "# It is thus needed to think about how to find again this information\n",
    "# Small data augmentation because some columns dont exist in the read metadata\n",
    "# metadata_listen = pd.read_csv(self.path / \"sourcedata/task-listen_run-{self.run}_extra_info.tsv\")\n",
    "# # Add to metadata the missing columns from the listen metadata: n_closing, is_last_word, pos, content_word\n",
    "# metadata = metadata.merge(metadata_listen[[\"word\", \"n_closing\", \"is_last_word\", \"pos\", \"content_word\"]], on=\"word\")\n",
    "\n",
    "rows_events, rows_metadata = match_list(\n",
    "    [str(word) for word in events_df[\"text\"].values],\n",
    "    [str(word) for word in metadata[\"word\"].values],\n",
    ")\n",
    "\n",
    "\n",
    "events_idx, metadata_idx = (\n",
    "    events_df.index[rows_events],\n",
    "    metadata.index[rows_metadata],events_df\n",
    ")\n",
    "\n",
    "# Adding the information about sequence_id and n_closing\n",
    "events_df[\"word\"] = events_df[\"text\"]\n",
    "# for col in [\"sequence_id\", \"n_closing\", \"is_last_word\", \"pos\"]:\n",
    "for col in [\"sequence_id\"]:\n",
    "    events_df.loc[events_idx, col] = metadata.loc[metadata_idx, col]\n",
    "\n",
    "# get the correct words (pb with apostrophes)\n",
    "eventsile = '/media/co/T7/workspace-LPP/data/MEG/LPP/PallierRead2023/download/sub-1/ses-01/meg/sub-1_ses-01_task-read_run-01_events.tsv'\n",
    "\n",
    "correct_words_df = pd.read_csv(eventsile, delimiter=\"\\t\")\n",
    "correct_words_df.trial_type = correct_words_df.trial_type.apply(\n",
    "    lambda x: eval(x)[\"word\"]\n",
    ")\n",
    "rows_events, rows_metadata = match_list(\n",
    "    events_df[\"text\"].values.astype(str),\n",
    "    correct_words_df[\"trial_type\"].values.astype(str),\n",
    ")\n",
    "\n",
    "events_idx, metadata_idx = (\n",
    "    events_df.index[rows_events],\n",
    "    correct_words_df.index[rows_metadata],\n",
    ")\n",
    "events_df.loc[events_idx, \"text\"] = correct_words_df.loc[metadata_idx, \"word\"]\n",
    "\n",
    "\n",
    "all_triggers = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "\n",
    "# Triggers are simpler for this modality: no need to get the step function / offsets\n",
    "word_triggers = all_triggers[all_triggers[:, 2] > 1]\n",
    "\n",
    "words = events_df.loc[events_df.type == \"Word\"]\n",
    "words[\"wlength\"] = words.text.apply(len)\n",
    "if word_triggers[:, 2].max() > 2048:\n",
    "    word_triggers[:, 2] = (\n",
    "        word_triggers[:, 2] - 2048\n",
    "    )  # HACK because of a bug in the word_triggers for 2 subjects that have particularly high word_triggers\n",
    "i, j = match_list(word_triggers[:, 2], words.wlength)\n",
    "print(f\"Matched: {len(i) / len(word_triggers)}\")\n",
    "\n",
    "true_indices = words.iloc[j].index\n",
    "\n",
    "events_df.loc[true_indices, \"start\"] = word_triggers[i, 0] / raw.info[\"sfreq\"]\n",
    "\n",
    "\n",
    "# sort by start\n",
    "events_df = events_df.sort_values(by=\"start\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "for annot in raw.annotations:\n",
    "    description = annot.pop(\"description\")\n",
    "    if \"BAD_ACQ_SKIP\" in description:\n",
    "        continue\n",
    "    event = eval(description)\n",
    "    event[\"condition\"] = \"sentence\"\n",
    "    event[\"type\"] = event.pop(\"kind\").capitalize()\n",
    "    event[\"start\"] = annot[\"onset\"]\n",
    "    event[\"duration\"] = annot[\"duration\"]\n",
    "    event[\"stop\"] = annot[\"onset\"] + annot[\"duration\"]\n",
    "    event[\"language\"] = \"french\"\n",
    "    events.append(event)\n",
    "\n",
    "events_df = pd.DataFrame(events).rename(columns=dict(word=\"text\"))\n",
    "\n",
    "# Remove empty words that were included in the metadata files...\n",
    "events_df.loc[events_df[\"text\"] == \" \", \"text\"] = None\n",
    "# Drop the rows containing NaN values in the text column\n",
    "events_df = events_df.dropna(subset=[\"text\"])\n",
    "events_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Match the events with the metadata\n",
    "metadata = pd.read_csv('/media/co/T7/workspace-LPP/data/MEG/LPP/PallierRead2023/download/sourcedata/task-read_run-01_extra_info.tsv')\n",
    "\n",
    "# TODO: this hack doesnt work as in read, the j and avais have been merged\n",
    "# It is thus needed to think about how to find again this information\n",
    "# Small data augmentation because some columns dont exist in the read metadata\n",
    "# metadata_listen = pd.read_csv(self.path / \"sourcedata/task-listen_run-{self.run}_extra_info.tsv\")\n",
    "# # Add to metadata the missing columns from the listen metadata: n_closing, is_last_word, pos, content_word\n",
    "# metadata = metadata.merge(metadata_listen[[\"word\", \"n_closing\", \"is_last_word\", \"pos\", \"content_word\"]], on=\"word\")\n",
    "\n",
    "rows_events, rows_metadata = match_list(\n",
    "    [str(word) for word in events_df[\"text\"].values],\n",
    "    [str(word) for word in metadata[\"word\"].values],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "events_idx, metadata_idx = (\n",
    "    events_df.index[rows_events],\n",
    "    metadata.index[rows_metadata],\n",
    ")\n",
    "\n",
    "# Adding the information about sequence_id and n_closing\n",
    "events_df[\"word\"] = events_df[\"text\"]\n",
    "# for col in [\"sequence_id\", \"n_closing\", \"is_last_word\", \"pos\"]:\n",
    "for col in [\"sequence_id\"]:\n",
    "    events_df.loc[events_idx, col] = metadata.loc[metadata_idx, col]\n",
    "\n",
    "# get the correct words (pb with apostrophes)\n",
    "eventsile = '/media/co/T7/workspace-LPP/data/MEG/LPP/PallierRead2023/download/sub-1/ses-01/meg/sub-1_ses-01_task-read_run-01_events.tsv'\n",
    "\n",
    "correct_words_df = pd.read_csv(eventsile, delimiter=\"\\t\")\n",
    "correct_words_df.trial_type = correct_words_df.trial_type.apply(\n",
    "    lambda x: eval(x)[\"word\"]\n",
    ")\n",
    "rows_events, rows_metadata = match_list(\n",
    "    events_df[\"text\"].values.astype(str),\n",
    "    correct_words_df[\"trial_type\"].values.astype(str),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.loc[events_idx, \"clean_text\"] = correct_words_df.loc[metadata_idx, \"trial_type\"].values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_words_match = correct_words_df[\"trial_type\"].values.astype(str)\n",
    "rows_events, rows_metadata = match_list(\n",
    "    events_df[\"text\"].values.astype(str),\n",
    "    correct_words_match,\n",
    ")\n",
    "\n",
    "events_idx, metadata_idx = (\n",
    "    events_df.index[rows_events],\n",
    "    correct_words_df.index[rows_metadata],\n",
    ")\n",
    "events_df.loc[events_idx, \"text\"] = correct_words_df.loc[metadata_idx, \"word\"]\n",
    "events_df.loc[events_idx, \"clean_text\"] = correct_words_df.loc[metadata_idx, \"trial_type\"].values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for annot in raw.annotations:\n",
    "    word = annot[\"description\"]\n",
    "    words.append(eval(word)['word'])\n",
    "    # Match it with the metadata\n",
    "\n",
    "word_meta = correct_words_df.trial_type\n",
    "i,j = match_list(words, word_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the words in word_meta not matched\n",
    "import numpy as np\n",
    "word_meta[~np.isin(np.arange(len(word_meta)), j)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_events.shape[0] / events_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triggers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the words of not matched words\n",
    "words.loc[~words.index.isin(j)][:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
