{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b10881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_path, get_subjects, epoch_data\n",
    "from utils import decod, correlate, match_list\n",
    "import mne_bids\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from wordfreq import zipf_frequency\n",
    "from Levenshtein import editops\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "mne.set_log_level(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f78c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 1\n",
    "\n",
    "def format_meta(meta,run_id):\n",
    "    model = 'fr_core_news_sm'\n",
    "    if not spacy.util.is_package(model):\n",
    "        spacy.cli.download(model)\n",
    "\n",
    "    nlp = spacy.load(model)\n",
    "\n",
    "    CHAPTERS = {\n",
    "    1: \"1-3\",\n",
    "    2: \"4-6\",\n",
    "    3: \"7-9\",\n",
    "    4: \"10-12\",\n",
    "    5: \"13-14\",\n",
    "    6: \"15-19\",\n",
    "    7: \"20-22\",\n",
    "    8: \"23-25\",\n",
    "    9: \"26-27\",\n",
    "    }\n",
    "    txt_file = f'~/code/data/syntax/ch{CHAPTERS[run]}.syntax.txt'\n",
    "    with open(txt_file, 'r') as f:\n",
    "        txt = f.read().replace('\\n', '')\n",
    "\n",
    "    # parse text file\n",
    "    doc = self.nlp(txt)\n",
    "\n",
    "    # add parse information to metadata\n",
    "    parse_annots = []\n",
    "    for sent_id, sent in enumerate(doc.sents):\n",
    "        # HERE ADD ERIC DE LA CLERGERIE parser instead\n",
    "        closings = parse(sent)\n",
    "        assert len(closings) == len(sent)\n",
    "        for word, closing in zip(sent, closings):\n",
    "            parse_annots.append(dict(\n",
    "                word_index=word.i - sent[0].i,\n",
    "                sequence_id=sent_id,\n",
    "                sequence_uid=str(sent),\n",
    "                closing=closing,\n",
    "                match_token=word.text,\n",
    "            ))\n",
    "\n",
    "    # align text file and meg metadata\n",
    "    def format_text(text):\n",
    "        for char in ('jlsmtncd'):\n",
    "            text = text.replace(f\"{char}'\", char)\n",
    "        text = text.replace('Å“', 'oe')\n",
    "        return text.lower()\n",
    "\n",
    "    meg_words = meta.word.fillna('######').values\n",
    "    text_words = [format_text(w.text) for w in doc]\n",
    "\n",
    "    i, j = utils.match_list(meg_words, text_words)\n",
    "\n",
    "    # deal with missed tokens (e.g. wrong spelling, punctuation)\n",
    "    assert len(parse_annots) == len(text_words)\n",
    "    parse_annots = pd.DataFrame(parse_annots)\n",
    "    parse_annots.closing = parse_annots.closing.fillna(0)\n",
    "    parse_annots['closing_'] = 0\n",
    "    parse_annots['missed_closing'] = 0\n",
    "    missing = np.setdiff1d(range(len(parse_annots)), j)\n",
    "    for missed in missing:\n",
    "        current_closing = parse_annots.iloc[missed].closing\n",
    "        prev_word = parse_annots.iloc[[missed-1]].index\n",
    "        if prev_word[0] >=0:\n",
    "            parse_annots.loc[prev_word, 'missed_closing'] = current_closing\n",
    "    parse_annots.closing_ = parse_annots.closing + parse_annots.missed_closing\n",
    "\n",
    "    # Add new columns to original mne.Epochs.metadata\n",
    "    # fill columns\n",
    "    columns = ('word_index', 'sequence_id', 'sequence_uid', 'closing_', 'match_token')\n",
    "    for column in columns:\n",
    "        meta[column] = None\n",
    "        meta.loc[meta.iloc[i].index, column] = parse_annots[column].iloc[j].values\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5450c496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subjects for which the decoding will be tested: \n",
      "\n",
      "['1', '2', '3', '4', '5', '6']\n",
      "Subject 1's decoding started\n",
      ".Running the script on RAW data:\n",
      "run 01, subject: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cb271805/272a9f9d-f140-48d6-b390-094ccc29aae0/workspace-LPP/code/neurospin-petit-prince/decoding/reading/dataset.py:55: RuntimeWarning: This file contains raw Internal Active Shielding data. It may be distorted. Elekta recommends it be run through MaxFilter to produce reliable results. Consider closing the file and running MaxFilter on the data.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/media/cb271805/272a9f9d-f140-48d6-b390-094ccc29aae0/workspace-LPP/code/neurospin-petit-prince/decoding/reading/dataset.py:55: RuntimeWarning: Omitted 4 annotation(s) that were outside data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/media/cb271805/272a9f9d-f140-48d6-b390-094ccc29aae0/workspace-LPP/code/neurospin-petit-prince/decoding/reading/dataset.py:55: RuntimeWarning: The unit for channel(s) STI001, STI002, STI003, STI004, STI005, STI006, STI007, STI008, STI009, STI010, STI011, STI012, STI013, STI014, STI015, STI016, STI101, STI201, STI301 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/media/cb271805/272a9f9d-f140-48d6-b390-094ccc29aae0/workspace-LPP/code/neurospin-petit-prince/decoding/reading/dataset.py:84: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=20 parameter will result in a sampling frequency of 50.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(\n",
      "/tmp/ipykernel_1891776/575032338.py:25: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(epochs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 38\u001b[0m\n\u001b[1;32m     32\u001b[0m epochs\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m epochs\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mtrial_type\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m s: \u001b[38;5;28meval\u001b[39m(s)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m epochs\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m epochs\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mtrial_type\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m s: \u001b[38;5;28meval\u001b[39m(s)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mprint\u001b[39m(format_meta(epochs\u001b[38;5;241m.\u001b[39mmetadata, run_id))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Run a linear regression between MEG signals\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# and word frequency classification\u001b[39;00m\n\u001b[1;32m     42\u001b[0m X \u001b[38;5;241m=\u001b[39m epochs\u001b[38;5;241m.\u001b[39mget_data()\n",
      "Cell \u001b[0;32mIn [7], line 8\u001b[0m, in \u001b[0;36mformat_meta\u001b[0;34m(meta, run_id)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spacy\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mis_package(model):\n\u001b[1;32m      6\u001b[0m     spacy\u001b[38;5;241m.\u001b[39mcli\u001b[38;5;241m.\u001b[39mdownload(model)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mnlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(model)\n\u001b[1;32m     10\u001b[0m CHAPTERS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1-3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4-6\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;241m9\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m26-27\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m }\n\u001b[1;32m     21\u001b[0m txt_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/code/data/syntax/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAPTERS[run]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.syntax.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "report = mne.Report()\n",
    "path = get_path('LPP_read')\n",
    "subjects = get_subjects(path)\n",
    "RUN = 1\n",
    "\n",
    "print(\"\\nSubjects for which the decoding will be tested: \\n\")\n",
    "print(subjects)\n",
    "\n",
    "for subject in subjects:  # Ignore the first one\n",
    "\n",
    "    print(f\"Subject {subject}'s decoding started\")\n",
    "    epochs = []\n",
    "    for run_id in range(1, RUN + 1):\n",
    "        print(\".\", end=\"\")\n",
    "        epo = epoch_data(subject, \"%.2i\" % run_id, task='listen', path=path)\n",
    "        epo.metadata[\"label\"] = f\"run_{run_id}\"\n",
    "        epochs.append(epo)\n",
    "\n",
    "    # Quick fix for the dev_head: has to be\n",
    "    # fixed before doing source reconstruction\n",
    "    for epo in epochs:\n",
    "        epo.info[\"dev_head_t\"] = epochs[0].info[\"dev_head_t\"]\n",
    "        # epo.info['nchan'] = epochs[0].info['nchan']\n",
    "\n",
    "    epochs = mne.concatenate_epochs(epochs)\n",
    "\n",
    "    # Get the evoked potential averaged on all epochs for each channel\n",
    "    evo = epochs.average(method=\"median\")\n",
    "    evo.plot(spatial_colors=True)\n",
    "\n",
    "    # Handling the data structure\n",
    "    epochs.metadata[\"kind\"] = epochs.metadata.trial_type.apply(\n",
    "        lambda s: eval(s)[\"kind\"]\n",
    "    )\n",
    "    epochs.metadata[\"word\"] = epochs.metadata.trial_type.apply(\n",
    "        lambda s: eval(s)[\"word\"]\n",
    "    )\n",
    "    # TODO : re-epoch\n",
    "    print(format_meta(epochs.metadata, run_id))\n",
    "    epochs.metadat\n",
    "    epochs.metadata['closing'] = epochs.metadata.closing_.fillna(0)\n",
    "    # Run a linear regression between MEG signals\n",
    "    # and word frequency classification\n",
    "    X = epochs.get_data()\n",
    "\n",
    "    embeddings = epochs.metadata.word.apply(lambda word: nlp(word).vector).values\n",
    "    embeddings = np.array([emb for emb in embeddings])\n",
    "\n",
    "    y = embeddings\n",
    "\n",
    "    R_vec = decod(X, y)\n",
    "    R_vec_avg = np.mean(R_vec,axis = 1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=[6, 6])\n",
    "    dec = plt.fill_between(epochs.times, R_vec_avg)\n",
    "    # plt.show()\n",
    "    report.add_evokeds(evo, titles=f\"Evoked for sub {subject} \")\n",
    "    report.add_figure(fig, title=f\"decoding for subject {subject}\")\n",
    "    # report.add_figure(dec, subject, tags=\"word\")\n",
    "    report.save(\"./figures/reading_decoding_embeddings.html\", open_browser=False, overwrite=True)\n",
    "\n",
    "    print(\"Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11164b2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/code/data/syntax/ch1-3.syntax.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(format_meta(epochs\u001b[38;5;241m.\u001b[39mmetadata, run_id))\n",
      "Cell \u001b[0;32mIn [12], line 22\u001b[0m, in \u001b[0;36mformat_meta\u001b[0;34m(meta, run_id)\u001b[0m\n\u001b[1;32m     10\u001b[0m CHAPTERS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1-3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4-6\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;241m9\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m26-27\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m }\n\u001b[1;32m     21\u001b[0m txt_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/code/data/syntax/ch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAPTERS[run]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.syntax.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtxt_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     23\u001b[0m     txt \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# parse text file\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/code/data/syntax/ch1-3.syntax.txt'"
     ]
    }
   ],
   "source": [
    "print(format_meta(epochs.metadata, run_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0dd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
