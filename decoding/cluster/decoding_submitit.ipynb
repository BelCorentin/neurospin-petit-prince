{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea15eb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataset import read_raw, get_subjects, get_path, mne_events\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "\n",
    "def analysis(subject):\n",
    "    all_evos = []\n",
    "    all_scores = []\n",
    "    runs=9\n",
    "    epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "    path = get_path(\"LPP_read\")\n",
    "    task = \"read\"\n",
    "            \n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    # Dict init\n",
    "    for start in ('onset', 'offset'): \n",
    "            for level in ('word', 'constituent', 'sentence'):\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "\n",
    "        # Word end\n",
    "        meta['word_offset'] = True\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Sent stop\n",
    "        meta['next_word_id'] = meta['word_id'].shift(-1)\n",
    "        meta['sentence_offset'] = meta.apply(lambda x: True if x['word_id'] > x['next_word_id'] else False, axis=1)\n",
    "        meta['sentence_offset'].fillna(False, inplace=True)\n",
    "        meta.drop('next_word_id', axis=1, inplace=True)\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: True if x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1 else False, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "\n",
    "        # Const stop\n",
    "        meta['next_closing'] = meta['n_closing'].shift(-1)\n",
    "        meta['constituent_offset'] = meta.apply(lambda x: True if x['n_closing'] > x['next_closing'] else False, axis=1)\n",
    "        meta['constituent_offset'].fillna(False, inplace=True)\n",
    "        meta.drop('next_closing', axis=1, inplace=True)\n",
    "\n",
    "        for start in ('onset', 'offset'): \n",
    "            # for level in ('word', 'constituent', 'sentence'):\n",
    "            for level in ('sentence', 'constituent', 'word'):\n",
    "                # Select only the rows containing the True for the conditions (sentence_end, etc..)\n",
    "                sel = meta.query(f'{level}_{start}==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "                # TODO check variance as well for sentences\n",
    "                # Matchlist events and meta\n",
    "                # So that we can epoch now that's we've sliced our metadata\n",
    "                i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "                sel = sel.reset_index().loc[j]\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw), decim = 10,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop', # check event repeated\n",
    "                                            preload=True)  # n_words OR n_constitutent OR n_sentences\n",
    "                epoch_key = f'{level}_{start}'\n",
    "            \n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "        \n",
    "            \n",
    "    # Once we have the dict of epochs per condition full (epoching for each run for a subject)\n",
    "    # we can concatenate them, and fix the dev_head             \n",
    "    for start_ in ('onset', 'offset'): \n",
    "        for level_ in ('word', 'constituent', 'sentence'):\n",
    "            epoch_key = f'{level_}_{start_}'\n",
    "            all_epochs_chosen = dict_epochs[epoch_key]\n",
    "            # Concatenate epochs\n",
    "            for epo in all_epochs_chosen:\n",
    "                epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "            dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n",
    "\n",
    "    # Now that we have all the epochs, rerun the plotting / decoding on averaged\n",
    "    for start in ('onset', 'offset'): \n",
    "            for level in ('word', 'constituent', 'sentence'):  \n",
    "                epoch_key = f'{level}_{start}'\n",
    "                epochs = dict_epochs[epoch_key]\n",
    "                # mean\n",
    "                evo = epochs.copy().pick_types(meg=True).average(method='median')\n",
    "                all_evos.append(dict(subject=subject, evo=evo, start=start, level=level))\n",
    "\n",
    "\n",
    "                # decoding word emb\n",
    "                epochs = epochs.load_data().pick_types(meg=True, stim=False, misc=False)\n",
    "                X = epochs.get_data()\n",
    "                embeddings = epochs.metadata.word.apply(lambda word: nlp(word).vector).values\n",
    "                embeddings = np.array([emb for emb in embeddings])\n",
    "                R_vec = decod_xy(X, embeddings)\n",
    "                scores = np.mean(R_vec, axis=1)\n",
    "\n",
    "                for t, score in enumerate(scores):\n",
    "                    all_scores.append(dict(subject=subject, score=score, start=start, level=level, t=epochs.times[t]))\n",
    "\n",
    "    all_scores.to_csv(f'./score_{sub}.csv')\n",
    "    all_evos.to_csv(f'./evos_{sub}.csv')\n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f46b226",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/data_path.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m executor \u001b[38;5;241m=\u001b[39m submitit\u001b[38;5;241m.\u001b[39mAutoExecutor(folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m executor\u001b[38;5;241m.\u001b[39mupdate_parameters(slurm_partition\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu_p1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#gpu_p2\u001b[39;00m\n\u001b[1;32m     11\u001b[0m                            cpus_per_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \u001b[38;5;66;03m#10\u001b[39;00m\n\u001b[1;32m     12\u001b[0m                            timeout_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, \u001b[38;5;66;03m#15 min for each checkpoint 700\u001b[39;00m\n\u001b[1;32m     13\u001b[0m                            account\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqtr@cpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLPP_read\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m subjects \u001b[38;5;241m=\u001b[39m get_subjects(path)\n",
      "File \u001b[0;32m~/git/neurospin-petit-prince/decoding/cluster/dataset.py:197\u001b[0m, in \u001b[0;36mget_path\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_path\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLPP_read\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    196\u001b[0m     path_file \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./../../data/data_path.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    198\u001b[0m         data \u001b[38;5;241m=\u001b[39m Path(f\u001b[38;5;241m.\u001b[39mreadlines()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLPP_read\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;66;03m# TASK = \"read\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/data_path.txt'"
     ]
    }
   ],
   "source": [
    "import submitit\n",
    "from dataset import read_raw\n",
    "\n",
    "# Define the function that will be run in parallel:\n",
    "# For each subject, we can decode from 6 sub-epoched sets\n",
    "\n",
    "## Create the submitit job\n",
    "# Using Jean Zay parameters\n",
    "executor = submitit.AutoExecutor(folder=\"logs\")\n",
    "executor.update_parameters(slurm_partition=\"cpu_p1\", #gpu_p2\n",
    "                           cpus_per_task=32, #10\n",
    "                           timeout_min=30, #15 min for each checkpoint 700\n",
    "                           account=\"qtr@cpu\") #\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "task = \"read\"\n",
    "subjects = get_subjects(path)\n",
    "\n",
    "jobs = executor.map_array(analysis, subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44738eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for job in jobs:\n",
    "    if job.state == 'COMPLETED':\n",
    "        results.append(job.results()[0])\n",
    "\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "task = Task()\n",
    "jobs = []\n",
    "with executor.batch():\n",
    "    for model in models:\n",
    "        job = executor.submit(task, model)\n",
    "        jobs.append(job)\n",
    "submitit.helpers.monitor_jobs(jobs)\n",
    "outputs = [job.result() for job in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "times = np.linspace(-.2, 1.5, 86)\n",
    "for key in results.keys():\n",
    "    plt.fill_between(times, np.mean(results[key].values, 0), alpha=.5, label=key)\n",
    "plt.axhline(0, color='k', ls=':')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9822ac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mlaser_embeddings\u001b[0m/  \u001b[01;34mphonemes\u001b[0m/  \u001b[01;34msyntax_new_but_wrong_txt\u001b[0m/  \u001b[01;34mtxt_raw\u001b[0m/\r\n",
      "origin.txt         \u001b[01;34msyntax\u001b[0m/    \u001b[01;34mtxt_laser\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0388ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting submitit\n",
      "  Downloading submitit-1.4.5-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.1/73.1 KB\u001b[0m \u001b[31m862.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=3.7.4.2 in /home/cb271805/.pyenv/versions/base/lib/python3.10/site-packages (from submitit) (4.4.0)\n",
      "Collecting cloudpickle>=1.2.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: cloudpickle, submitit\n",
      "Successfully installed cloudpickle-2.2.1 submitit-1.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install submitit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979d2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
