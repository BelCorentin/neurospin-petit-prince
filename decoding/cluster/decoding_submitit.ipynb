{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea15eb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataset import read_raw, get_subjects, get_path, mne_events\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "\n",
    "def analysis(subject):\n",
    "    all_evos = []\n",
    "    all_scores = []\n",
    "    runs=9\n",
    "    epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "    path = get_path(\"LPP_read\")\n",
    "    task = \"read\"\n",
    "            \n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    # Dict init\n",
    "    for start in ('onset', 'offset'): \n",
    "            for level in ('word', 'constituent', 'sentence'):\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "\n",
    "        # Word end\n",
    "        meta['word_offset'] = True\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Sent stop\n",
    "        meta['next_word_id'] = meta['word_id'].shift(-1)\n",
    "        meta['sentence_offset'] = meta.apply(lambda x: True if x['word_id'] > x['next_word_id'] else False, axis=1)\n",
    "        meta['sentence_offset'].fillna(False, inplace=True)\n",
    "        meta.drop('next_word_id', axis=1, inplace=True)\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: True if x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1 else False, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "\n",
    "        # Const stop\n",
    "        meta['next_closing'] = meta['n_closing'].shift(-1)\n",
    "        meta['constituent_offset'] = meta.apply(lambda x: True if x['n_closing'] > x['next_closing'] else False, axis=1)\n",
    "        meta['constituent_offset'].fillna(False, inplace=True)\n",
    "        meta.drop('next_closing', axis=1, inplace=True)\n",
    "\n",
    "        for start in ('onset', 'offset'): \n",
    "            # for level in ('word', 'constituent', 'sentence'):\n",
    "            for level in ('sentence', 'constituent', 'word'):\n",
    "                # Select only the rows containing the True for the conditions (sentence_end, etc..)\n",
    "                sel = meta.query(f'{level}_{start}==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "                # TODO check variance as well for sentences\n",
    "                # Matchlist events and meta\n",
    "                # So that we can epoch now that's we've sliced our metadata\n",
    "                i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "                sel = sel.reset_index().loc[j]\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw), decim = 10,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop', # check event repeated\n",
    "                                            preload=True)  # n_words OR n_constitutent OR n_sentences\n",
    "                epoch_key = f'{level}_{start}'\n",
    "            \n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "        \n",
    "            \n",
    "    # Once we have the dict of epochs per condition full (epoching for each run for a subject)\n",
    "    # we can concatenate them, and fix the dev_head             \n",
    "    for start_ in ('onset', 'offset'): \n",
    "        for level_ in ('word', 'constituent', 'sentence'):\n",
    "            epoch_key = f'{level_}_{start_}'\n",
    "            all_epochs_chosen = dict_epochs[epoch_key]\n",
    "            # Concatenate epochs\n",
    "            for epo in all_epochs_chosen:\n",
    "                epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "            dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n",
    "\n",
    "    # Now that we have all the epochs, rerun the plotting / decoding on averaged\n",
    "    for start in ('onset', 'offset'): \n",
    "            for level in ('word', 'constituent', 'sentence'):  \n",
    "                epoch_key = f'{level}_{start}'\n",
    "                epochs = dict_epochs[epoch_key]\n",
    "                # mean\n",
    "                evo = epochs.copy().pick_types(meg=True).average(method='median')\n",
    "                all_evos.append(dict(subject=subject, evo=evo, start=start, level=level))\n",
    "\n",
    "\n",
    "                # decoding word emb\n",
    "                epochs = epochs.load_data().pick_types(meg=True, stim=False, misc=False)\n",
    "                X = epochs.get_data()\n",
    "                embeddings = epochs.metadata.word.apply(lambda word: nlp(word).vector).values\n",
    "                embeddings = np.array([emb for emb in embeddings])\n",
    "                R_vec = decod_xy(X, embeddings)\n",
    "                scores = np.mean(R_vec, axis=1)\n",
    "\n",
    "                for t, score in enumerate(scores):\n",
    "                    all_scores.append(dict(subject=subject, score=score, start=start, level=level, t=epochs.times[t]))\n",
    "\n",
    "    all_scores.to_csv(f'./score_{subject}.csv')\n",
    "    all_evos.to_csv(f'./evos_{subject}.csv')\n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46b226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is153802/.pyenv/versions/meg-masc/lib/python3.10/site-packages/submitit/auto/auto.py:23: UserWarning: Setting 'account' is deprecated. Use 'slurm_account' instead.\n",
      "  warnings.warn(f\"Setting '{arg}' is deprecated. Use '{new_arg}' instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring 22 jobs from job arrays 200907, 200940, 201012, 201084, 201156, 201228, 201302, 201374, 201446, 201518, 201591, 201663, 201735, 201807, 201879, 201951, 202023, 202095, 202167, 202239, 202311, 202383 \n",
      "\n",
      "[2023-04-25 13:35:57] Launched 0 minutes ago, 22/22 jobs running,  0/22 jobs failed,  0/22 jobs done\n",
      "[2023-04-25 13:36:27] Launched 0 minutes ago, 22/22 jobs running,  0/22 jobs failed,  0/22 jobs done\n",
      "[2023-04-25 13:36:57] Launched 1 minutes ago, 22/22 jobs running,  0/22 jobs failed,  0/22 jobs done\n",
      "[2023-04-25 13:37:27] Launched 1 minutes ago, 22/22 jobs running,  0/22 jobs failed,  0/22 jobs done\n",
      "[2023-04-25 13:37:57] Launched 2 minutes ago, 22/22 jobs running,  0/22 jobs failed,  0/22 jobs done\n",
      "[2023-04-25 13:38:27] Launched 2 minutes ago, 22/22 jobs running,  0/22 jobs failed,  0/22 jobs done\n",
      "[2023-04-25 13:38:57] Launched 3 minutes ago, 22/22 jobs running,  0/22 jobs failed,  0/22 jobs done\n",
      "[2023-04-25 13:39:27] Launched 3 minutes ago, 22/22 jobs running,  0/22 jobs failed,  0/22 jobs done\n",
      "[2023-04-25 13:39:57] Launched 4 minutes ago, 22/22 jobs running,  0/22 jobs failed,  0/22 jobs done\n",
      "[2023-04-25 13:40:27] Launched 4 minutes ago, 22/22 jobs running,  0/22 jobs failed,  0/22 jobs done\n",
      "[2023-04-25 13:40:57] Launched 5 minutes ago, 21/22 jobs running,  0/22 jobs failed,  1/22 jobs done\n",
      "[2023-04-25 13:41:27] Launched 5 minutes ago, 21/22 jobs running,  0/22 jobs failed,  1/22 jobs done\n",
      "[2023-04-25 13:41:57] Launched 6 minutes ago, 21/22 jobs running,  0/22 jobs failed,  1/22 jobs done\n",
      "[2023-04-25 13:42:27] Launched 6 minutes ago, 21/22 jobs running,  0/22 jobs failed,  1/22 jobs done\n",
      "[2023-04-25 13:42:58] Launched 7 minutes ago, 21/22 jobs running,  0/22 jobs failed,  1/22 jobs done\n",
      "[2023-04-25 13:43:28] Launched 7 minutes ago, 21/22 jobs running,  0/22 jobs failed,  1/22 jobs done\n",
      "[2023-04-25 13:43:58] Launched 8 minutes ago, 21/22 jobs running,  0/22 jobs failed,  1/22 jobs done\n",
      "[2023-04-25 13:44:28] Launched 8 minutes ago, 21/22 jobs running,  0/22 jobs failed,  1/22 jobs done\n"
     ]
    }
   ],
   "source": [
    "import submitit\n",
    "from dataset import read_raw\n",
    "\n",
    "# Define the function that will be run in parallel:\n",
    "# For each subject, we can decode from 6 sub-epoched sets\n",
    "\n",
    "## Create the submitit job\n",
    "# Using Jean Zay parameters\n",
    "executor = submitit.AutoExecutor(folder='logs')\n",
    "executor.update_parameters(slurm_partition=\"cpu_p1\", #gpu_p2\n",
    "                           cpus_per_task=32, #10\n",
    "                           timeout_min=60, #15 min for each checkpoint 700\n",
    "                           account=\"qtr@cpu\",\n",
    "                           hint='nomultithread') #\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "task = \"read\"\n",
    "subjects = get_subjects(path)\n",
    "\n",
    "jobs = executor.map_array(analysis, subjects)\n",
    "submitit.helpers.monitor_jobs(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44738eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for job in jobs:\n",
    "    if job.state == 'COMPLETED':\n",
    "        results.append(job.results()[0])\n",
    "\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "task = Task()\n",
    "jobs = []\n",
    "with executor.batch():\n",
    "    for model in models:\n",
    "        job = executor.submit(task, model)\n",
    "        jobs.append(job)\n",
    "submitit.helpers.monitor_jobs(jobs)\n",
    "outputs = [job.result() for job in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "times = np.linspace(-.2, 1.5, 86)\n",
    "for key in results.keys():\n",
    "    plt.fill_between(times, np.mean(results[key].values, 0), alpha=.5, label=key)\n",
    "plt.axhline(0, color='k', ls=':')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9822ac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mlaser_embeddings\u001b[0m/  \u001b[01;34mphonemes\u001b[0m/  \u001b[01;34msyntax_new_but_wrong_txt\u001b[0m/  \u001b[01;34mtxt_raw\u001b[0m/\r\n",
      "origin.txt         \u001b[01;34msyntax\u001b[0m/    \u001b[01;34mtxt_laser\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0388ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting submitit\n",
      "  Downloading submitit-1.4.5-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.1/73.1 KB\u001b[0m \u001b[31m862.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=3.7.4.2 in /home/cb271805/.pyenv/versions/base/lib/python3.10/site-packages (from submitit) (4.4.0)\n",
      "Collecting cloudpickle>=1.2.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: cloudpickle, submitit\n",
      "Successfully installed cloudpickle-2.2.1 submitit-1.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install submitit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979d2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
