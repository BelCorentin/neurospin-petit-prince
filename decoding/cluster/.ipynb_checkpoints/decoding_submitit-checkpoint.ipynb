{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "485ef3c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataset import read_raw, get_subjects, get_path, mne_events\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "\n",
    "def analysis(subject):\n",
    "    all_evos = []\n",
    "    all_scores = []\n",
    "    runs=9\n",
    "    epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "    path = get_path(\"LPP_read\")\n",
    "    task = \"read\"\n",
    "            \n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    # Dict init\n",
    "    for start in ('onset', 'offset'): \n",
    "            for level in ('word', 'constituent', 'sentence'):\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "\n",
    "        # Word end\n",
    "        meta['word_offset'] = True\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Sent stop\n",
    "        meta['next_word_id'] = meta['word_id'].shift(-1)\n",
    "        meta['sentence_offset'] = meta.apply(lambda x: True if x['word_id'] > x['next_word_id'] else False, axis=1)\n",
    "        meta['sentence_offset'].fillna(False, inplace=True)\n",
    "        meta.drop('next_word_id', axis=1, inplace=True)\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: True if x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1 else False, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "\n",
    "        # Const stop\n",
    "        meta['next_closing'] = meta['n_closing'].shift(-1)\n",
    "        meta['constituent_offset'] = meta.apply(lambda x: True if x['n_closing'] > x['next_closing'] else False, axis=1)\n",
    "        meta['constituent_offset'].fillna(False, inplace=True)\n",
    "        meta.drop('next_closing', axis=1, inplace=True)\n",
    "\n",
    "        for start in ('onset', 'offset'): \n",
    "            # for level in ('word', 'constituent', 'sentence'):\n",
    "            for level in ('sentence', 'constituent', 'word'):\n",
    "                # Select only the rows containing the True for the conditions (sentence_end, etc..)\n",
    "                sel = meta.query(f'{level}_{start}==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "                # TODO check variance as well for sentences\n",
    "                # Matchlist events and meta\n",
    "                # So that we can epoch now that's we've sliced our metadata\n",
    "                i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "                sel = sel.reset_index().loc[j]\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw), decim = 10,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop', # check event repeated\n",
    "                                            preload=True)  # n_words OR n_constitutent OR n_sentences\n",
    "                epoch_key = f'{level}_{start}'\n",
    "            \n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "        \n",
    "            \n",
    "    # Once we have the dict of epochs per condition full (epoching for each run for a subject)\n",
    "    # we can concatenate them, and fix the dev_head             \n",
    "    for start_ in ('onset', 'offset'): \n",
    "        for level_ in ('word', 'constituent', 'sentence'):\n",
    "            epoch_key = f'{level_}_{start_}'\n",
    "            all_epochs_chosen = dict_epochs[epoch_key]\n",
    "            # Concatenate epochs\n",
    "            for epo in all_epochs_chosen:\n",
    "                epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "            dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n",
    "\n",
    "    # Now that we have all the epochs, rerun the plotting / decoding on averaged\n",
    "    for start in ('onset', 'offset'): \n",
    "            for level in ('word', 'constituent', 'sentence'):  \n",
    "                epoch_key = f'{level}_{start}'\n",
    "                epochs = dict_epochs[epoch_key]\n",
    "                # mean\n",
    "                evo = epochs.copy().pick_types(meg=True).average(method='median')\n",
    "                all_evos.append(dict(subject=subject, evo=evo, start=start, level=level))\n",
    "\n",
    "\n",
    "                # decoding word emb\n",
    "                epochs = epochs.load_data().pick_types(meg=True, stim=False, misc=False)\n",
    "                X = epochs.get_data()\n",
    "                embeddings = epochs.metadata.word.apply(lambda word: nlp(word).vector).values\n",
    "                embeddings = np.array([emb for emb in embeddings])\n",
    "                R_vec = decod_xy(X, embeddings)\n",
    "                scores = np.mean(R_vec, axis=1)\n",
    "\n",
    "                for t, score in enumerate(scores):\n",
    "                    all_scores.append(dict(subject=subject, score=score, start=start, level=level, t=epochs.times[t]))\n",
    "\n",
    "    all_scores.to_csv(f'./score_{sub}.csv')\n",
    "    all_evos.to_csv(f'./evos_{sub}.csv')\n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05316cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import submitit\n",
    "from dataset import read_raw\n",
    "\n",
    "# Define the function that will be run in parallel:\n",
    "# For each subject, we can decode from 6 sub-epoched sets\n",
    "\n",
    "## Create the submitit job\n",
    "# Using Jean Zay parameters\n",
    "executor = submitit.AutoExecutor(folder=\"logs\")\n",
    "executor.update_parameters(slurm_partition=\"cpu_p1\", #gpu_p2\n",
    "                           cpus_per_task=32, #10\n",
    "                           timeout_min=30, #15 min for each checkpoint 700\n",
    "                           account=\"qtr@cpu\") #\n",
    "\n",
    "\n",
    "subjects = get_subjects(path)\n",
    "\n",
    "jobs = executor.map_array(analysis, subjects)\n",
    "\n",
    "results = []\n",
    "for job in jobs:\n",
    "    if job.state == 'COMPLETED':\n",
    "        results.append(job.results()[0])\n",
    "\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "times = np.linspace(-.2, 1.5, 86)\n",
    "for key in results.keys():\n",
    "    plt.fill_between(times, np.mean(results[key].values, 0), alpha=.5, label=key)\n",
    "plt.axhline(0, color='k', ls=':')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "class Task:\n",
    "    def __call__(self,model):\n",
    "\n",
    "        print(\"exporting PyTorch distributed environment variables\")\n",
    "        dist_env = submitit.helpers.TorchDistributedEnvironment().export(set_cuda_visible_devices=False)\n",
    "        print(f\"master: {dist_env.master_addr}:{dist_env.master_port}\")\n",
    "        print(f\"rank: {dist_env.rank}\")\n",
    "        print(f\"world size: {dist_env.world_size}\")\n",
    "        print(f\"local rank: {dist_env.local_rank}\")\n",
    "        print(f\"local world size: {dist_env.local_world_size}\")\n",
    "\n",
    "        model_dir = os.path.join(args_class.modelpath, model)\n",
    "        if args_class.save is None:\n",
    "            save_dir = os.path.join(\"outputs\", model)\n",
    "        else:\n",
    "            save_dir = args_class.save\n",
    "\n",
    "        if \"LOCAL_RANK\" in os.environ.keys():\n",
    "            local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "            torch.cuda.set_device(local_rank)\n",
    "            # We don't need to set the specific environment but assign the right gpu\n",
    "            # to this process by setting the local_rank\n",
    "\n",
    "        ## Load the set of checkpoint names\n",
    "        chkList = os.listdir(model_dir)\n",
    "        lmodels = list(filter(lambda e: e.__contains__(\"checkpoint\"), chkList))\n",
    "        lmodels = np.array(lmodels)[np.argsort(np.array([int(l.split(\"checkpoint-\")[1]) for l in lmodels]))]\n",
    "        chkList = np.array([\"initialmodel\"] + list(lmodels))\n",
    "        nb_total_checkpoint = len(chkList)\n",
    "        chunk_size = np.ceil(nb_total_checkpoint / (args_class.nb_nodes * args_class.nb_gpu))\n",
    "\n",
    "        postAnalyzer = PostAnalyzer(dataset_names, model_dir, save_dir,\n",
    "                                    nb_total_checkpoint, chunk_size)\n",
    "        # The checkpoints are a function of the total rank in the set of subprocess.\n",
    "        checkpoints_id = np.arange(int(dist_env.rank)*chunk_size,\n",
    "                                   int(np.min([(int(dist_env.rank)+1)*chunk_size,nb_total_checkpoint])))\n",
    "        print(\"checkpoint managed by this process:\")\n",
    "        print(checkpoints_id)\n",
    "\n",
    "        checkpoints_names = np.array(chkList)[np.array(checkpoints_id,dtype=int)]\n",
    "\n",
    "        postAnalyzer.multiprocess_init(np.array(checkpoints_id,dtype=int), checkpoints_names)\n",
    "\n",
    "        print(\"extraction of quantize codevectors\")\n",
    "        postAnalyzer.save_quantize()\n",
    "        return True\n",
    "\n",
    "    def checkpoint(self):\n",
    "        print(\"checkpointing\")\n",
    "        return submitit.helpers.DelayedSubmission(self)\n",
    "\n",
    "task = Task()\n",
    "jobs = []\n",
    "with executor.batch():\n",
    "    for model in models:\n",
    "        job = executor.submit(task, model)\n",
    "        jobs.append(job)\n",
    "submitit.helpers.monitor_jobs(jobs)\n",
    "outputs = [job.result() for job in jobs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
