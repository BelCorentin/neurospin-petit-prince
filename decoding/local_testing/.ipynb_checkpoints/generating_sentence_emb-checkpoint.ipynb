{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aebc7b4",
   "metadata": {},
   "source": [
    "# New Approach:\n",
    "\n",
    "For both:\n",
    "- Sentence embeddings\n",
    "- Constituent embeddings\n",
    "\n",
    "Generate the embedding by iterating through them, instead of generating it from the whole txt file and chunking after.\n",
    "\n",
    "In order to get the right chunking:\n",
    "- get the metadata format from a normal analysis,\n",
    "- get the frontiers of constituents / sentences from it, and generate the embeddings from there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eec524",
   "metadata": {},
   "source": [
    "## Testing metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435880e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw files for modality: auditory\n",
      "\n",
      " Epoching for run 1, subject: 3\n",
      "\n",
      "Opening raw data file /home/is153802/data/LPP_MEG_auditory/sub-3/ses-01/meg/sub-3_ses-01_task-listen_run-01_meg.fif...\n",
      "    Read a total of 13 projection items:\n",
      "        grad_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v6 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v7 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v8 (1 x 306)  idle\n",
      "    Range : 28000 ... 658999 =     28.000 ...   658.999 secs\n",
      "Ready.\n",
      "Reading events from /home/is153802/data/LPP_MEG_auditory/sub-3/ses-01/meg/sub-3_ses-01_task-listen_run-01_events.tsv.\n",
      "Reading channel info from /home/is153802/data/LPP_MEG_auditory/sub-3/ses-01/meg/sub-3_ses-01_task-listen_run-01_channels.tsv.\n",
      "Using 4 HPI coils: 293 307 314 321 Hz\n",
      "Not fully anonymizing info - keeping his_id, sex, and hand info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:53: RuntimeWarning: This file contains raw Internal Active Shielding data. It may be distorted. Elekta recommends it be run through MaxFilter to produce reliable results. Consider closing the file and running MaxFilter on the data.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:53: RuntimeWarning: The unit for channel(s) STI001, STI002, STI003, STI004, STI005, STI006, STI007, STI008, STI009, STI010, STI011, STI012, STI013, STI014, STI015, STI016, STI101, STI201, STI301 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1954 events found\n",
      "Event IDs: [  1 129]\n",
      "Reading 0 ... 630999  =      0.000 ...   630.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 20 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 20.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 306 out of 306 | elapsed:    7.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 27 columns\n",
      "1597 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1597 events and 1301 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 27 columns\n",
      "484 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 484 events and 2501 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 27 columns\n",
      "155 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 155 events and 5001 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 27 columns\n",
      "1597 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1597 events and 1301 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 27 columns\n",
      "484 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 484 events and 2501 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 27 columns\n",
      "155 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 155 events and 5001 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "Reading raw files for modality: auditory\n",
      "\n",
      " Epoching for run 2, subject: 3\n",
      "\n",
      "Opening raw data file /home/is153802/data/LPP_MEG_auditory/sub-3/ses-01/meg/sub-3_ses-01_task-listen_run-02_meg.fif...\n",
      "    Read a total of 13 projection items:\n",
      "        grad_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v6 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v7 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v8 (1 x 306)  idle\n",
      "    Range : 120000 ... 772999 =    120.000 ...   772.999 secs\n",
      "Ready.\n",
      "Reading events from /home/is153802/data/LPP_MEG_auditory/sub-3/ses-01/meg/sub-3_ses-01_task-listen_run-02_events.tsv.\n",
      "Reading channel info from /home/is153802/data/LPP_MEG_auditory/sub-3/ses-01/meg/sub-3_ses-01_task-listen_run-02_channels.tsv.\n",
      "Using 4 HPI coils: 293 307 314 321 Hz\n",
      "Not fully anonymizing info - keeping his_id, sex, and hand info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:53: RuntimeWarning: This file contains raw Internal Active Shielding data. It may be distorted. Elekta recommends it be run through MaxFilter to produce reliable results. Consider closing the file and running MaxFilter on the data.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:53: RuntimeWarning: The unit for channel(s) STI001, STI002, STI003, STI004, STI005, STI006, STI007, STI008, STI009, STI010, STI011, STI012, STI013, STI014, STI015, STI016, STI101, STI201, STI301 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1769 events found\n",
      "Event IDs: [  1 129]\n",
      "Reading 0 ... 652999  =      0.000 ...   652.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 20 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 20.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 306 out of 306 | elapsed:    9.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 27 columns\n",
      "1765 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1765 events and 1301 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 27 columns\n",
      "510 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 510 events and 2501 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 27 columns\n",
      "141 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 141 events and 5001 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 27 columns\n",
      "1765 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1765 events and 1301 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 27 columns\n",
      "510 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 510 events and 2501 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 27 columns\n",
      "141 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 141 events and 5001 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_226103/2415584108.py:95: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "Adding metadata with 27 columns\n",
      "6720 matching events found\n",
      "No baseline correction applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:117: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n",
      "/tmp/ipykernel_226103/2415584108.py:117: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 27 columns\n",
      "1984 matching events found\n",
      "No baseline correction applied\n",
      "Adding metadata with 27 columns\n",
      "588 matching events found\n",
      "No baseline correction applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226103/2415584108.py:117: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Concatenate epochs\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epo \u001b[38;5;129;01min\u001b[39;00m all_epochs_chosen:\n\u001b[0;32m--> 115\u001b[0m     epo\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev_head_t\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_epochs_chosen[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev_head_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    117\u001b[0m dict_epochs[epoch_key] \u001b[38;5;241m=\u001b[39m mne\u001b[38;5;241m.\u001b[39mconcatenate_epochs(all_epochs_chosen)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "from dataset import read_raw, get_subjects, get_path\n",
    "from utils import decod_xy, mne_events\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "\n",
    "modality = \"auditory\"\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "all_evos = []\n",
    "all_scores = []\n",
    "path = get_path(modality)\n",
    "subjects = get_subjects(path)\n",
    "runs = 2\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "levels = ('word','constituent','sentence')\n",
    "starts = ('onset', 'offset')\n",
    "      \n",
    "# Iterate on subjects to epochs, and mean later\n",
    "for subject in subjects[2:3]:\n",
    "    \n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    \n",
    "    # Initialization of the dictionary\n",
    "    for start in starts: \n",
    "            for level in levels:\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "                \n",
    "    # Iterating on runs, building the metadata and re-epoching\n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True, modality=modality)\n",
    "        meta = meta_.copy()\n",
    "        \n",
    "        # Metadata update\n",
    "        meta['word_onset'] = True\n",
    "        meta['word_stop'] = meta.start + meta.duration\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sentence stop info\n",
    "        meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "        for s, d in meta.groupby('sentence_id'):\n",
    "            meta.loc[d.index, 'sent_word_id'] = range(len(d))\n",
    "            meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'sentence_stop'] = d.start.max()\n",
    "            \n",
    "        # Adding the constituents stop info\n",
    "        meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "        for s, d in meta.groupby('constituent_id'):\n",
    "            meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'constituent_stop'] = d.start.max()\n",
    "            meta.loc[d.index, 'const_word_id'] = range(len(d))\n",
    "\n",
    "        for start in starts: \n",
    "            for level in levels:\n",
    "                # Select only the rows containing the True for the conditions\n",
    "                # Simplified to only get for the onset: sentence onset epochs, constituent onset epochs,etc\n",
    "                start = 'onset' # DEBUG\n",
    "                sel = meta.query(f'{level}_{start}==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "                \n",
    "                # Do we need to do that ???\n",
    "                \"\"\"\n",
    "                # Matchlist events and meta\n",
    "                # So that we can epoch now that's we've sliced our metadata\n",
    "                if modality == 'auditory':\n",
    "                    word_events = events[events[:, 2] > 1]\n",
    "                    meg_delta = np.round(np.diff(word_events[:, 0]/raw.info['sfreq']))\n",
    "                    meta_delta = np.round(np.diff(sel.onset.values))\n",
    "                    i, j = match_list(meg_delta, meta_delta)\n",
    "\n",
    "                # For auditory, we match on the time difference between triggers\n",
    "                elif modality == \"visual\":\n",
    "\n",
    "                    i, j = match_list(events[:, 2], sel.wlength)\n",
    "                    assert len(i) > (0.9 * len(events))\n",
    "                    assert (events[i, 2] == sel.loc[j].wlength).mean() > 0.95\n",
    "                sel = sel.reset_index().loc[j]\n",
    "                # Making sure there is not hidden bug when matching\n",
    "                assert sel.shape[0] > 0.5 *  (meta.query(f'{level}_onset==True')).shape[0]\n",
    "                \"\"\"\n",
    "                \n",
    "                # Epoching from the metadata having all onset events: if the start=Offset, the mne events\n",
    "                # Function will epoch on the offset of each level instead of the onset\n",
    "                # TODO: add adaptative baseline\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop',\n",
    "                                            preload=True,\n",
    "                                                baseline=None)\n",
    "                epoch_key = f'{level}_{start}'\n",
    "\n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "            \n",
    "    # Once we have the dict of epochs per condition full (epoching for each run for a subject)\n",
    "    # we can concatenate them, and fix the dev_head             \n",
    "    for start_ in starts: \n",
    "        for level_ in levels:\n",
    "            start_ = 'onset' # DEBUG\n",
    "            epoch_key = f'{level_}_{start_}'\n",
    "            all_epochs_chosen = dict_epochs[epoch_key]\n",
    "            # Concatenate epochs\n",
    "\n",
    "            for epo in all_epochs_chosen:\n",
    "                epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "            dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fafd9e",
   "metadata": {},
   "source": [
    "## LASER embeddings for sentences (easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b0ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it a function:\n",
    "from dataset import get_code_path\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def add_embeddings(meta, run, level):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function made to generate laser embeddings, store them,\n",
    "    and add them to the metadata \n",
    "\n",
    "    Does so for both constituent embeddings, and sentence ones\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Parse the metadata into constituents / sentences, \n",
    "    # and generate txt files for each constituents / sentence\n",
    "    # So that it can be parsed by LASER\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    meta['const_end'] = meta.constituent_onset.shift(-1)\n",
    "    for index, row in meta.iterrows():\n",
    "\n",
    "        # Append word to current sentence \n",
    "        current_sentence.append(row['word'])\n",
    "\n",
    "        # Check if end of sentence \n",
    "        if level == 'sentence' and row['is_last_word']:\n",
    "            # Join words into sentence string and append to list\n",
    "            sentences.append(' '.join(current_sentence)) \n",
    "            # Reset current sentence   \n",
    "            current_sentence = []\n",
    "            \n",
    "        if level == 'constituent' and row['const_end']:\n",
    "            # Join words into sentence string and append to list\n",
    "            sentences.append(' '.join(current_sentence)) \n",
    "            # Reset current sentence   \n",
    "            current_sentence = []\n",
    "\n",
    "    # Loop through sentences \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Get sentence number\n",
    "        sentence_num = i + 1\n",
    "\n",
    "        # Create file name\n",
    "        file_name = f'./embeds/txt/run{run}_{level}_{sentence_num}.txt'\n",
    "\n",
    "        # Open text file \n",
    "        with open(file_name, 'w') as f:\n",
    "            # Write sentence to file\n",
    "            f.write(sentence)\n",
    "            \n",
    "    # Run LASER using the run number\n",
    "    path = Path('/home/is153802/github/LASER/tasks/embed')\n",
    "    %env LASER=/home/is153802/github/LASER\n",
    "\n",
    "    for i, _ in enumerate(sentences):\n",
    "    # Get sentence number\n",
    "        sentence_num = i + 1\n",
    "\n",
    "        txt_file = f\"/home/is153802/code/decoding/local_testing/embeds/txt/run{run}_{level}_{sentence_num}.txt\"\n",
    "        emb_file = f\"/home/is153802/code/decoding/local_testing/embeds/emb/run{run}_{level}_{sentence_num}.bin\"\n",
    "        if os.path.exists(emb_file):\n",
    "            continue\n",
    "        else:\n",
    "            !bash /home/is153802/github/LASER/tasks/embed/embed.sh {txt_file} {emb_file}\n",
    "        \n",
    "    # Get the embeddings from the generated txt file, and add them to metadata\n",
    "    dim = 1024\n",
    "    embeddings = {}\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        embeds = np.fromfile(\n",
    "                f\"{get_code_path()}/decoding/local_testing/embeds/emb/run{run}_{level}_{index+1}.bin\",\n",
    "                dtype=np.float32,\n",
    "                count=-1,\n",
    "                )\n",
    "        embeds.resize(embeds.shape[0] // dim, dim)\n",
    "        embeds = embeds.reshape(-1)\n",
    "        embeddings[index] = embeds\n",
    "    sent_index = 0\n",
    "    embed_arrays = []\n",
    "    for index, row in meta.iterrows():\n",
    "        embed_arrays.append(embeddings[sent_index])\n",
    "        # Check if end of sentence \n",
    "        if row['is_last_word']:\n",
    "            sent_index += 1\n",
    "\n",
    "    meta['embed'] = embed_arrays\n",
    "    \n",
    "    return meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d90e41",
   "metadata": {},
   "source": [
    "# Previous Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7019f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: generate the run{i}.txt file to input to LASER\n",
    "\n",
    "# What was done previously: chunk the txt file raw by actual sentence (based on ., ?, !, etc..)\n",
    "# Problem: the metadata in epochs (sentence_end calculated using the word onset difference) doesn't match, as there are\n",
    "# Offsets that happen sometimes not at the end of sentences\n",
    "\n",
    "# Solution: temporary: generate the line chunking for LASER by word onset difference from the metadata file\n",
    "# Final: it will only work for read modality: for audio, an option could be to replicate the metadata file\n",
    "# => supposing the shape of both metadata files are the same, we can add the sentence_end column to the audio one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Open the events files to get the metadata, and then generate the txt file from there\n",
    "for run in np.arange(1,10):\n",
    "\n",
    "    file = f'/home/co/data/BIDS_lecture/sub-{sub}/ses-01/meg/sub-{sub}_ses-01_task-read_run-0{run}_events.tsv'\n",
    "\n",
    "\n",
    "\n",
    "    # Load the TSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "\n",
    "    # Keep track of the previous onset value\n",
    "    prev_onset = None\n",
    "\n",
    "    # Open the output file for writing\n",
    "    with open(f'run{run}.txt', 'w') as output_file:\n",
    "\n",
    "        # Loop through each row in the DataFrame\n",
    "        for i, row in df.iterrows():\n",
    "\n",
    "            # Get the onset value for this row\n",
    "            onset = row['onset']\n",
    "\n",
    "            # If this is the first row, or the onset difference with the previous row is less than 0.7, append the current column to the output\n",
    "            if ((row.word).__contains__(\".\")\n",
    "                or (row.word).__contains__(\"?\")\n",
    "                or (row.word).__contains__(\"!\")):\n",
    "                output_file.write(row['word'] +'\\n')\n",
    "                \n",
    "\n",
    "            # Otherwise, start a new line in the output file\n",
    "            else:\n",
    "                \n",
    "                output_file.write(row['word'] + ' ')\n",
    "\n",
    "            # Remember the onset value for the next iteration\n",
    "            prev_onset = onset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "path = Path('/home/is153802/github/LASER/tasks/embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c561d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env LASER=/home/is153802/github/LASER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aedc33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAPTERS = {\n",
    "        1: \"1-3\",\n",
    "        2: \"4-6\",\n",
    "        3: \"7-9\",\n",
    "        4: \"10-12\",\n",
    "        5: \"13-14\",\n",
    "        6: \"15-19\",\n",
    "        7: \"20-22\",\n",
    "        8: \"23-25\",\n",
    "        9: \"26-27\",\n",
    "    }\n",
    "\n",
    "for run in np.arange(1,10):\n",
    "    ch = CHAPTERS[run]\n",
    "    txt_file = f\"/home/is153802/code/data/txt_laser/run{run}.txt\"\n",
    "    emb_file = f\"/home/is153802/code/data/laser_embeddings/emb_{ch}.bin\"\n",
    "    !bash /home/is153802/github/LASER/tasks/embed/embed.sh {txt_file} {emb_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3719b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
