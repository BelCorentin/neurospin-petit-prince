{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import read_raw, get_subjects, get_path, add_embeddings\n",
    "from utils import decod_xy, mne_events\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "\n",
    "modality = \"auditory\"\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "all_evos = []\n",
    "all_scores = []\n",
    "path = get_path(modality)\n",
    "subjects = get_subjects(path)\n",
    "runs = 2\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "levels = ('word','constituent','sentence')\n",
    "starts = ('onset', 'offset')\n",
    "      \n",
    "# Iterate on subjects to epochs, and mean later\n",
    "for subject in subjects[2:3]:\n",
    "    \n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    \n",
    "    # Initialization of the dictionary\n",
    "    for start in starts: \n",
    "            for level in levels:\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "                \n",
    "    # Iterating on runs, building the metadata and re-epoching\n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True, modality=modality)\n",
    "        meta = meta_.copy()\n",
    "        \n",
    "        # Metadata update\n",
    "        meta['word_onset'] = True\n",
    "        meta['word_stop'] = meta.start + meta.duration\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sentence stop info\n",
    "        meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "        for s, d in meta.groupby('sentence_id'):\n",
    "            meta.loc[d.index, 'sent_word_id'] = range(len(d))\n",
    "            meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'sentence_stop'] = d.start.max()\n",
    "            \n",
    "        # Adding the constituents stop info\n",
    "        meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "        for s, d in meta.groupby('constituent_id'):\n",
    "            meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'constituent_stop'] = d.start.max()\n",
    "            meta.loc[d.index, 'const_word_id'] = range(len(d))\n",
    "        \n",
    "        for start in starts: \n",
    "            for level in levels:\n",
    "                # Select only the rows containing the True for the conditions\n",
    "                # Simplified to only get for the onset: sentence onset epochs, constituent onset epochs,etc\n",
    "                sel = meta.query(f'{level}_onset==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "\n",
    "                # Epoching from the metadata having all onset events: if the start=Offset, the mne events\n",
    "                # Function will epoch on the offset of each level instead of the onset\n",
    "                # TODO: add adaptative baseline\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop',\n",
    "                                            preload=True,\n",
    "                                                baseline=None)\n",
    "                epoch_key = f'{level}_{start}'\n",
    "\n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "            \n",
    "    # Once we have the dict of epochs per condition full (epoching for each run for a subject)\n",
    "    # we can concatenate them, and fix the dev_head             \n",
    "    for start_ in starts: \n",
    "        for level_ in levels:\n",
    "            epoch_key = f'{level_}_{start_}'\n",
    "            all_epochs_chosen = dict_epochs[epoch_key]\n",
    "            # Concatenate epochs\n",
    "\n",
    "            for epo in all_epochs_chosen:\n",
    "                epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "            dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n",
    "            \n",
    "    # All epochs -> Decoding and generate evoked potentials\n",
    "    for start in starts: \n",
    "        for level in levels:\n",
    "            epoch_key = f'{level}_{start}'\n",
    "            epochs = dict_epochs[epoch_key]\n",
    "            # mean\n",
    "            evo = epochs.copy().pick_types(meg=True).average(method='median')\n",
    "            all_evos.append(dict(subject=subject, evo=evo, start=start, level=level))\n",
    "\n",
    "\n",
    "            # decoding word emb\n",
    "            epochs = epochs.load_data().pick_types(meg=True, stim=False, misc=False)\n",
    "            X = epochs.get_data()\n",
    "            \n",
    "            \n",
    "            y = epochs.metadata['content_word']\n",
    "            \n",
    "            \n",
    "            R_vec = decod_xy(X, embeddings)\n",
    "            scores = np.mean(R_vec, axis=1)\n",
    "\n",
    "            for t, score in enumerate(scores):\n",
    "                all_scores.append(dict(subject=subject, score=score, start=start, level=level, t=epochs.times[t]))\n",
    "\n",
    "all_scores = pd.DataFrame(all_scores)\n",
    "all_evos = pd.DataFrame(all_evos)\n",
    "\n",
    "all_scores.to_csv('./score.csv')\n",
    "all_evos.to_csv('./evos.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
