{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1f3dcf",
   "metadata": {},
   "source": [
    "# Journal2Bord\n",
    "\n",
    "- 17.04: Retook on JR's pseudocode, in order to make it work with the dataset, and fix some misalignment issues (matchlist needed when re-epoching, index_resetting needed, and also the use of mne_events saves a lot of time)\n",
    "\n",
    "*Next: run it on Neurospin's workstation on a few more subjects, as it takes too much time on laptop.*\n",
    "\n",
    "- 18.04: Fixed more queries handling and general structure, and it runs on Neurospin Workstation.\n",
    "Pb: For one subject, one run, and decoding one feature (word embeddings), it takes around 1h...\n",
    "Added also the averaged on subject \n",
    "\n",
    "- 19.04: Fixed all the problems discussed during our weekly meeting (decim, meta handling, preload, etc..).\n",
    "\n",
    "Came up with a way to generate the plots for all conditions, added a better epochs window handling.\n",
    "\n",
    "- 20.04: Trying to reduce the RAM usage, by calculating the evos & scores directly, not keeping the epochs in memory.\n",
    "Created script for JeanZay, running jobs successfully now. \n",
    "Plot generated for different sliding windows, multiple subjects, all runs. \n",
    "\n",
    "- 21.04: Tried to adapt the newly parsed files for the syntax, adapted but the problem is that the parser has not been run on the correct LPP translation.. TODO contact the person in charge and rerun the parser\n",
    "\n",
    "Also, adapted script for Jean Zay: sent email for support to understand the memory faults.\n",
    "\n",
    "- 26.04: After iterating on Jean Zay: there is a problem with the way the offset of the different modalities is handled. The way it is currently done, there is a shift: the offset is the offset of the previous word/sent/const, instead of being the end of the one we care about. \n",
    "\n",
    "Next steps include:\n",
    "Ongoing / Done\n",
    "- Pausing the testing on Jean Zay and instead focusing on testing locally with first: only word on/offset and decim * 100.\n",
    "- \n",
    "- Changing the mne_events function so that we can give in the parameter sent/const/word and \n",
    "- Adding the info about the sent_id/sent_stop in the metadata using cumsum / groupby\n",
    "\n",
    "Next:\n",
    "\n",
    "- check it worked for words with the decoding\n",
    "\n",
    "## TODO important\n",
    "\n",
    "- Check if the sentence_stop makes sense: currently done by taking the max of the subgroup (groupby sentence_id) and add the duration of the last word\n",
    "\n",
    "- Ask JR about look of word onset/offset decoding\n",
    "\n",
    "TODO another time:\n",
    "- investigate the events_repeated\n",
    "- train on subset for words, and decode on other modalities ?\n",
    "- If the script runs correctly on Jean Zay, add other decoding modalities, and run them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710b125",
   "metadata": {},
   "source": [
    "# Testing new syntactic parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9262d",
   "metadata": {},
   "source": [
    "### Removing regex punct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fbd2545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(SENT (NP-MOD (DET 0=Le) (ADJ 1=cinquième) (NC 2=jour))  (PP-MOD (P+D+ (ADV 4=toujours) (P+D+ (NC 5=grâce) (P+D 6=au))) (NP (NC 7=mouton)))  (NP-SUJ (DET 9=ce) (NC 10=secret) (PP (P 11=de) (NP (DET 12=la) (NC 13=vie) (PP (P+D 14=du) (NP (ADJ 15=petit) (NC 16=prince)))))) (VN (CLO-A_OBJ 17=me) (V 18=fut) (VPP 19=révélé)) (PONCT 20=.))\\n(SENT (VN (CLS-SUJ 0=Il) (CLO-A_OBJ 1=me) (V 2=demanda)) (PP-MOD (P 3=avec) (NP (NC 4=brusquerie)))  (PP-MOD (P 6=sans) (NP (NC 7=préambule)))  (PP-MOD (P 9=comme) (NP (DET 10=le) (NC 11=fruit) (PP (P 12=d') (NP (DET 13=un) (NC 14=problème) (VPpart (ADV 15=longtemps) (VPP 16=médité) (ADV+ (P 17=en) (NC 18=silence))))))) (PONCT 19=:))\\n(SENT (NP-MOD (DET 0=un) (NC 1=mouton))  (Ssub-MOD (CS 3=s') (Sint (VN (CLS-SUJ 4=il) (V 5=mange)) (NP-OBJ (DET 6=les) (NC 7=arbustes))))  (VN (CLS-SUJ 9=il) (V 10=mange)) (ADV 11=aussi) (NP-OBJ (DET 12=les) (NC 13=fleurs)) (PONCT 14=?))\\n(SENT (NP-SUJ (DET 0=un) (NC 1=mouton)) (VN (V 2=mange)) (NP-OBJ (ADJ 3=tout) (PRO 4=ce) (Srel (NP-OBJ (PROREL 5=qu')) (VN (CLS-SUJ 6=il) (V 7=rencontre)))) (PONCT 8=.))\\n(SENT (NP (ADV 0=même) (DET 1=les) (NC 2=fleurs) (Srel (NP-SUJ (PROREL 3=qui)) (VN (V 4=ont)) (NP-OBJ (DET 5=des) (NC 6=épines)))) (PONCT 7=?))\\n(SENT (I 0=oui) (PONCT 1=.))\\n(SENT (NP (ADV 0=Même) (DET 1=les) (NC 2=fleurs) (Srel (NP-SUJ (PROREL 3=qui)) (VN (V 4=ont)) (NP-OBJ (DET 5=des) (NC 6=épines)))) (PONCT 7=.))\\n(SENT (ADV 0=alors) (NP (DET 1=les) (NC 2=épines))  (PP-A_OBJ (P 4=à) (NP (PROWH 5=quoi))) (VN (VN (V 6=servent)) (CLS-SUJ 7=elles?)))\\n(SENT (VN (CLS-SUJ 0=Je) (ADV 1=ne) (CLO-OBJ 2=le) (V 3=savais)) (ADV 4=pas) (PONCT 5=.))\\n(SENT (VN (CLS-SUJ 0=J') (V 1=étais)) (ADV 2=alors) (AP-ATS (ADV 3=très) (VPP 4=occupé)) (PP-A_OBJ (P 5=à) (VPinf (VN (VINF 6=essayer)) (PP-OBJ (P 7=de) (VPinf (VN (VINF 8=dévisser)) (NP-OBJ (DET 9=un) (NC 10=boulon) (AP (ADV 11=trop) (ADJ 12=serré)) (PP (P 13=de) (NP (DET 14=mon) (NC 15=moteur)))))))) (PONCT 16=.))\\n(SENT (VN (CLS-SUJ 0=J') (V 1=étais)) (AP-ATS (ADV 2=très) (ADJ 3=soucieux)) (COORD (CC 4=car) (Sint (NP-SUJ (DET 5=ma) (NC 6=panne)) (VN (V 7=commençait)) (PP-OBJ (P 8=de) (VPinf (VN (CLO-A_OBJ 9=m') (VINF 10=apparaître)) (PP-ATS (P 11=comme) (AP (ADV 12=très) (ADJ 13=grave)))))))  (COORD (CC 15=et) (Sint (NP-SUJ (DET 16=l') (NC+ (NC 17=eau) (P 18=à) (VINF 19=boire)) (Srel (NP-SUJ (PROREL 20=qui)) (VN (CLR 21=s') (V 22=épuisait)))) (VN (CLO-A_OBJ 23=me) (V 24=faisait) (VINF 25=craindre)) (NP-OBJ (DET 26=le) (NC 27=pire)))) (PONCT 28=.))\\n(SENT (NP-MOD (DET 0=les) (NC 1=épines))  (PP-A_OBJ (P 3=à) (NP (PROWH 4=quoi))) (VN (V 5=servent) (CLS-SUJ 6=elles?)))\\n(SENT (NP-SUJ (DET 0=Le) (ADJ 1=petit) (NC 2=prince)) (VN (ADV 3=ne) (V 4=renonçait)) (ADV 5=jamais) (PP-A_OBJ (P 6=à) (NP (DET 7=une) (NC 8=question)))  (Ssub-MOD (CS+ (ADV+ (DET 10=une) (NC 11=fois)) (CS 12=qu')) (Sint (VN (CLS-SUJ 13=il) (CLO-OBJ 14=l') (V 15=avait) (VPP 16=posée)))) (PONCT 17=.))\\n(SENT (VN (CLS-SUJ 0=J') (V 1=étais) (VPP 2=irrité)) (PP-P_OBJ (P 3=par) (NP (DET 4=mon) (NC 5=boulon))) (COORD (CC 6=et) (Sint (VN (CLS-SUJ 7=je) (V 8=répondis)) (NP-OBJ (PRO+ (ADV 9=n') (V 10=importe) (PRO 11=quoi))))) (PONCT 12=:))\\n(SENT (NP-MOD (DET 0=les) (NC 1=épines))  (NP-SUJ (PRO 3=ça)) (VN (ADV 4=ne) (V 5=sert)) (PP-A_OBJ (P 6=à) (NP (PRO 7=rien)))  (Sint-MOD (VN (CLS-SUJ 9=c') (V 10=est)) (NP-ATS (DET+ (P 11=de) (DET 12=la)) (ADJ 13=pure) (NC 14=méchanceté)) (PP-MOD (P+D+ (P 15=de) (DET 16=la) (NC 17=part) (P+D 18=des)) (NP (NC 19=fleurs)))) (PONCT 20=!))\\n(SENT (I 0=oh) (PONCT 1=!))\\n(SENT (CC 0=Mais) (Sint (PP-MOD (P 1=après) (NP (DET 2=un) (NC 3=silence))) (VN (CLS-SUJ 4=il) (CLO-A_OBJ 5=me) (V 6=lança))  (PP-MOD (P 8=avec) (NP (DET+ (DET 9=une) (NC 10=sorte) (P 11=de)) (NC 12=rancune)))) (PONCT 13=:))\\n(SENT (VN (CLS-SUJ 0=je) (ADV 1=ne) (CLO-OBJ 2=te) (V 3=crois)) (ADV 4=pas) (PONCT 5=!))\\n(SENT (NP-SUJ (DET 0=Les) (NC 1=fleurs)) (VN (V 2=sont)) (AP-ATS (ADJ 3=faibles)) (PONCT 4=.))\\n(SENT (VN (CLS-SUJ 0=Elles) (V 1=sont)) (AP-ATS (ADJ 2=naïves)) (PONCT 3=.))\\n(SENT (VN (CLS-SUJ 0=Elles) (CLR-OBJ 1=se) (V 2=rassurent)) (Ssub-MOD (CS 3=comme) (Sint (VN (CLS-SUJ 4=elles) (V 5=peuvent)))) (PONCT 6=.))\\n(SENT (VN (CLS-SUJ 0=Elles) (CLR 1=se) (V 2=croient)) (AP-ATO (ADJ 3=terribles)) (PP-MOD (P 4=avec) (NP (DET 5=leurs) (NC 6=épines))) (PONCT 7=...) (Sint-MOD (VN (CLS-SUJ 8=Je) (ADV 9=ne) (V 10=répondis)) (NP-OBJ (PRO 11=rien))) (PONCT 12=.))\\n(SENT (PP-MOD (P 0=À) (NP (DET 1=cet) (NC 2=instant) (ADV 3=là))) (VN (CLS-SUJ 4=je) (CLR 5=me) (V 6=disais)) (PONCT 7=:))\\n(SENT (Ssub-MOD (CS 0=si) (Sint (NP-SUJ (DET 1=ce) (NC 2=boulon)) (VN (V 3=résiste)) (ADV 4=encore)))  (VN (CLS-SUJ 6=je) (CLO-OBJ 7=le) (V 8=ferai) (VINF 9=sauter)) (PP-MOD (P 10=d') (NP (DET 11=un) (NC 12=coup) (PP (P 13=de) (NP (NC 14=marteau))))) (PONCT 15=.))\\n(SENT (NP-SUJ (DET 0=Le) (ADJ 1=petit) (NC 2=prince)) (VN (V 3=dérangea)) (ADV+ (P 4=de) (NC 5=nouveau)) (NP-OBJ (DET 6=mes) (NC 7=réflexions)) (PONCT 8=:))\\n(SENT (COORD (CC 0=et) (Sint (VN (CLS-SUJ 1=tu) (V 2=crois))  (NP-MOD (PRO 4=toi))  (Ssub-OBJ (CS 6=que) (NP (DET 7=les) (NC 8=fleurs))))) (PONCT 9=...))\\n(SENT (COORD (CC 0=mais) (ADV 1=non)) (PONCT 2=!))\\n(SENT (COORD (CC 0=Mais) (ADV 1=non)) (PONCT 2=!))\\n(SENT (VN (CLS-SUJ 0=Je) (ADV 1=ne) (V 2=crois)) (NP-OBJ (PRO 3=rien)) (PONCT 4=!))\\n(SENT (VN (CLS-SUJ 0=J') (V 1=ai) (VPP 2=répondu)) (NP-OBJ (ADV 3=n') (V 4=importe) (PRO 5=quoi)) (PONCT 6=.))\\n(SENT (VN (CLS-SUJ 0=Je) (CLR-OBJ 1=m') (V 2=occupe))  (NP-MOD (PRO 4=moi))  (PP-DE_OBJ (P 6=de) (NP (NC 7=choses) (AP (ADJ 8=sérieuses)))) (PONCT 9=!))\\n(SENT (VN (CLS-SUJ 0=Il) (CLO-OBJ 1=me) (V 2=regarda)) (AP-ATO (ADJ 3=stupéfait)) (PONCT 4=.))\\n(SENT (NP (P 0=de) (NP (NC 1=choses) (AP (ADJ 2=sérieuses)))) (PONCT 3=!))\\n(SENT (VN (CLS-SUJ 0=Il) (CLO-OBJ 1=me) (V 2=voyait))  (NP-MOD (DET 4=mon) (NC 5=marteau) (ADV+ (P 6=à) (DET 7=la) (NC 8=main))  (COORD (CC 10=et) (NP (DET 11=les) (NC 12=doigts) (AP (ADJ 13=noirs)) (PP (P 14=de) (NP (NC 15=cambouis))))))  (VPpart-MOD (VPP 17=penché) (PP (P 18=sur) (NP (DET 19=un) (NC 20=objet) (Srel (NP-SUJ (PROREL 21=qui)) (VN (CLO-A_OBJ 22=lui) (V 23=semblait)) (AP-ATS (ADV 24=très) (ADJ 25=laid)))))) (PONCT 26=.))\\n(SENT (VN (CLS-SUJ 0=tu) (V 1=parles)) (PP-MOD (P 2=comme) (NP (DET 3=les) (ADJ 4=grandes) (NC 5=personnes))) (PONCT 6=!))\\n(SENT (NP-SUJ (PRO 0=Ça)) (VN (CLO-A_OBJ 1=me) (V 2=fit)) (ADV+ (DET 3=un) (ADV 4=peu)) (NP-OBJ (NC 5=honte)) (PONCT 6=.))\\n(SENT (CC 0=Mais)  (AP-MOD (ADJ 2=impitoyable))  (VN (CLS-SUJ 4=il) (V 5=ajouta)) (PONCT 6=:))\\n(SENT (VN (CLS-SUJ 0=tu) (V 1=confonds)) (NP-OBJ (PRO 2=tout)) (PONCT 3=...))\\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=mélanges)) (NP-OBJ (PRO 2=tout)) (PONCT 3=!))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=était) (AdP (ADV 2=vraiment) (ADV 3=très)) (VPP 4=irrité)) (PONCT 5=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=secouait)) (ADV+ (P+D 2=au) (NC 3=vent)) (NP-OBJ (DET 4=des) (NC 5=cheveux) (AP (ADV 6=tout) (ADJ 7=dorés))) (PONCT 8=:))\\n(SENT (VN (CLS-SUJ 0=je) (V 1=connais)) (NP-OBJ (DET 2=une) (NC 3=planète) (Srel (NP-MOD (PROREL 4=où)) (VN (V+ (CLS 5=il) (CLO 6=y) (V 7=a))) (NP-OBJ (DET 8=un) (NC 9=monsieur) (AP (ADJ 10=cramoisi))))) (PONCT 11=.))\\n(SENT (VN (CLS-SUJ 0=Il) (ADV 1=n') (V 2=a) (ADV 3=jamais) (VPP 4=respiré)) (NP-OBJ (DET 5=une) (NC 6=fleur)) (PONCT 7=.))\\n(SENT (VN (CLS-SUJ 0=Il) (ADV 1=n') (V 2=a) (ADV 3=jamais) (VPP 4=regardé)) (NP-OBJ (DET 5=une) (NC 6=étoile)) (PONCT 7=.))\\n(SENT (VN (CLS-SUJ 0=Il) (ADV 1=n') (V 2=a) (ADV 3=jamais) (VPP 4=aimé)) (NP-OBJ (PRO 5=personne)) (PONCT 6=.))\\n(SENT (VN (CLS-SUJ 0=Il) (ADV 1=n') (V 2=a) (ADV 3=jamais) (PRO 4=rien) (VPP 5=fait)) (PP-DE_OBJ (P 6=d') (NP (PRO 7=autre) (Ssub (CS 8=que) (NP (DET 9=des) (NC 10=additions))))) (PONCT 11=.))\\n(COORD (CC 0=Et) (Sint (NP-MOD (ADJ 1=toute) (DET 2=la) (NC 3=journée)) (VN (CLS-SUJ 4=il) (V 5=répète)) (PP-MOD (P 6=comme) (NP (PRO 7=toi:)))))\\n(SENT (VN (CLS-SUJ 0=je) (V 1=suis)) (NP-ATS (DET 2=un) (NC 3=homme) (AP (ADJ 4=sérieux))) (PONCT 5=!))\\n(SENT (VN (CLS-SUJ 0=Je) (V 1=suis)) (NP-ATS (DET 2=un) (NC 3=homme) (AP (ADJ 4=sérieux))) (PONCT 5=!))\\n(SENT  (COORD (CC 1=Et) (NP-SUJ (PRO 2=ça)) (VN (CLO-OBJ 3=le) (V 4=fait) (VINF 5=gonfler)) (PP-DE_OBJ (P 6=d') (NP (NC 7=orgueil)))) (PONCT 8=.))\\n(SENT (CC 0=Mais) (VN (CLS-SUJ 1=ce) (ADV 2=n') (V 3=est)) (ADV 4=pas) (NP-OBJ (DET 5=un) (NC 6=homme))  (Sint-MOD (VN (CLS-SUJ 8=c') (V 9=est)) (NP-ATS (DET 10=un) (NC 11=champignon))) (PONCT 12=!))\\n(NP (DET 0=un) (PROWH 1=quoi?))\\n(SENT (NP (DET 0=un) (NC 1=champignon)) (PONCT 2=!))\\n(SENT (NP-SUJ (DET 0=Le) (ADJ 1=petit) (NC 2=prince)) (VN (V 3=était)) (ADV 4=maintenant) (AP-ATS (ADV 5=tout) (ADJ 6=pâle) (PP (P 7=de) (NP (NC 8=colère)))) (PONCT 9=.))\\n(SENT (VN (V+ (CLS 0=il) (CLO 1=y) (V 2=a))) (NP-OBJ (DET 3=des) (NC 4=millions) (PP (P 5=d') (NP (NC 6=années)))) (Ssub-OBJ (CS 7=que) (Sint (NP-SUJ (DET 8=les) (NC 9=fleurs)) (VN (V 10=fabriquent)) (NP-OBJ (DET 11=des) (NC 12=épines)))) (PONCT 13=.))\\n(SENT (VN (V+ (CLS 0=Il) (CLO 1=y) (V 2=a))) (NP-OBJ (DET 3=des) (NC 4=millions) (PP (P 5=d') (NP (NC 6=années)))) (Ssub-OBJ (CS 7=que) (Sint (NP-SUJ (DET 8=les) (NC 9=moutons)) (VN (V 10=mangent)) (ADV+ (ADV 11=quand) (ADV 12=même)) (NP-OBJ (DET 13=les) (NC 14=fleurs)))) (PONCT 15=.))\\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=ce) (ADV 2=n') (V 3=est)) (ADV 4=pas) (AP-ATS (ADJ 5=sérieux)) (PP-OBJ (P 6=de) (VPinf (VN (VINF 7=chercher)) (PP-OBJ (P 8=à) (VPinf (VN (VINF 9=comprendre)) (Ssub-OBJ (ADVWH 10=pourquoi) (VN (CLS-SUJ 11=elles) (CLR 12=se) (V 13=donnent)) (NP-OBJ (ADV 14=tant) (P 15=de) (NC 16=mal)) (PP-MOD (P 17=pour) (VPinf (VN (CLR 18=se) (VINF 19=fabriquer)) (NP-OBJ (DET 20=des) (NC 21=épines) (Srel (NP-SUJ (PROREL 22=qui)) (VN (ADV 23=ne) (V 24=servent)) (ADV 25=jamais) (PP-A_OBJ (P 26=à) (NP (PRO 27=rien)))))))))))))) (PONCT 28=?))\\n(SENT (VN (CLS-SUJ 0=Ce) (ADV 1=n') (V 2=est)) (ADV 3=pas) (AP-ATS (ADJ 4=important)) (NP-ATS (DET 5=la) (NC 6=guerre) (PP (P+D 7=des) (NP (NC 8=moutons)) (COORD (CC 9=et) (PP (P+D 10=des) (NP (NC 11=fleurs)))))) (PONCT 12=?))\\n(SENT (VN (CLS-SUJ 0=Ce) (ADV 1=n') (V 2=est)) (ADV 3=pas) (AP-ATS (ADV 4=plus) (ADJ 5=sérieux) (COORD (CC 6=et) (AP (ADV 7=plus) (ADJ 8=important))) (Ssub (CS 9=que) (NP (DET 10=les) (NC 11=additions) (PP (P 12=d') (NP (DET 13=un) (ADJ 14=gros) (NC 15=monsieur) (AP (ADJ 16=rouge))))))) (PONCT 17=?))\\n(COORD (CC 0=Et) (Sint (Ssub-MOD (CS 1=si) (Sint (VN (CLS-SUJ 2=je) (V 3=connais))  (NP-MOD (PRO 5=moi))  (NP-OBJ (DET 7=une) (NC 8=fleur) (AP (ADJ 9=unique) (PP (P+D 10=au) (NP (NC 11=monde))))  (Srel (NP-SUJ (PROREL 13=qui)) (VN (ADV 14=n') (V 15=existe)) (ADV+ (DET 16=nulle) (NC 17=part))  (PP-MOD (P 19=sauf) (PP (P 20=dans) (NP (DET 21=ma) (NC 22=planète)))))  (COORD (CC 24=et) (Srel (VPinf-OBJ (NP-OBJ (PROREL 25=qu')) (VN (VINF 30=anéantir)) (PP-MOD (P 31=d') (NP (DET 32=un) (ADJ 33=seul) (NC 34=coup)))  (PP-MOD (P 36=comme) (NP (PRO 37=ça)))  (NP-MOD (DET 39=un) (NC 40=matin))  (PP-MOD (P 42=sans) (VPinf (VN (VINF+ (CLR 43=se) (VN (VINF+ (VINF 44=rendre) (NC 45=compte))))) (PP-DE_OBJ (P 46=de) (NP (PRO 47=ce) (Srel (NP-OBJ (PROREL 48=qu')) (VN (CLS-SUJ 49=il) (V 50=fait)))))))) (NP-SUJ (DET 26=un) (ADJ 27=petit) (NC 28=mouton)) (VN (V 29=peut)))))))  (VN (CLS-SUJ 52=ce) (ADV 53=n') (V 54=est)) (ADV 55=pas) (AP-ATS (ADJ 56=important)) (NP-MOD (PRO 57=ça!))))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=rougit))  (COORD (CC 3=puis) (VN (V 4=reprit))) (PONCT 5=:))\\n(SENT (Ssub-MOD (CS 0=si) (Sint (DET 1=quelqu'un) (V 2=aime) (NP-OBJ (DET 3=une) (NC 4=fleur) (Srel (NP-SUJ (PROREL 5=qui)) (VN (ADV 6=n') (V 7=existe)) (ADV 8=qu') (PP-MOD (P 9=à) (NP (DET 10=un) (NC 11=exemplaire))) (PP-MOD (P 12=dans) (NP (DET 13=les) (NC 14=millions) (COORD (CC 15=et) (NP (DET 16=les) (NC 17=millions))) (PP (P 18=d') (NP (NC 19=étoiles)))))))))  (NP-SUJ (PRO 21=ça)) (VN (V 22=suffit)) (Ssub-MOD (CS+ (P 23=pour) (CS 24=qu')) (Sint (VN (CLS-SUJ 25=il) (VS 26=soit)) (AP-ATS (ADJ 27=heureux)) (Ssub-MOD (CS 28=quand) (Sint (VN (CLS-SUJ 29=il) (CLO-OBJ 30=les) (V 31=regarde)))))) (PONCT 32=.))\\n(SENT (VN (CLS-SUJ 0=Il) (CLR 1=se) (V 2=dit)) (PONCT 3=:))\\n(SENT (NP-SUJ (DET 0=ma) (NC 1=fleur)) (VN (V 2=est)) (ADV 3=là) (ADV+ (ADJ 4=quelque) (NC 5=part)) (PONCT 6=...))\\n(SENT (COORD (CC 0=Mais) (Sint  (Ssub-MOD (CS 2=si) (Sint (NP-SUJ (DET 3=le) (NC 4=mouton)) (VN (V 5=mange)) (NP-OBJ (DET 6=la) (NC 7=fleur))))  (VN (CLS-SUJ 9=c') (V 10=est)) (PP-MOD (P 11=pour) (NP (PRO 12=lui))) (Ssub-MOD (CS+ (ADV 13=comme) (CS 14=si)) (Sint  (ADV 16=brusquement)  (NP-SUJ (ADJ 18=toutes) (DET 19=les) (NC 20=étoiles)) (VN (CLR 21=s') (V 22=éteignaient)))))) (PONCT 23=!))\\n(COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=ce) (ADV 2=n') (V 3=est)) (ADV 4=pas) (AP-ATS (ADJ 5=important)) (NP-MOD (PRO 6=ça!))))\\n(SENT (VN (CLS-SUJ 0=Il) (ADV 1=ne) (V 2=put)) (VPinf-OBJ (NP-OBJ (PRO 3=rien)) (VN (VINF 4=dire)) (PP-MOD (P 5=de) (ADV 6=plus))) (PONCT 7=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=éclata)) (ADV 2=brusquement) (PP-P_OBJ (P 3=en) (NP (NC 4=sanglots))) (PONCT 5=.))\\n(SENT (NP-SUJ (DET 0=La) (NC 1=nuit)) (VN (V 2=était) (VPP 3=tombée)) (PONCT 4=.))\\n(SENT (VN (CLS-SUJ 0=J') (V 1=avais) (VPP 2=lâché)) (NP-OBJ (DET 3=mes) (NC 4=outils)) (PONCT 5=.))\\n(SENT (VN (CLS-SUJ 0=Je) (CLR 1=me) (V 2=moquais)) (ADV 3=bien) (PP-DE_OBJ (P 4=de) (NP (DET 5=mon) (NC 6=marteau)) (COORD  (PP (P 8=de) (NP (DET 9=mon) (NC 10=boulon)))) (COORD  (PP (P 12=de) (NP (DET 13=la) (NC 14=soif)))) (COORD (CC 15=et) (PP (P 16=de) (NP (DET 17=la) (NC 18=mort))))) (PONCT 19=.))\\n(SENT (VN (CLS-SUJ 0=Il) (CLO 1=y) (V 2=avait))  (PP-MOD (P 4=sur) (NP (DET 5=une) (NC 6=étoile)))  (NP-OBJ (DET 8=une) (NC 9=planète)  (NP (DET 11=la) (PRO 12=mienne))  (NP (DET 14=la) (NC 15=terre)))  (NP-OBJ (DET 17=un) (ADJ 18=petit) (NC 19=prince) (PP (P 20=à) (VPinf (VN (VINF 21=consoler))))) (PONCT 22=!))\\n(SENT (VN (CLS-SUJ 0=Je) (CLO-OBJ 1=le) (V 2=pris)) (PP-P_OBJ.O (P 3=dans) (NP (DET 4=les) (NC 5=bras))) (PONCT 6=.))\\n(SENT (VN (CLS-SUJ 0=Je) (CLO-OBJ 1=le) (V 2=berçai)) (PONCT 3=.))\\n(SENT (VN (CLS-SUJ 0=Je) (CLO-A_OBJ 1=lui) (V 2=disais)) (PONCT 3=:))\\n(SENT (NP-SUJ (DET 0=la) (NC 1=fleur) (Srel (NP-OBJ (PROREL 2=que)) (VN (CLS-SUJ 3=tu) (V 4=aimes)))) (VN (ADV 5=n') (V 6=est)) (ADV 7=pas) (PP-ATS (P 8=en) (NP (NC 9=danger))) (PONCT 10=...))\\n(SENT (VN (CLS-SUJ 0=Je) (CLO-A_OBJ 1=lui) (V 2=dessinerai)) (NP-OBJ (DET 3=une) (NC 4=muselière))  (PP-MOD (P 6=à) (NP (DET 7=ton) (NC 8=mouton))) (PONCT 9=...))\\n(SENT (VN (CLS-SUJ 0=Je) (CLO-A_OBJ 1=te) (V 2=dessinerai)) (NP-OBJ (DET 3=une) (NC 4=armure) (PP (P 5=pour) (NP (DET 6=ta) (NC 7=fleur)))) (PONCT 8=...))\\n(SENT (VN (CLS-SUJ 0=Je) (PONCT 1=...) (VN (CLS-SUJ 2=Je) (ADV 3=ne) (V 4=savais))) (ADV 5=pas) (ADV 6=trop) (VPinf-OBJ (NP-OBJ (PROWH 7=quoi)) (VN (VINF 8=dire))) (PONCT 9=.))\\n(SENT (VN (CLS-SUJ 0=Je) (CLR 1=me) (V 2=sentais)) (AP-ATS (ADV 3=très) (ADJ 4=maladroit)) (PONCT 5=.))\\n(SENT (VN (CLS-SUJ 0=Je) (ADV 1=ne) (V 2=savais)) (VPinf-OBJ (ADVWH 3=comment) (VN (CLO-OBJ 4=l') (VINF 5=atteindre))  (VPinf-MOD (ADVWH 7=où) (VN (CLO-OBJ 8=le) (VINF 9=rejoindre)))) (PONCT 10=...))\\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (AP-ATS (ADV 2=tellement) (ADJ 3=mystérieux))  (NP-MOD (DET 5=le) (NC 6=pays) (PP (P+D 7=des) (NP (NC 8=larmes)))) (PONCT 9=!))\\n(SENT (VN (CLS-SUJ 0=J') (V 1=appris)) (AdP-MOD (ADV 2=bien) (ADV 3=vite)) (PP-OBJ (P 4=à) (VPinf (ADV 5=mieux) (VN (VINF 6=connaître)) (NP-OBJ (DET 7=cette) (NC 8=fleur)))) (PONCT 9=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=avait) (ADV 2=toujours) (VPP 3=eu))  (PP-MOD (P 5=sur) (NP (DET 6=la) (NC 7=planète) (PP (P+D 8=du) (NP (ADJ 9=petit) (NC 10=prince)))))  (NP-OBJ (DET 12=des) (NC 13=fleurs) (AP (ADV 14=très) (ADJ 15=simples))  (VPpart (VPP 17=ornées) (PP-DE_OBJ (P 18=d') (NP (DET 19=un) (ADJ 20=seul) (NC 21=rang) (PP (P 22=de) (NP (NC 23=pétales)))))  (COORD (CC 25=et) (Srel (NP-SUJ (PROREL 26=qui)) (VN (ADV 27=ne) (V 28=tenaient)) (ADV 29=point) (NP-OBJ (P 30=de) (NP (NC 31=place)))  (COORD (CC 33=et) (Srel (NP-SUJ (PROREL 34=qui)) (VN (ADV 35=ne) (V 36=dérangeaient)) (NP-OBJ (PRO 37=personne)))))))) (PONCT 38=.))\\n(SENT (VN (CLS-SUJ 0=Elles) (V 1=apparaissaient)) (NP-MOD (DET 2=un) (NC 3=matin)) (PP-MOD (P 4=dans) (NP (DET 5=l') (NC 6=herbe)))  (COORD (CC+ (CC 8=et) (ADV 9=puis)) (VN (CLS-SUJ 10=elles) (CLR 11=s') (V 12=éteignaient)) (NP-MOD (DET 13=le) (NC 14=soir))) (PONCT 15=.))\\n(SENT (COORD (CC 0=Mais) (Sint (NP-SUJ (PRO+ (PRO 1=celle) (ADV 2=là))) (VN (V 3=avait) (VPP 4=germé)) (NP-MOD (DET 5=un) (NC 6=jour))  (PP-MOD (P 8=d') (NP (DET 9=une) (NC 10=graine) (VPpart (VPP 11=apportée) (PP (P 12=d') (CLS-SUJ 13=on) (ADV 14=ne) (V 15=sait) (ADVWH 16=où)))))  (COORD (CC 18=et) (Sint (NP-SUJ (DET 19=le) (ADJ 20=petit) (NC 21=prince)) (VN (V 22=avait) (VPP 23=surveillé)) (ADV+ (P 24=de) (AdP (ADV 25=très) (ADV 26=près))) (NP-OBJ (DET 27=cette) (NC 28=brindille) (Srel (NP-SUJ (PROREL 29=qui)) (VN (ADV 30=ne) (V 31=ressemblait)) (ADV 32=pas) (PP-A_OBJ (P+D 33=aux) (NP (ADJ 34=autres) (NC 35=brindilles))))))))) (PONCT 36=.))\\n(SENT (NP-SUJ (PRO 0=Ça)) (VN (V 1=pouvait)) (VPinf-OBJ (VN (VINF 2=être)) (NP-OBJ (DET 3=un) (ADJ 4=nouveau) (NC 5=genre) (PP (P 6=de) (NP (NC 7=baobab))))) (PONCT 8=.))\\n(SENT (COORD (CC 0=Mais) (Sint (NP-SUJ (DET 1=l') (NC 2=arbuste)) (VN (V 3=cessa)) (ADV 4=vite) (PP-OBJ (P 5=de) (VPinf (VN (VINF 6=croître))))  (COORD (CC 8=et) (VN (V 9=commença)) (PP-OBJ (P 10=de) (VPinf (VN (VINF 11=préparer)) (NP-OBJ (DET 12=une) (NC 13=fleur))))))) (PONCT 14=.))\\n(SENT (NP-SUJ (DET 0=Le) (ADJ 1=petit) (NC 2=prince)  (Srel (NP-SUJ (PROREL 4=qui)) (VN (V 5=assistait)) (PP-A_OBJ (P 6=à) (NP (DET 7=l') (NC 8=installation) (PP (P 9=d') (NP (DET 10=un) (NC 11=bouton) (AP (ADJ 12=énorme))))))))  (VN (V 14=sentait)) (ADV 15=bien) (Ssub-OBJ (CS 16=qu') (Sint (VN (CLS-SUJ 17=il) (CLO-DE_OBJ 18=en) (V 19=sortirait)) (NP-OBJ (DET 20=une) (NC 21=apparition) (AP (ADJ 22=miraculeuse)))))  (COORD (CC 24=mais) (Sint (NP-SUJ (DET 25=la) (NC 26=fleur)) (VN (ADV 27=n') (CLO 28=en) (V 29=finissait)) (ADV 30=pas) (PP-OBJ (P 31=de) (VPinf (VN (CLR-OBJ 32=se) (VINF 33=préparer)) (PP-A_OBJ (P 34=à) (VPinf (VN (VINF 35=être)) (AP-ATS (ADJ 36=belle))  (PP-MOD (P+ (P 38=à) (DET 39=l') (NC 40=abri) (P 41=de)) (NP (DET 42=sa) (NC 43=chambre) (AP (ADJ 44=verte)))))))))) (PONCT 45=.))\\n(SENT (VN (CLS-SUJ 0=Elle) (V 1=choisissait)) (PP-MOD (P 2=avec) (NP (NC 3=soin))) (NP-OBJ (DET 4=ses) (NC 5=couleurs)) (PONCT 6=.))\\n(SENT (VN (CLS-SUJ 0=Elle) (CLR 1=s') (V 2=habillait)) (ADV 3=lentement)  (Sint-MOD (VN (CLS-SUJ 5=elle) (V 6=ajustait)) (NP-MOD (PRO 7=un) (PP (P 8=à) (NP (PRO 9=un)))) (NP-OBJ (DET 10=ses) (NC 11=pétales))) (PONCT 12=.))\\n(SENT (VN (CLS-SUJ 0=Elle) (ADV 1=ne) (V 2=voulait)) (ADV 3=pas) (VPinf-OBJ (VN (VINF 4=sortir)) (AP-ATS (ADV 5=toute) (ADJ 6=fripée) (PP (P 7=comme) (NP (DET 8=les) (NC 9=coquelicots))))) (PONCT 10=.))\\n(SENT (VN (CLS-SUJ 0=Elle) (ADV 1=ne) (V 2=voulait)) (VPinf-OBJ (VN (VINF 3=apparaître)) (ADV 4=que) (PP-MOD (P 5=dans) (NP (DET 6=le) (ADJ 7=plein) (NC 8=rayonnement) (PP (P 9=de) (NP (DET 10=sa) (NC 11=beauté)))))) (PONCT 12=.))\\n(SENT (I 0=Eh!))\\n(SENT (I 0=Oui) (PONCT 1=.))\\n(SENT (VN (CLS-SUJ 0=Elle) (V 1=était)) (AP-ATS (ADV 2=très) (ADJ 3=coquette)) (PONCT 4=!))\\n(SENT (NP-SUJ (DET 0=Sa) (NC 1=toilette) (AP (ADJ 2=mystérieuse))) (VN (V 3=avait) (ADV 4=donc) (VPP 5=duré)) (NP-OBJ (DET 6=des) (NC 7=jours) (COORD (CC 8=et) (NP (DET 9=des) (NC 10=jours)))) (PONCT 11=.))\\n(SENT (COORD (CC+ (CC 0=Et) (ADV 1=puis)) (Sint (VN (V 2=voici)) (Ssub-OBJ (CS 3=qu') (Sint (NP-MOD (DET 4=un) (NC 5=matin))  (PP-MOD (ADV 7=justement) (P+D+ (P 8=à) (DET 9=l') (NC 10=heure) (P+D 11=du)) (NP (NC+ (NC 12=lever) (P+D 13=du) (NC 14=soleil))))  (VN (CLS-SUJ 16=elle) (CLR 17=s') (V 18=était) (VPP 19=montrée)))))) (PONCT 20=.))\\n(SENT (CC 0=Et) (Sint (Sint (NP-SUJ (PRO 1=elle)  (Srel (NP-SUJ (PROREL 3=qui)) (VN (V 4=avait) (VPP 5=travaillé)) (PP-MOD (P 6=avec) (NP (ADV 7=tant) (P 8=de) (NC 9=précision)))))  (VN (V 11=dit)) (PP-MOD (P 12=en) (VPpart (VN (VPR 13=bâillant))))) (PONCT 14=:)))\\n(SENT (I 0=ah) (PONCT 1=!))\\n(SENT (VN (CLS-SUJ 0=Je) (CLR 1=me) (V 2=réveille)) (ADV+ (P 3=à) (NC 4=peine)) (PONCT 5=...))\\n(SENT (VN (CLS-SUJ 0=Je) (CLO-A_OBJ 1=vous) (V 2=demande)) (NP-OBJ (NC 3=pardon)) (PONCT 4=...))\\n(SENT (VN (VN (CLS-SUJ 0=Je) (V 1=suis)) (ADV 2=encore) (AP-ATS (ADV 3=toute) (VPP 4=décoiffée))) (PONCT 5=...))\\n(SENT (NP-SUJ (DET 0=Le) (ADJ 1=petit) (NC 2=prince))  (ADV 4=alors)  (VN (ADV 6=ne) (V 7=put)) (VPinf-OBJ (VN (VINF 8=contenir)) (NP-OBJ (DET 9=son) (NC 10=admiration))) (PONCT 11=:))\\n(SENT (ADV 0=que) (VN (CLS-SUJ 1=vous) (V 2=êtes)) (AP-ATS (ADJ 3=belle)) (PONCT 4=!))\\n(SENT (VN (ADV 0=n') (V 1=est) (CLS-SUJ 2=ce)) (ADV 3=pas)  (Sint-MOD (VN (V 5=répondit)) (ADV 6=doucement) (NP-SUJ (DET 7=la) (NC 8=fleur))) (PONCT 9=.))\\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=je) (V 2=suis) (VPP 3=née)) (Ssub-MOD (CS+ (P 4=en) (ADJ 5=même) (NC 6=temps) (CS 7=que)) (NP (DET 8=le) (NC 9=soleil))))) (PONCT 10=...))\\n(SENT (NP-SUJ (DET 0=Le) (ADJ 1=petit) (NC 2=prince)) (VN (V 3=devina)) (ADV 4=bien) (Ssub-OBJ (CS 5=qu') (Sint (Sint (VN (CLS-SUJ 6=elle) (ADV 7=n') (V 8=était)) (ADV 9=pas) (AP-ATS (ADV 10=trop) (ADJ 11=modeste)))  (COORD (CC 13=mais) (Sint (VN (CLS-SUJ 14=elle) (V 15=était)) (AP-ATS (ADV 16=si) (ADJ 17=émouvante)))))) (PONCT 18=!))\\n(SENT (VN (CLS-SUJ 0=c') (V 1=est)) (NP-ATS (DET 2=l') (NC 3=heure) (Sint-MOD  (VN (CLS-SUJ 5=je) (V 6=crois))  (PP (P+D 8=du) (NP (NC+ (ADJ 9=petit) (NC 10=déjeuner)))))) (Sint-MOD (Sint-MOD  (VN (V 12=avait) (CLS-SUJ 13=elle) (ADV 14=bientôt) (VPP 15=ajouté))) (Sint-MOD  (VN (V 17=auriez) (CLS-SUJ 18=vous)) (NP-OBJ (DET 19=la) (NC 20=bonté) (PP (P 21=de) (VPinf (VN (VINF 22=penser)) (PP-A_OBJ (P 23=à) (NP (PRO 24=moi)))))))) (PONCT 25=...))\\n(SENT (COORD (CC 0=Et) (Sint (NP-SUJ (DET 1=le) (ADJ 2=petit) (NC 3=prince)  (AP (ADV 5=tout) (ADJ 6=confus))  (VPpart (VN (VPR 8=ayant) (VPP 9=été)) (VPinf-OBJ (VN (VINF 10=chercher)) (NP-OBJ (DET 11=un) (NC 12=arrosoir) (PP (P 13=d') (NP (NC 14=eau) (AP (ADJ 15=fraîche))))))))  (VN (V 17=avait) (VPP 18=servi)) (NP-OBJ (DET 19=la) (NC 20=fleur)))) (PONCT 21=.))\\n(SENT (ADV 0=Ainsi) (VN (CLO-OBJ 1=l') (V 2=avait) (PRO 3=elle) (AdP (ADV 4=bien) (ADV 5=vite)) (VPP 6=tourmenté)) (PP-MOD (P 7=par) (NP (DET 8=sa) (NC 9=vanité) (AP (ADV+ (DET 10=un) (NC 11=peu)) (ADJ 12=ombrageuse)))) (PONCT 13=.))\\n(SENT (NP-MOD (DET 0=Un) (NC 1=jour))  (ADV+ (P 3=par) (NC 4=exemple))  (VPpart-MOD (VN (VPR 6=parlant)) (PP-DE_OBJ (P 7=de) (NP (DET 8=ses) (ADJ 9=quatre) (NC 10=épines))))  (VN (CLS-SUJ 12=elle) (V 13=avait) (VPP 14=dit)) (PP-A_OBJ (P+D 15=au) (NP (ADJ 16=petit) (NC 17=prince))) (PONCT 18=:))\\n(SENT (VN (CLS-SUJ 0=ils) (V 1=peuvent)) (VPinf-OBJ (VN (VINF 2=venir))  (NP-MOD (DET 4=les) (NC 5=tigres))  (PP-MOD (P 7=avec) (NP (DET 8=leurs) (NC 9=griffes)))) (PONCT 10=!))\\n(SENT (VN (CLS-SUJ 0=il) (ADV 1=n') (CLO 2=y) (V 3=a)) (ADV 4=pas) (NP-OBJ (DET 5=de) (NC 6=tigres)) (PP-MOD (P 7=sur) (NP (DET 8=ma) (NC 9=planète))) (Sint-MOD  (VN (V 11=avait) (VPP 12=objecté)) (NP-SUJ (DET 13=le) (ADJ 14=petit) (NC 15=prince)) ) (COORD (CC+ (CC 17=et) (ADV 18=puis)) (Sint (NP-SUJ (DET 19=les) (NC 20=tigres)) (VN (ADV 21=ne) (V 22=mangent)) (ADV 23=pas) (NP-OBJ (DET 24=d') (NC 25=herbe)))) (PONCT 26=.))\\n(SENT (VN (CLS-SUJ 0=je) (ADV 1=ne) (V 2=suis)) (ADV 3=pas) (NP-ATS (DET 4=une) (NC 5=herbe)) (Sint-MOD  (VN (V 7=avait) (ADV 8=doucement) (VPP 9=répondu)) (NP-SUJ (DET 10=la) (NC 11=fleur))) (PONCT 12=.))\\n(SENT (VN (VIMP 0=pardonnez) (PRO 1=moi)) (PONCT 2=...))\\n(SENT (VN (CLS-SUJ 0=je) (ADV 1=ne) (V 2=crains)) (NP-OBJ (NP-OBJ (PRO 3=rien)) (PP (P+D 4=des) (NP (NC 5=tigres))))  (COORD (CC 7=mais) (Sint (VN (CLS-SUJ 8=j') (V 9=ai)) (NP-OBJ (NC 10=horreur) (PP (P+D 11=des) (NP (NC+ (NC 12=courants) (P 13=d') (NC 14=air))))))) (PONCT 15=.))\\n(SENT (VN (CLS-SUJ 0=Vous) (ADV 1=n') (V 2=auriez)) (ADV 3=pas) (NP-OBJ (DET 4=un) (NC 5=paravent?)))\\n(SENT (NP (NC 0=horreur) (PP (P+D 1=des) (NP (NC+ (NC 2=courants) (P 3=d') (NC 4=air))))) (PONCT 5=...))\\n(SENT (VN (CLS-SUJ 0=Ce) (ADV 1=n') (V 2=est)) (ADV 3=pas) (NP-ATS (DET 4=de) (NC 5=chance))  (PP-MOD (P 7=pour) (NP (DET 8=une) (NC 9=plante))) (Sint-MOD  (VN (V 11=avait) (VPP 12=remarqué)) (NP-SUJ (DET 13=le) (ADJ 14=petit) (NC 15=prince))) (PONCT 16=.))\\n(SENT (NP-SUJ (DET 0=Cette) (NC 1=fleur)) (VN (V 2=est)) (AP-ATS (ADV 3=bien) (ADJ 4=compliquée)) (PONCT 5=...) (Sint-MOD (NP-MOD (DET 6=le) (NC 7=soir)) (VN (CLS-SUJ 8=vous) (CLO-OBJ 9=me) (V 10=mettrez)) (PP-P_OBJ.O (P 11=sous) (NP (NC 12=globe)))) (PONCT 13=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=fait)) (AP-ATS (ADV 2=très) (ADJ 3=froid)) (PP-MOD (P 4=chez) (NP (PRO 5=vous))) (PONCT 6=.))\\n(SENT (VN (CLS-SUJ 0=C') (V 1=est) (ADV 2=mal) (VPP 3=installé)) (PONCT 4=.))\\n(SENT (NP (ADV 0=Là) (Srel (PP-DE_OBJ (P 1=d') (NP (PROREL 2=où))) (VN (CLS-SUJ 3=je) (V 4=viens)))) (PONCT 5=...))\\n(SENT (COORD (CC 0=Mais) (VN (CLS-SUJ 1=elle) (CLR-OBJ 2=s') (V 3=était) (VPP 4=interrompue))) (PONCT 5=.))\\n(SENT (VN (CLS-SUJ 0=Elle) (V 1=était) (VPP 2=venue)) (PP-MOD (P+ (P 3=sous) (NC 4=forme) (P 5=de)) (NP (NC 6=graine))) (PONCT 7=.))\\n(SENT (VN (CLS-SUJ 0=Elle) (ADV 1=n') (V 2=avait) (PRO 3=rien) (VPP 4=pu)) (VPinf-OBJ (VN (VINF 5=connaître)) (PP-DE_OBJ (P+D 6=des) (NP (ADJ 7=autres) (NC 8=mondes)))) (PONCT 9=.))\\n(SENT (VPpart-MOD (VPP 0=Humiliée) (PP (P 1=de) (VPinf (VN (CLR 2=s') (VINF 3=être) (VPP 4=laissé)) (VPinf-OBJ (VN (VINF 5=surprendre)) (PP-A_OBJ (P 6=à) (VPinf (VN (VINF 7=préparer)) (NP-OBJ (DET 8=un) (NC 9=mensonge) (AP (ADV 10=aussi) (ADJ 11=naïf)))))))))  (VN (CLS-SUJ 13=elle) (V 14=avait) (VPP 15=toussé)) (NP-MOD (DET 16=deux) (COORD (CC 17=ou) (DET 18=trois)) (NC 19=fois))  (PP-MOD (P 21=pour) (VPinf (VN (VINF 22=mettre)) (NP-OBJ (DET 23=le) (ADJ 24=petit) (NC 25=prince)) (PP-P_OBJ (P 26=dans) (NP (DET 27=son) (NC 28=tort))))) (PONCT 29=:))\\n(SENT (NP (DET 0=ce) (NC 1=paravent)) (PONCT 2=?...))\\n(SENT (VN (CLS-SUJ 0=j') (V 1=allais)) (VPinf-OBJ (VN (CLO-OBJ 2=le) (VINF 3=chercher))) (COORD (CC 4=mais) (Sint (VN (CLS-SUJ 5=vous) (CLO-A_OBJ 6=me) (V 7=parliez)))) (PONCT 8=!))\\n(SENT (ADV 0=Alors) (VN (CLS-SUJ 1=elle) (V 2=avait) (VPP 3=forcé)) (NP-OBJ (DET 4=sa) (NC 5=toux)) (PP-MOD (P 6=pour) (VPinf (VN (CLO-A_OBJ 7=lui) (VINF 8=infliger)) (ADV+ (ADV 9=quand) (ADV 10=même)) (NP-OBJ (DET 11=des) (NC 12=remords)))) (PONCT 13=.))\\n(SENT (ADV 0=Ainsi) (NP-SUJ (DET 1=le) (ADJ 2=petit) (NC 3=prince))  (PP-MOD (P 5=malgré) (NP (DET 6=la) (NC+ (ADJ 7=bonne) (NC 8=volonté)) (PP (P 9=de) (NP (DET 10=son) (NC 11=amour)))))  (VN (V 13=avait) (ADV 14=vite) (VPP 15=douté)) (PP-DE_OBJ (P 16=d') (NP (PRO 17=elle))) (PONCT 18=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=avait) (VPP 2=pris)) (PP-A_OBJ (P+D 3=au) (AP (ADJ 4=sérieux))) (NP-OBJ (DET 5=des) (NC 6=mots) (PP (P 7=sans) (NP (NC 8=importance))))  (COORD (CC 10=et) (VN (V 11=était) (VPP 12=devenu)) (AP-ATS (ADV 13=très) (ADJ 14=malheureux))) (PONCT 15=.))\\n(SENT (VN (CLS-SUJ 0=j') (V 1=aurais) (VPP 2=dû)) (VPinf-OBJ (ADV+ (ADV 3=ne) (ADV 4=pas)) (CLO-OBJ 5=l') (VINF 6=écouter)) (Sint-MOD  (VN (VN (CLO-A_OBJ 8=me) (V 9=confia) (CLS-SUJ 10=t)) (CLS-SUJ 11=il)) (NP-MOD (DET 12=un) (NC 13=jour)))  (Sint-MOD (VN (CLS-SUJ 15=il) (ADV 16=ne) (V 17=faut)) (ADV 18=jamais) (VPinf-OBJ (VN (VINF 19=écouter)) (NP-OBJ (DET 20=les) (NC 21=fleurs)))) (PONCT 22=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=faut)) (VPinf-OBJ (VN (CLO-OBJ 2=les) (VINF 3=regarder)) (COORD (CC 4=et) (VPinf (VN (CLO-OBJ 5=les) (VINF 6=respirer))))) (PONCT 7=.))\\n(SENT (NP-SUJ (DET 0=La) (PRO 1=mienne)) (VN (V 2=embaumait)) (NP-OBJ (DET 3=ma) (NC 4=planète))  (COORD (CC 6=mais) (Sint (VN (CLS-SUJ 7=je) (ADV 8=ne) (V 9=savais)) (ADV 10=pas) (VPinf-OBJ (VN (CLR 11=m') (CLO-DE_OBJ 12=en) (VINF 13=réjouir))))) (PONCT 14=.))\\n(SENT (NP-SUJ (DET 0=Cette) (NC 1=histoire) (PP (P 2=de) (NP (NC 3=griffes)))  (Srel (NP-SUJ (PROREL 5=qui)) (VN (CLO-OBJ 6=m') (V 7=avait) (ADV 8=tellement) (VPP 9=agacé))))  (VN (V 11=eût) (VPP 12=dû)) (VPinf-OBJ (VN (CLO-OBJ 13=m') (VINF 14=attendrir))) (PONCT 15=...))\\n(SENT (VN (CLS-SUJ 0=Il) (CLO-A_OBJ 1=me) (V 2=confia)) (ADV 3=encore) (PONCT 4=:))\\n(SENT (VN (CLS-SUJ 0=je) (ADV 1=n') (V 2=ai) (ADV 3=alors) (NP-OBJ (PRO 4=rien)) (VPP 5=su)) (VPinf-OBJ (VN (VINF 6=comprendre))) (PONCT 7=!))\\n(SENT (VN (CLS-SUJ 0=J') (V 1=aurais) (VPP 2=dû)) (VPinf-OBJ (VN (CLO-OBJ 3=la) (VINF 4=juger)) (PP-MOD (P 5=sur) (NP (DET 6=les) (NC 7=actes))) (COORD (CC 8=et) (ADV 9=non) (PP (P 10=sur) (NP (DET 11=les) (NC 12=mots))))) (PONCT 13=.))\\n(SENT (VN (CLS-SUJ 0=Elle) (CLO-OBJ 1=m') (V 2=embaumait)) (COORD (CC 3=et) (VN (CLO-OBJ 4=m') (V 5=éclairait))) (PONCT 6=.))\\n(SENT (VN (CLS-SUJ 0=Je) (ADV 1=n') (V 2=aurais) (ADV 3=jamais) (VPP 4=dû)) (VPinf-OBJ (VN (CLR 5=m') (VINF 6=enfuir))) (PONCT 7=!))\\n(SENT (VN (CLS-SUJ 0=J') (V 1=aurais) (VPP 2=dû)) (VPinf-OBJ (VN (VINF 3=deviner)) (NP-OBJ (DET 4=sa) (NC 5=tendresse)) (PP-MOD (P 6=derrière) (NP (DET 7=ses) (ADJ 8=pauvres) (NC 9=ruses)))) (PONCT 10=.))\\n(SENT (NP-SUJ (DET 0=Les) (NC 1=fleurs)) (VN (V 2=sont)) (AP-ATS (ADV 3=si) (ADJ 4=contradictoires)) (PONCT 5=!))\\n(SENT (COORD (CC 0=Mais) (Sint (VN (CLS-SUJ 1=j') (V 2=étais)) (AP-ATS (ADV 3=trop) (ADJ 4=jeune) (PP (P 5=pour) (VPinf (VN (VINF 6=savoir)) (VPinf-OBJ (VN (CLO-OBJ 7=l') (VINF 8=aimer)))))))) (PONCT 9=.))\\n(SENT (VN (CLS-SUJ 0=Je) (V 1=crois)) (Ssub-OBJ (CS 2=qu') (Sint (VN (CLS-SUJ 3=il) (V 4=profita))  (PP-MOD (P 6=pour) (NP (DET 7=son) (NC 8=évasion)))  (PP-DE_OBJ (P 10=d') (NP (DET 11=une) (NC 12=migration) (PP (P 13=d') (NP (NC 14=oiseaux) (AP (ADJ 15=sauvages)))))))) (PONCT 16=.))\\n(SENT (PP-MOD (P+D 0=Au) (NP (NC 1=matin) (PP (P+D 2=du) (NP (NC 3=départ))))) (VN (CLS-SUJ 4=il) (V 5=mit)) (NP-OBJ (DET 6=sa) (NC 7=planète)) (PP-P_OBJ.O (ADV 8=bien) (P 9=en) (NP (NC 10=ordre))) (PONCT 11=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=ramona)) (ADV 2=soigneusement) (NP-OBJ (DET 3=ses) (NC 4=volcans) (PP (P 5=en) (NP (NC 6=activité)))) (PONCT 7=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=possédait)) (NP-OBJ (DET 2=deux) (NC 3=volcans) (PP (P 4=en) (NP (NC 5=activité)))) (PONCT 6=.))\\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=c') (V 2=était)) (AP-ATS (ADV 3=bien) (ADJ 4=commode)) (PP-MOD (P 5=pour) (VPinf (VN (VINF 6=faire) (VINF 7=chauffer)) (NP-OBJ (DET 8=le) (NC+ (ADJ 9=petit) (NC 10=déjeuner)) (PP (P+D 11=du) (NP (NC 12=matin)))))))) (PONCT 13=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=possédait)) (ADV 2=aussi) (NP-OBJ (DET 3=un) (NC 4=volcan) (AP (ADJ 5=éteint))) (PONCT 6=.))\\n(SENT (CC 0=Mais) (SENT (Sint  (Ssub-MOD (CS 2=comme) (Sint (VN (CLS-SUJ 3=il) (V 4=disait))))) (PONCT 5=:)))\\n(SENT (VN (CLS-SUJ 0=on) (ADV 1=ne) (V 2=sait)) (ADV 3=jamais) (PONCT 4=!))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=ramona)) (ADV 2=donc) (ADV 3=également) (NP-OBJ (DET 4=le) (NC 5=volcan) (AP (ADJ 6=éteint))) (PONCT 7=.))\\n(SENT (Ssub-MOD (CS 0=S') (Sint (VN (CLS-SUJ 1=ils) (V 2=sont) (ADV 3=bien) (VPP 4=ramonés))))  (NP-SUJ (DET 6=les) (NC 7=volcans)) (VN (V 8=brûlent)) (AdP-MOD (ADV 9=doucement) (COORD (CC 10=et) (ADV 11=régulièrement)))  (PP-MOD (P 13=sans) (NP (NC 14=éruptions))) (PONCT 15=.))\\n(SENT (NP-SUJ (DET 0=Les) (NC+ (NC 1=éruptions) (ADJ 2=volcaniques))) (VN (V 3=sont)) (PP-ATS (P 4=comme) (NP (DET 5=des) (NC+ (NC 6=feux) (P 7=de) (NC 8=cheminée)))) (PONCT 9=.))\\n(SENT (ADV 0=Évidemment) (PP-MOD (P 1=sur) (NP (DET 2=notre) (NC 3=terre))) (VN (CLS-SUJ 4=nous) (V 5=sommes)) (AP-ATS (AdP (ADV 6=beaucoup) (ADV 7=trop)) (ADJ 8=petits) (PP (P 9=pour) (VPinf (VN (VINF 10=ramoner)) (NP-OBJ (DET 11=nos) (NC 12=volcans))))) (PONCT 13=.))\\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (ADVWH 2=pourquoi) (Sint (VN (CLS-SUJ 3=ils) (CLO-A_OBJ 4=nous) (V 5=causent)) (NP-OBJ (DET 6=des) (NC 7=tas) (PP (P 8=d') (NP (NC 9=ennuis))))) (PONCT 10=.))\\n(SENT (NP-SUJ (DET 0=Le) (ADJ 1=petit) (NC 2=prince)) (VN (V 3=arracha)) (ADV 4=aussi)  (PP-MOD (P 6=avec) (NP (ADV+ (DET 7=un) (NC 8=peu)) (P 9=de) (NC 10=mélancolie)))  (NP-OBJ (DET 12=les) (ADJ 13=dernières) (NC 14=pousses) (PP (P 15=de) (NP (NC 16=baobabs)))) (PONCT 17=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=croyait)) (VPinf-OBJ (VN (ADV+ (ADV 2=ne) (ADV 3=jamais)) (VN (VINF 4=devoir))) (VPinf-OBJ (VN (VINF 5=revenir)))) (PONCT 6=.))\\n(SENT (COORD (CC 0=Mais) (Sint (NP-SUJ (ADJ 1=tous) (DET 2=ces) (NC 3=travaux) (AP (ADJ 4=familiers))) (VN (CLO-A_OBJ 5=lui) (V 6=parurent))  (NP-MOD (DET 8=ce) (NC 9=matin) (ADV 10=là))  (AP-ATS (ADV 12=extrêmement) (ADJ 13=doux)))) (PONCT 14=.))\\n(SENT (COORD (CC 0=Et) (Sint  (Ssub-MOD (CS 2=quand) (Sint (VN (CLS-SUJ 3=il) (V 4=arrosa)) (NP-MOD (DET 5=une) (ADJ 6=dernière) (NC 7=fois)) (NP-OBJ (DET 8=la) (NC 9=fleur))  (COORD (CC 11=et) (VN (CLR 12=se) (V 13=prépara)) (PP-A_OBJ (P 14=à) (VPinf (VN (CLO-OBJ 15=la) (VINF 16=mettre)) (PP-MOD (P 17=à) (NP (DET 18=l') (NC 19=abri))) (PP-MOD (P 20=sous) (NP (DET 21=son) (NC 22=globe))))))))  (VN (CLS-SUJ 24=il) (CLR 25=se) (V 26=découvrit)) (NP-OBJ (DET 27=l') (NC 28=envie) (PP (P 29=de) (VPinf (VN (VINF 30=pleurer))))))) (PONCT 31=.))\\n(SENT (I 0=adieu)  (VN (V 2=dit) (CLS-SUJ 3=il)) (PP-A_OBJ (P 4=à) (NP (DET 5=la) (NC 6=fleur))) (PONCT 7=.))\\n(SENT (COORD (CC 0=Mais) (Sint (VN (CLS-SUJ 1=elle) (ADV 2=ne) (CLO-A_OBJ 3=lui) (V 4=répondit)) (ADV 5=pas))) (PONCT 6=.))\\n(SENT (I 0=adieu)  (Sint-MOD (VN (VN (V 2=répéta) (CLS-SUJ 3=t)) (CLS-SUJ 4=il))) (PONCT 5=.))\\n(SENT (NP-SUJ (DET 0=La) (NC 1=fleur)) (VN (V 2=toussa)) (PONCT 3=.))\\n(SENT (COORD (CC 0=Mais) (Sint (VN (CLS-SUJ 1=ce) (ADV 2=n') (V 3=était)) (ADV 4=pas) (PP-ATS (P+ (P 5=à) (NC 6=cause) (P 7=de)) (NP (DET 8=son) (NC 9=rhume))))) (PONCT 10=.))\\n(SENT (VN (CLS-SUJ 0=j') (V 1=ai) (VPP 2=été)) (AP-ATS (ADJ 3=sotte)) (Sint-MOD  (VN (CLO-A_OBJ 5=lui) (V 6=dit) (CLS-SUJ 7=elle)) (ADV 8=enfin)) (PONCT 9=.))\\n(SENT (VN (CLS-SUJ 0=Je) (CLO-A_OBJ 1=te) (V 2=demande)) (NP-OBJ (NC 3=pardon)) (PONCT 4=.))\\n(SENT (NP (NC 0=Tâche)) (PP-DE_OBJ (P 1=d') (VPinf (VN (VINF 2=être)) (AP-ATS (ADJ 3=heureux)))) (PONCT 4=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=fut) (VPP 2=surpris)) (PP-P_OBJ (P 3=par) (NP (DET 4=l') (NC 5=absence) (PP (P 6=de) (NP (NC 7=reproches))))) (PONCT 8=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=restait)) (ADV 2=là) (VPpart-ATS (ADV 3=tout) (VPP 4=déconcerté))  (NP-MOD (DET 6=le) (NC 7=globe) (ADV+ (P 8=en) (DET 9=l') (NC 10=air))) (PONCT 11=.))\\n(SENT (VN (CLS-SUJ 0=Il) (ADV 1=ne) (V 2=comprenait)) (ADV 3=pas) (NP-OBJ (DET 4=cette) (NC 5=douceur) (AP (ADJ 6=calme))) (PONCT 7=.))\\n(SENT (COORD (CC 0=mais) (ADV 1=oui)  (VN (CLS-SUJ 3=je) (CLO-OBJ 4=t') (V 5=aime)) (Sint-MOD  (VN (CLO-A_OBJ 7=lui) (V 8=dit)) (NP-SUJ (DET 9=la) (NC 10=fleur)))) (PONCT 11=.))\\n(SENT (VN (CLS-SUJ 0=Tu) (ADV 1=n') (CLO 2=en) (V 3=as) (PRO 4=rien) (VPP 5=su))  (PP-MOD (P 7=par) (NP (DET 8=ma) (NC 9=faute))) (PONCT 10=.))\\n(SENT (NP-SUJ (PRO 0=Cela)) (VN (ADV 1=n') (V 2=a)) (NP-OBJ (DET 3=aucune) (NC 4=importance)) (PONCT 5=.))\\n(SENT (COORD (CC 0=Mais) (Sint (VN (CLS-SUJ 1=tu) (V 2=as) (VPP 3=été)) (AP-ATS (ADV 4=aussi) (ADJ 5=sot) (Ssub (CS 6=que) (NP (PRO 7=moi)))))) (PONCT 8=.))\\n(SENT (VN (V 0=Tâche)) (PP-DE_OBJ (P 1=d') (VPinf (VN (VINF 2=être)) (AP-ATS (ADJ 3=heureux)))) (PONCT 4=...))\\n(SENT (VN (VIMP 0=Laisse)) (NP-OBJ (DET 1=ce) (NC 2=globe)) (AP-ATO (ADJ 3=tranquille)) (PONCT 4=.))\\n(SENT (VN (CLS-SUJ 0=Je) (ADV 1=n') (CLO-OBJ 2=en) (V 3=veux)) (ADV 4=plus) (PONCT 5=.))\\n(SENT (COORD (CC 0=mais) (NP (DET 1=le) (NC 2=vent))) (PONCT 3=...))\\n(SENT (VN (CLS-SUJ 0=je) (ADV 1=ne) (V 2=suis) (ADV 3=pas) (ADV 4=si) (VPP 5=enrhumée)) (Ssub-OBJ (CS 6=que) (NP (PRO 7=ça...))))\\n(SENT (NP-SUJ (DET 0=L') (NC 1=air) (AP (ADJ 2=frais)) (PP (P 3=de) (NP (DET 4=la) (NC 5=nuit)))) (VN (CLO-A_OBJ 6=me) (V 7=fera)) (NP-OBJ (DET 8=du) (NC 9=bien)) (PONCT 10=.))\\n(SENT (VN (CLS-SUJ 0=Je) (V 1=suis)) (NP-ATS (DET 2=une) (NC 3=fleur)) (PONCT 4=.))\\n(SENT (COORD (CC 0=mais) (NP (DET 1=les) (NC 2=bêtes))) (PONCT 3=...))\\n(SENT (VN (CLS-SUJ 0=il) (V 1=faut)) (ADV 2=bien) (Ssub-OBJ (CS 3=que) (Sint (VN (CLS-SUJ 4=je) (VS 5=supporte)) (NP-OBJ (DET 6=deux) (COORD (CC 7=ou) (DET 8=trois)) (NC 9=chenilles)))) (Ssub-MOD (CS 10=si) (Sint (VN (CLS-SUJ 11=je) (V 12=veux)) (VPinf-OBJ (VN (VINF 13=connaître)) (NP-OBJ (DET 14=les) (NC 15=papillons))))) (PONCT 16=.))\\n(SENT (VN (CLS-SUJ 0=Il) (V 1=paraît)) (Ssub-ATS (CS 2=que) (Sint (VN (CLS-SUJ 3=c') (V 4=est)) (AP-ATS (ADV 5=tellement) (ADJ 6=beau)))) (PONCT 7=.))\\n(SENT (COORD (CC 0=Sinon) (Sint (NP-SUJ (PROWH 1=qui)) (VN (CLO-A_OBJ 2=me) (V 3=rendra)) (NP-OBJ (NC 4=visite)))) (PONCT 5=?))\\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=seras)) (ADV 2=loin)  (NP-MOD (PRO 4=toi)) (PONCT 5=.))\\n(SENT (PP-MOD (P+D+ (ADV 0=Quant) (P+D 1=aux)) (NP (ADJ 2=grosses) (NC 3=bêtes)))  (VN (CLS-SUJ 5=je) (ADV 6=ne) (V 7=crains)) (NP-OBJ (PRO 8=rien)) (PONCT 9=.))\\n(SENT (VN (CLS-SUJ 0=J') (V 1=ai)) (NP-OBJ (DET 2=mes) (NC 3=griffes)) (PONCT 4=.))\\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=elle) (V 2=montrait)) (ADV 3=naïvement) (NP-OBJ (DET 4=ses) (ADJ 5=quatre) (NC 6=épines)))) (PONCT 7=.))\\n(SENT (COORD (CC 0=Puis) (VN (CLS-SUJ 1=elle) (V 2=ajouta))) (PONCT 3=:))\\n(SENT (VN (ADV 0=ne) (V 1=traîne)) (ADV 2=pas) (PP-MOD (P 3=comme) (NP (PRO 4=ça))) (Sint-MOD  (VN (CLS-SUJ 6=c') (V 7=est)) (AP-ATS (ADJ 8=agaçant))) (PONCT 9=.))\\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=as) (VPP 2=décidé)) (PP-OBJ (P 3=de) (VPinf (VN (VINF 4=partir)))) (PONCT 5=.))\\n(SENT (VIMP 0=Va) (CLR 1=t'en) (PONCT 2=.))\\n(SENT (COORD (CC 0=Car) (Sint (VN (CLS-SUJ 1=elle) (ADV 2=ne) (V 3=voulait)) (ADV 4=pas) (Ssub-OBJ (CS 5=qu') (Sint (VN (CLS-SUJ 6=il) (CLO-OBJ 7=la) (VS 8=vît)) (VPinf-OBJ (VN (VINF 9=pleurer))))))) (PONCT 10=.))\\n(SENT (VN (CLS-SUJ 0=C') (V 1=était)) (NP-ATS (DET 2=une) (NC 3=fleur) (AP (ADV 4=tellement) (ADJ 5=orgueilleuse))) (PONCT 6=...))\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "for i in range(10):\n",
    "    \n",
    "    # Read the contents of the text file\n",
    "    with open(f\"/home/co/code/data/syntax_new_untested/run{i}_v2_0.25_0.5-tokenized.syntax.txt\", \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Define the pattern to match substrings like (PONCT 3=,)\n",
    "    pattern = r'\\(PONCT\\s\\d+=,\\)'\n",
    "\n",
    "    # Remove the substrings using re.sub()\n",
    "    clean_text = re.sub(pattern, '', text)\n",
    "\n",
    "    clean_text\n",
    "    # Write the cleaned text back to the file\n",
    "    with open(f\"/home/co/code/data/syntax_new_no_punct/run{i}_v2_0.25_0.5-tokenized.syntax.txt\", \"w\") as file:\n",
    "        file.write(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4454fd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoching for run 1, subject: 5\n",
      "\n",
      "Opening raw data file /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-01_meg.fif...\n",
      "    Read a total of 13 projection items:\n",
      "        grad_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v6 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v7 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v8 (1 x 306)  idle\n",
      "    Range : 89000 ... 554999 =     89.000 ...   554.999 secs\n",
      "Ready.\n",
      "Reading events from /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-01_events.tsv.\n",
      "Reading channel info from /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-01_channels.tsv.\n",
      "Using 4 HPI coils: 293 307 314 321 Hz\n",
      "Not fully anonymizing info - keeping his_id, sex, and hand info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: This file contains raw Internal Active Shielding data. It may be distorted. Elekta recommends it be run through MaxFilter to produce reliable results. Consider closing the file and running MaxFilter on the data.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: Omitted 128 annotation(s) that were outside data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: The unit for channel(s) STI001, STI002, STI003, STI004, STI005, STI006, STI007, STI008, STI009, STI010, STI011, STI012, STI013, STI014, STI015, STI016, STI101, STI201, STI301 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1468 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "Reading 0 ... 465999  =      0.000 ...   465.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 20 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 20.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 306 out of 306 | elapsed:    5.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoching for run 2, subject: 5\n",
      "\n",
      "Opening raw data file /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-02_meg.fif...\n",
      "    Read a total of 13 projection items:\n",
      "        grad_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v6 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v7 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v8 (1 x 306)  idle\n",
      "    Range : 8000 ... 514999 =      8.000 ...   514.999 secs\n",
      "Ready.\n",
      "Reading events from /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-02_events.tsv.\n",
      "Reading channel info from /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-02_channels.tsv.\n",
      "Using 4 HPI coils: 293 307 314 321 Hz\n",
      "Not fully anonymizing info - keeping his_id, sex, and hand info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: This file contains raw Internal Active Shielding data. It may be distorted. Elekta recommends it be run through MaxFilter to produce reliable results. Consider closing the file and running MaxFilter on the data.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: Omitted 134 annotation(s) that were outside data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: The unit for channel(s) STI001, STI002, STI003, STI004, STI005, STI006, STI007, STI008, STI009, STI010, STI011, STI012, STI013, STI014, STI015, STI016, STI101, STI201, STI301 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1605 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n",
      "Reading 0 ... 506999  =      0.000 ...   506.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 20 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 20.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 306 out of 306 | elapsed:    5.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoching for run 3, subject: 5\n",
      "\n",
      "Opening raw data file /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-03_meg.fif...\n",
      "    Read a total of 13 projection items:\n",
      "        grad_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v6 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v7 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v8 (1 x 306)  idle\n",
      "    Range : 7000 ... 567999 =      7.000 ...   567.999 secs\n",
      "Ready.\n",
      "Reading events from /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-03_events.tsv.\n",
      "Reading channel info from /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-03_channels.tsv.\n",
      "Using 4 HPI coils: 293 307 314 321 Hz\n",
      "Not fully anonymizing info - keeping his_id, sex, and hand info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: This file contains raw Internal Active Shielding data. It may be distorted. Elekta recommends it be run through MaxFilter to produce reliable results. Consider closing the file and running MaxFilter on the data.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: Omitted 130 annotation(s) that were outside data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: The unit for channel(s) STI001, STI002, STI003, STI004, STI005, STI006, STI007, STI008, STI009, STI010, STI011, STI012, STI013, STI014, STI015, STI016, STI101, STI201, STI301 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1718 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "Reading 0 ... 560999  =      0.000 ...   560.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 20 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 20.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 306 out of 306 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoching for run 4, subject: 5\n",
      "\n",
      "Opening raw data file /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-04_meg.fif...\n",
      "    Read a total of 13 projection items:\n",
      "        grad_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v6 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v7 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v8 (1 x 306)  idle\n",
      "    Range : 9000 ... 500999 =      9.000 ...   500.999 secs\n",
      "Ready.\n",
      "Reading events from /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-04_events.tsv.\n",
      "Reading channel info from /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-04_channels.tsv.\n",
      "Using 4 HPI coils: 293 307 314 321 Hz\n",
      "Not fully anonymizing info - keeping his_id, sex, and hand info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: This file contains raw Internal Active Shielding data. It may be distorted. Elekta recommends it be run through MaxFilter to produce reliable results. Consider closing the file and running MaxFilter on the data.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: Omitted 128 annotation(s) that were outside data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: The unit for channel(s) STI001, STI002, STI003, STI004, STI005, STI006, STI007, STI008, STI009, STI010, STI011, STI012, STI013, STI014, STI015, STI016, STI101, STI201, STI301 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1491 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19]\n",
      "Reading 0 ... 491999  =      0.000 ...   491.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 20 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 20.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m             dict_epochs[epoch_key] \u001b[38;5;241m=\u001b[39m [] \n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,runs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 49\u001b[0m     raw, meta_, events \u001b[38;5;241m=\u001b[39m \u001b[43mread_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents_return\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     meta \u001b[38;5;241m=\u001b[39m meta_\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Metadata update\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Word start\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:98\u001b[0m, in \u001b[0;36mread_raw\u001b[0;34m(subject, run_id, events_return)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# preproc raw\u001b[39;00m\n\u001b[1;32m     97\u001b[0m raw\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m---> 98\u001b[0m raw \u001b[38;5;241m=\u001b[39m \u001b[43mraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events_return:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m raw, meta, events[i]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/base.py:976\u001b[0m, in \u001b[0;36mBaseRaw.filter\u001b[0;34m(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;129m@copy_doc\u001b[39m(FilterMixin\u001b[38;5;241m.\u001b[39mfilter)\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter\u001b[39m(\u001b[38;5;28mself\u001b[39m, l_freq, h_freq, picks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filter_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    971\u001b[0m            l_trans_bandwidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, h_trans_bandwidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    974\u001b[0m            skip_by_annotation\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbad_acq_skip\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    975\u001b[0m            pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreflect_limited\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: D102\u001b[39;00m\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43miir_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miir_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfir_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfir_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfir_design\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfir_design\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_by_annotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_by_annotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-119>:12\u001b[0m, in \u001b[0;36mfilter\u001b[0;34m(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/filter.py:2023\u001b[0m, in \u001b[0;36mFilterMixin.filter\u001b[0;34m(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose)\u001b[0m\n\u001b[1;32m   2019\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m si, (start, stop) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(onsets, ends)):\n\u001b[1;32m   2020\u001b[0m     \u001b[38;5;66;03m# Only output filter params once (for info level), and only warn\u001b[39;00m\n\u001b[1;32m   2021\u001b[0m     \u001b[38;5;66;03m# once about the length criterion (longest segment is too short)\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m     use_verbose \u001b[38;5;241m=\u001b[39m verbose \u001b[38;5;28;01mif\u001b[39;00m si \u001b[38;5;241m==\u001b[39m max_idx \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 2023\u001b[0m     \u001b[43mfilter_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msfreq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miir_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfir_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfir_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfir_design\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfir_design\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_verbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[38;5;66;03m# update info if filter is applied to all data channels,\u001b[39;00m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;66;03m# and it's not a band-stop filter\u001b[39;00m\n\u001b[1;32m   2031\u001b[0m _filt_update_info(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo, update_info, l_freq, h_freq)\n",
      "File \u001b[0;32m<decorator-gen-114>:12\u001b[0m, in \u001b[0;36mfilter_data\u001b[0;34m(data, sfreq, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, copy, phase, fir_window, fir_design, pad, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/filter.py:817\u001b[0m, in \u001b[0;36mfilter_data\u001b[0;34m(data, sfreq, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, copy, phase, fir_window, fir_design, pad, verbose)\u001b[0m\n\u001b[1;32m    813\u001b[0m filt \u001b[38;5;241m=\u001b[39m create_filter(\n\u001b[1;32m    814\u001b[0m     data, sfreq, l_freq, h_freq, filter_length, l_trans_bandwidth,\n\u001b[1;32m    815\u001b[0m     h_trans_bandwidth, method, iir_params, phase, fir_window, fir_design)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfft\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 817\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_overlap_add_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    820\u001b[0m     data \u001b[38;5;241m=\u001b[39m _filtfilt(data, filt, picks, n_jobs, copy)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/filter.py:209\u001b[0m, in \u001b[0;36m_overlap_add_filter\u001b[0;34m(x, h, n_fft, phase, picks, n_jobs, copy, pad)\u001b[0m\n\u001b[1;32m    206\u001b[0m         x[p] \u001b[38;5;241m=\u001b[39m _1d_overlap_filter(x[p], \u001b[38;5;28mlen\u001b[39m(h), n_edge, phase,\n\u001b[1;32m    207\u001b[0m                                   cuda_dict, pad, n_fft)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     data_new \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_edge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcuda_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pp, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(picks):\n\u001b[1;32m    212\u001b[0m         x[p] \u001b[38;5;241m=\u001b[39m data_new[pp]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/filter.py:237\u001b[0m, in \u001b[0;36m_1d_overlap_filter\u001b[0;34m(x, n_h, n_edge, phase, cuda_dict, pad, n_fft)\u001b[0m\n\u001b[1;32m    234\u001b[0m seg \u001b[38;5;241m=\u001b[39m x_ext[start:stop]\n\u001b[1;32m    235\u001b[0m seg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([seg, np\u001b[38;5;241m.\u001b[39mzeros(n_fft \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(seg))])\n\u001b[0;32m--> 237\u001b[0m prod \u001b[38;5;241m=\u001b[39m \u001b[43m_fft_multiply_repeated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m start_filt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, start \u001b[38;5;241m-\u001b[39m shift)\n\u001b[1;32m    240\u001b[0m stop_filt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start \u001b[38;5;241m-\u001b[39m shift \u001b[38;5;241m+\u001b[39m n_fft, n_x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/cuda.py:204\u001b[0m, in \u001b[0;36m_fft_multiply_repeated\u001b[0;34m(x, cuda_dict)\u001b[0m\n\u001b[1;32m    202\u001b[0m x_fft \u001b[38;5;241m=\u001b[39m cuda_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrfft\u001b[39m\u001b[38;5;124m'\u001b[39m](x, cuda_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_fft\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    203\u001b[0m x_fft \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m cuda_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh_fft\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 204\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mcuda_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mirfft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_fft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/meg-masc/lib/python3.10/site-packages/scipy/fft/_backend.py:25\u001b[0m, in \u001b[0;36m_ScipyBackend.__ua_function__\u001b[0;34m(method, args, kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/meg-masc/lib/python3.10/site-packages/scipy/fft/_pocketfft/basic.py:97\u001b[0m, in \u001b[0;36mc2r\u001b[0;34m(forward, x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[1;32m     94\u001b[0m     tmp, _ \u001b[38;5;241m=\u001b[39m _fix_shape_1d(tmp, (n\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, axis)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Note: overwrite_x is not utilized\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc2r\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def mne_events(meta, raw, start, level):\n",
    "    if start=='onset':\n",
    "        events = np.ones((len(meta), 3), dtype=int)\n",
    "        events[:, 0] = meta.start * raw.info[\"sfreq\"]\n",
    "        return dict(events=events, metadata=meta.reset_index())\n",
    "    elif start=='offset':\n",
    "        events = np.ones((len(meta), 3), dtype=int)\n",
    "        events[:, 0] = (meta.start+meta.duration) * raw.info[\"sfreq\"]\n",
    "        return dict(events=events, metadata=meta.reset_index())\n",
    "        \n",
    "    else:\n",
    "        print('start should be either onset or offset')\n",
    "        return 0\n",
    "from dataset import read_raw, get_subjects, get_path\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "all_evos = []\n",
    "all_scores = []\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 1\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "levels = ('word')\n",
    "starts = ('onset')\n",
    "            \n",
    "for subject in subjects[2:3]:\n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    # Dict init\n",
    "    for start in starts: \n",
    "            for level in levels:\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "        meta['word_stop'] = meta.start + meta.duration\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sentence stop info\n",
    "        meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "        for s, d in meta.groupby('sentence_id'):\n",
    "            meta.loc[d.index, 'sent_word_id'] = range(len(d))\n",
    "            meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'sentence_stop'] = d.start.max() # TO Verify!\n",
    "            \n",
    "        # Adding the constituents stop info\n",
    "        meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "        for s, d in meta.groupby('constituent_id'):\n",
    "            meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'constituent_stop'] = d.start.max() # TO Verify!\n",
    "            meta.loc[d.index, 'const_word_id'] = range(len(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d0a3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>constituent_id</th>\n",
       "      <th>n_closing</th>\n",
       "      <th>const_word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Le</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cinquième</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jour</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>toujours</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grâce</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>au</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mouton</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ce</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>secret</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>de</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>la</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vie</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>du</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>petit</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>prince</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>me</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fut</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>révélé</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Il</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>me</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>demanda</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>avec</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>brusquerie</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sans</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>préambule</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>comme</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>le</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>fruit</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>dun</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>problème</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>longtemps</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>médité</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>en</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>silence</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>un</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mouton</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>sil</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>mange</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>les</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>arbustes</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>il</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>mange</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>aussi</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>les</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>fleurs</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>un</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>mouton</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>mange</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tout</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ce</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  constituent_id  n_closing  const_word_id\n",
       "0           Le               0          1            0.0\n",
       "1    cinquième               0          1            1.0\n",
       "2         jour               0          2            2.0\n",
       "3     toujours               1          1            0.0\n",
       "4        grâce               1          1            1.0\n",
       "5           au               1          1            2.0\n",
       "6       mouton               1          3            3.0\n",
       "7           ce               1          3            4.0\n",
       "8       secret               2          1            0.0\n",
       "9           de               2          1            1.0\n",
       "10          la               2          1            2.0\n",
       "11         vie               2          1            3.0\n",
       "12          du               2          1            4.0\n",
       "13       petit               2          1            5.0\n",
       "14      prince               2          1            6.0\n",
       "15          me               2          1            7.0\n",
       "16         fut               2          6            8.0\n",
       "17      révélé               3          1            0.0\n",
       "18          Il               3          1            1.0\n",
       "19          me               3          2            2.0\n",
       "20     demanda               3          2            3.0\n",
       "21        avec               4          1            0.0\n",
       "22  brusquerie               4          1            1.0\n",
       "23        sans               4          2            2.0\n",
       "24   préambule               5          1            0.0\n",
       "25       comme               5          3            1.0\n",
       "26          le               6          1            0.0\n",
       "27       fruit               6          1            1.0\n",
       "28         dun               6          1            2.0\n",
       "29    problème               6          1            3.0\n",
       "30   longtemps               6          1            4.0\n",
       "31      médité               6          1            5.0\n",
       "32          en               6          1            6.0\n",
       "33     silence               6          1            7.0\n",
       "34          un               6          1            8.0\n",
       "35      mouton               6          1            9.0\n",
       "36         sil               6          1           10.0\n",
       "37       mange               6          1           11.0\n",
       "38         les               6          7           12.0\n",
       "39    arbustes               6          2           13.0\n",
       "40          il               7          1            0.0\n",
       "41       mange               7          2            1.0\n",
       "42       aussi               8          1            0.0\n",
       "43         les               8          1            1.0\n",
       "44      fleurs               8          1            2.0\n",
       "45          un               8          1            3.0\n",
       "46      mouton               8          2            4.0\n",
       "47       mange               9          1            0.0\n",
       "48        tout               9          4            1.0\n",
       "49          ce              10          1            0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[['word', 'constituent_id','n_closing','const_word_id']][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9799f565",
   "metadata": {},
   "source": [
    "# Initial Plotting for ERPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572b284",
   "metadata": {},
   "source": [
    "### 0.5\n",
    "\n",
    "Just the evoked, for all conditions, all subjects:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb924f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It is currently being done by keeping all the epochs in memory: might want to do like in the #1, and generate the evo\n",
    "# or score from the epochs (for a subject), and from there try\n",
    "# To find a way to, starting with an array of evoked, average them!!!\n",
    "from dataset import read_raw, get_subjects, get_path, mne_events\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "all_evos = []\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 9\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "# Dict init\n",
    "for start in ('onset', 'offset'): \n",
    "        for level in ('word', 'constituent', 'sentence'):\n",
    "            epoch_key = f'{level}_{start}'\n",
    "            dict_epochs[epoch_key] = [] \n",
    "            \n",
    "for subject in subjects[2:5]:\n",
    "    all_epochs = []\n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "\n",
    "        # Word end\n",
    "        meta['word_offset'] = True\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Sent stop\n",
    "        meta['next_word_id'] = meta['word_id'].shift(-1)\n",
    "        meta['sentence_offset'] = meta.apply(lambda x: True if x['word_id'] > x['next_word_id'] else False, axis=1)\n",
    "        meta['sentence_offset'].fillna(False, inplace=True)\n",
    "        meta.drop('next_word_id', axis=1, inplace=True)\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: True if x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1 else False, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "\n",
    "        # Const stop\n",
    "        meta['next_closing'] = meta['n_closing'].shift(-1)\n",
    "        meta['constituent_offset'] = meta.apply(lambda x: True if x['n_closing'] > x['next_closing'] else False, axis=1)\n",
    "        meta['constituent_offset'].fillna(False, inplace=True)\n",
    "        meta.drop('next_closing', axis=1, inplace=True)\n",
    "\n",
    "        for start in ('onset', 'offset'): \n",
    "            # for level in ('word', 'constituent', 'sentence'):\n",
    "            for level in ('sentence', 'constituent', 'word'):\n",
    "                # Select only the rows containing the True for the conditions (sentence_end, etc..)\n",
    "                sel = meta.query(f'{level}_{start}==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "                # TODO check variance as well for sentences\n",
    "                # Matchlist events and meta\n",
    "                # So that we can epoch now that's we've sliced our metadata\n",
    "                i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "                sel = sel.reset_index().loc[j]\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw), decim = 10,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop',\n",
    "                                            preload=True)  # n_words OR n_constitutent OR n_sentences\n",
    "                epoch_key = f'{level}_{start}'\n",
    "            \n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "\n",
    "# Once we have the dict of epochs per condition full, we can concatenate them, and fix the dev_head             \n",
    "for start_ in ('onset', 'offset'): \n",
    "    for level_ in ('word', 'constituent', 'sentence'):\n",
    "        epoch_key = f'{level_}_{start_}'\n",
    "        all_epochs_chosen = dict_epochs[epoch_key]\n",
    "        # Concatenate epochs\n",
    "        for epo in all_epochs_chosen:\n",
    "            epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "        dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n",
    "            \n",
    "dict_evos = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "# Dict init\n",
    "for start in ('onset', 'offset'): \n",
    "        for level in ('word', 'constituent', 'sentence'):\n",
    "            epoch_key = f'{level}_{start}'\n",
    "            dict_evos[epoch_key] = [] \n",
    "\n",
    "# Now that we have all the epochs, rerun the plotting / decoding on averaged\n",
    "for start in ('onset', 'offset'): \n",
    "        for level in ('word', 'constituent', 'sentence'):  \n",
    "            epoch_key = f'{level}_{start}'\n",
    "            epochs = dict_epochs[epoch_key]\n",
    "            # mean\n",
    "            evo = epochs.copy().pick_types(meg=True).average(method='median')\n",
    "            dict_evos[epoch_key] = evo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740163c2",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e5db5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for level in ('word', 'constituent', 'sentence'):\n",
    "    for start in ('onset', 'offset'):        \n",
    "            epoch_key = f'{level}_{start}'\n",
    "            print(f\"Plotting for: {epoch_key}\")\n",
    "            dict_evos[epoch_key].plot(gfp=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb01a54",
   "metadata": {},
   "source": [
    "### #1\n",
    "First plot: 3x2 plot, that shows:\n",
    "- From the onset, and offset of {word, constituent, sentence}:\n",
    "\n",
    "The evoked potential linked to it, as well as the decoding of the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38245f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mne_events(meta, raw, start, level):\n",
    "    if start=='onset':\n",
    "        events = np.ones((len(meta), 3), dtype=int)\n",
    "        events[:, 0] = meta.start * raw.info[\"sfreq\"]\n",
    "        return dict(events=events, metadata=meta.reset_index())\n",
    "    elif start=='offset':\n",
    "        \"\"\"\n",
    "        It should generalize, no need for different cases?\n",
    "        if level == 'word':\n",
    "            events = np.ones((len(meta), 3), dtype=int)\n",
    "            events[:, 0] = (meta.start+meta.duration) * raw.info[\"sfreq\"]\n",
    "            return dict(events=events, metadata=meta.reset_index())\n",
    "        elif level == 'sentence':\n",
    "            events = np.ones((len(meta), 3), dtype=int)\n",
    "            events[:, 0] = (meta.start+meta.duration) * raw.info[\"sfreq\"]\n",
    "            return dict(events=events, metadata=meta.reset_index())\n",
    "\n",
    "        else:\n",
    "            print('hi')\n",
    "            # Fill\n",
    "        \"\"\"\n",
    "        events = np.ones((len(meta), 3), dtype=int)\n",
    "        events[:, 0] = (meta.start+meta.duration) * raw.info[\"sfreq\"]\n",
    "        return dict(events=events, metadata=meta.reset_index())\n",
    "        \n",
    "    else:\n",
    "        print('start should be either onset or offset')\n",
    "        return 0\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15915c86",
   "metadata": {},
   "source": [
    "# Test on words offset only # Done\n",
    "\n",
    "# Generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fb212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataset import read_raw, get_subjects, get_path\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "all_evos = []\n",
    "all_scores = []\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 9\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "levels = ('word','constituent','sentence')\n",
    "starts = ('onset', 'offset')\n",
    "            \n",
    "for subject in subjects[2:6]:\n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    # Dict init\n",
    "    for start in starts: \n",
    "            for level in levels:\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "        meta['word_stop'] = meta.start + meta.duration\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sentence stop info\n",
    "        meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "        for s, d in meta.groupby('sentence_id'):\n",
    "            meta.loc[d.index, 'sent_word_id'] = range(len(d))\n",
    "            meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'sentence_stop'] = d.start.max() # TO Verify!\n",
    "            \n",
    "        # Adding the constituents stop info\n",
    "        meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "        for s, d in meta.groupby('constituent_id'):\n",
    "            meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'constituent_stop'] = d.start.max() # TO Verify!\n",
    "            meta.loc[d.index, 'const_word_id'] = range(len(d))\n",
    "\n",
    "        for start in starts: \n",
    "            # for level in ('word', 'constituent', 'sentence'):\n",
    "            # for level in ('sentence', 'constituent', 'word'):\n",
    "            for level in levels:\n",
    "                \n",
    "                # Select only the rows containing the True for the conditions\n",
    "                # Simplified to only get for the onset: sentence onset epochs, constituent onset epochs,etc\n",
    "                sel = meta.query(f'{level}_onset==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "                # TODO check variance as well for sentences\n",
    "                # Matchlist events and meta\n",
    "                # So that we can epoch now that's we've sliced our metadata\n",
    "                i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "                sel = sel.reset_index().loc[j]\n",
    "                # Making sure there is not hidden bug when matching\n",
    "                assert sel.shape[0] > 0.8 *  (meta.query(f'{level}_onset==True')).shape[0]\n",
    "\n",
    "                # Epoching from the metadata having all onset events: if the start=Offset, the mne events\n",
    "                # Function will epoch on the offset of each level instead of the onset\n",
    "                # TODO: add adaptative baseline\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop', # check event repeated\n",
    "                                            preload=True,\n",
    "                                                baseline=None)  # n_words OR n_constitutent OR n_sentences\n",
    "                epoch_key = f'{level}_{start}'\n",
    "\n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "            \n",
    "    # Once we have the dict of epochs per condition full (epoching for each run for a subject)\n",
    "    # we can concatenate them, and fix the dev_head             \n",
    "    for start_ in starts: \n",
    "        for level_ in levels:\n",
    "            epoch_key = f'{level_}_{start_}'\n",
    "            all_epochs_chosen = dict_epochs[epoch_key]\n",
    "            # Concatenate epochs\n",
    "\n",
    "            for epo in all_epochs_chosen:\n",
    "                epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "            dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n",
    "\n",
    "    # Now that we have all the epochs, rerun the plotting / decoding on averaged\n",
    "    for start in starts: \n",
    "        for level in levels:\n",
    "            epoch_key = f'{level}_{start}'\n",
    "            epochs = dict_epochs[epoch_key]\n",
    "            # mean\n",
    "            evo = epochs.copy().pick_types(meg=True).average(method='median')\n",
    "            all_evos.append(dict(subject=subject, evo=evo, start=start, level=level))\n",
    "\n",
    "\n",
    "            # decoding word emb\n",
    "            epochs = epochs.load_data().pick_types(meg=True, stim=False, misc=False)\n",
    "            X = epochs.get_data()\n",
    "            embeddings = epochs.metadata.word.apply(lambda word: nlp(word).vector).values\n",
    "            embeddings = np.array([emb for emb in embeddings])\n",
    "            R_vec = decod_xy(X, embeddings)\n",
    "            scores = np.mean(R_vec, axis=1)\n",
    "\n",
    "            for t, score in enumerate(scores):\n",
    "                all_scores.append(dict(subject=subject, score=score, start=start, level=level, t=epochs.times[t]))\n",
    "\n",
    "all_scores = pd.DataFrame(all_scores)\n",
    "all_evos = pd.DataFrame(all_evos)\n",
    "\n",
    "all_scores.to_csv('./score.csv')\n",
    "all_evos.to_csv('./evos.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e983491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores = pd.DataFrame(all_scores)\n",
    "all_evos = pd.DataFrame(all_evos)\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(16, 10), dpi=80)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2)\n",
    "\n",
    "for axes_, level in zip( axes, levels):  \n",
    "    for ax, start in zip( axes_, starts):  \n",
    "        cond1 = all_scores.level==f'{level}'\n",
    "        cond2 = all_scores.start==f'{start}'\n",
    "        data = all_scores[ cond1 & cond2]\n",
    "        y = []\n",
    "        x = []\n",
    "        for s, t in data.groupby('t'):\n",
    "            score_avg = t.score.mean()\n",
    "            y.append(score_avg)\n",
    "            x.append(s)\n",
    "\n",
    "        ax.plot(x,y)\n",
    "        ax.set_title(f'{level} {start}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67755eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel[['word','start']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11151285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import read_raw, get_subjects, get_path\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "all_evos = []\n",
    "all_scores = []\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 9\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "\n",
    "            \n",
    "for subject in subjects[2]:\n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    # Dict init\n",
    "    for start in ('onset', 'offset'): \n",
    "            for level in ('word', 'constituent', 'sentence'):\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "    for run in range(1,2):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "        meta['word_stop'] = meta.start + meta.duration\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sentence stop info\n",
    "        meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "        for s, d in meta.groupby('sentence_id'):\n",
    "            meta.loc[d.index, 'sent_word_id'] = range(len(d))\n",
    "            meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'sentence_stop'] = d.start.max()\n",
    "            \n",
    "        # Adding the constituents stop info\n",
    "        meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "        for s, d in meta.groupby('constituent_id'):\n",
    "            meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'constituent_stop'] = d.start.max()\n",
    "            meta.loc[d.index, 'const_word_id'] = range(len(d))\n",
    "        print(meta.head(50))\n",
    "        \n",
    "        for start in ('onset', 'offset'): \n",
    "            # for level in ('word', 'constituent', 'sentence'):\n",
    "            # for level in ('sentence', 'constituent', 'word'):\n",
    "            level = 'word'\n",
    "                \n",
    "            # Select only the rows containing the True for the conditions\n",
    "            # Simplified to only get for the onset: sentence onset epochs, constituent onset epochs,etc\n",
    "            sel = meta.query(f'{level}_onset==True')\n",
    "            assert sel.shape[0] > 10  #\n",
    "            # TODO check variance as well for sentences\n",
    "            # Matchlist events and meta\n",
    "            # So that we can epoch now that's we've sliced our metadata\n",
    "            i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "            sel = sel.reset_index().loc[j]\n",
    "            # Making sure there is not hidden bug when matching\n",
    "            assert sel.shape[0] > 0.8 *  (meta.query(f'{level}_onset==True')).shape[0]\n",
    "\n",
    "            # Epoching from the metadata having all onset events: if the start=Offset, the mne events\n",
    "            # Function will epoch on the offset of each level instead of the onset\n",
    "            # TODO: add adaptative baseline\n",
    "            epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
    "                                 tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                   tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                     event_repeated = 'drop', # check event repeated\n",
    "                                        preload=True,\n",
    "                                            baseline=None)  # n_words OR n_constitutent OR n_sentences\n",
    "            epoch_key = f'{level}_{start}'\n",
    "\n",
    "            dict_epochs[epoch_key].append(epochs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9906493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs.copy().pick_types(meg=True).average(method='median').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed860c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import read_raw, get_subjects, get_path, mne_events\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "all_evos = []\n",
    "all_scores = []\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 9\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "\n",
    "            \n",
    "for subject in subjects[2:10]:\n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    # Dict init\n",
    "    for start in ('onset', 'offset'): \n",
    "            for level in ('word', 'constituent', 'sentence'):\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: True if x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1 else False, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sequence\n",
    "        meta['sequence_id'] = np.cumsum(meta.is_last_word.shift(1, fill_value=False))\n",
    "        for s, d in meta.groupby('sequence_id'):\n",
    "            meta.loc[d.index, 'word_id'] = range(len(d))\n",
    "        \n",
    "\n",
    "        for start in ('onset', 'offset'): \n",
    "            # for level in ('word', 'constituent', 'sentence'):\n",
    "            for level in ('sentence', 'constituent', 'word'):\n",
    "                # Select only the rows containing the True for the conditions (sentence_end, etc..)\n",
    "                sel = meta.query(f'{level}_{start}==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "                # TODO check variance as well for sentences\n",
    "                # Matchlist events and meta\n",
    "                # So that we can epoch now that's we've sliced our metadata\n",
    "                i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "                sel = sel.reset_index().loc[j]\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw), decim = 100,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop', # check event repeated\n",
    "                                            preload=True)  # n_words OR n_constitutent OR n_sentences\n",
    "                epoch_key = f'{level}_{start}'\n",
    "            \n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "        \n",
    "            \n",
    "    # Once we have the dict of epochs per condition full (epoching for each run for a subject)\n",
    "    # we can concatenate them, and fix the dev_head             \n",
    "    for start_ in ('onset', 'offset'): \n",
    "        for level_ in ('word', 'constituent', 'sentence'):\n",
    "            epoch_key = f'{level_}_{start_}'\n",
    "            all_epochs_chosen = dict_epochs[epoch_key]\n",
    "            # Concatenate epochs\n",
    "            for epo in all_epochs_chosen:\n",
    "                epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "            dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n",
    "\n",
    "    # Now that we have all the epochs, rerun the plotting / decoding on averaged\n",
    "    for start in ('onset', 'offset'): \n",
    "            for level in ('word', 'constituent', 'sentence'):  \n",
    "                epoch_key = f'{level}_{start}'\n",
    "                epochs = dict_epochs[epoch_key]\n",
    "                # mean\n",
    "                evo = epochs.copy().pick_types(meg=True).average(method='median')\n",
    "                all_evos.append(dict(subject=subject, evo=evo, start=start, level=level))\n",
    "\n",
    "\n",
    "                # decoding word emb\n",
    "                epochs = epochs.load_data().pick_types(meg=True, stim=False, misc=False)\n",
    "                X = epochs.get_data()\n",
    "                embeddings = epochs.metadata.word.apply(lambda word: nlp(word).vector).values\n",
    "                embeddings = np.array([emb for emb in embeddings])\n",
    "                R_vec = decod_xy(X, embeddings)\n",
    "                scores = np.mean(R_vec, axis=1)\n",
    "\n",
    "                for t, score in enumerate(scores):\n",
    "                    all_scores.append(dict(subject=subject, score=score, start=start, level=level, t=epochs.times[t]))\n",
    "\n",
    "all_scores = pd.DataFrame(all_scores,index=False)\n",
    "all_evos = pd.DataFrame(all_evos,index=False)\n",
    "\n",
    "all_scores.to_csv('./score.csv')\n",
    "all_evos.to_csv('./evos.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = pd.DataFrame(all_scores)\n",
    "all_evos = pd.DataFrame(all_evos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores.query('level==\"sentence\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(16, 10), dpi=80)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2)\n",
    "\n",
    "for axes_, level in zip(axes, ('word', 'constituent', 'sentence')):\n",
    "    for ax, start in zip( axes_, ('onset', 'offset')):  \n",
    "        cond1 = all_scores.level==f'{level}'\n",
    "        cond2 = all_scores.start==f'{start}'\n",
    "        data = all_scores[ cond1 & cond2]\n",
    "        print(data.shape)\n",
    "        x = data['t']\n",
    "        y = data['score']\n",
    "        \n",
    "        ax.plot(x,y)\n",
    "        ax.set_title(f'{level} {start}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd270ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67f9be",
   "metadata": {},
   "source": [
    "###  #3\n",
    "Now the same idea, but iterating on the targeted decoding: {word embedding, sentence embedding, etc..}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34178be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b72a766",
   "metadata": {},
   "source": [
    "### #4 \n",
    "Now baselined on offset, no matter whether it's on onset or offset window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c77b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "824fb580",
   "metadata": {},
   "source": [
    "### #5 \n",
    "Submitit-compatible version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1494a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install submitit\n",
    "import submitit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621358ab",
   "metadata": {},
   "source": [
    "# Jitter test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c5154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3507de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne_bids\n",
    "import mne\n",
    "from pathlib import Path\n",
    "subject = '5'\n",
    "run_id = 1\n",
    "path = get_path(\"LPP_read\")\n",
    "task = \"read\"\n",
    "print(f\"\\n Epoching for run {run_id}, subject: {subject}\\n\")\n",
    "bids_path = mne_bids.BIDSPath(\n",
    "    subject=subject,\n",
    "    session=\"01\",\n",
    "    task=task,\n",
    "    datatype=\"meg\",\n",
    "    root=path,\n",
    "    run=run_id,\n",
    ")\n",
    "\n",
    "raw = mne_bids.read_raw_bids(bids_path)\n",
    "raw.del_proj()  # To fix proj issues\n",
    "raw.pick_types(meg=True, stim=True)\n",
    "\n",
    "# Generate event_file path\n",
    "event_file = path / f\"sub-{bids_path.subject}\"\n",
    "event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "event_file = event_file / \"meg\"\n",
    "event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "event_file += f\"_ses-{bids_path.session}\"\n",
    "event_file += f\"_task-{bids_path.task}\"\n",
    "event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "assert Path(event_file).exists()\n",
    "\n",
    "events = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "events_  = events[:,0] / raw.info[\"sfreq\"]\n",
    "diffs = np.diff(events_)\n",
    "x,y = np.unique(diffs, return_counts=True)\n",
    "plt.plot(x,y)\n",
    "plt.xlim([0.2,0.3])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa654f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw.copy().pick_types(meg=False, stim=True).plot(start=50, duration=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4566943",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = mne.find_events(raw, stim_channel=\"STI002\", shortest_event=1)\n",
    "events_  = events[:,0] / raw.info[\"sfreq\"]\n",
    "diffs = np.diff(events_)\n",
    "x,y = np.unique(diffs, return_counts=True)\n",
    "plt.plot(x,y)\n",
    "plt.xlim([0.2,0.4])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "meta = epochs.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb0fef",
   "metadata": {},
   "source": [
    "# Testing decoding more and more difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import word_epochs_debug, get_path, get_subjects, sentence_epochs_debug\n",
    "from utils import decod\n",
    "from plot import plot_R\n",
    "import mne\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "subjects = subjects[:2]\n",
    "\n",
    "# WORDS\n",
    "all_epochs = []\n",
    "for sub in subjects:\n",
    "\n",
    "    epochs = sentence_epochs_debug(sub, 3)\n",
    "    all_epochs.append(epochs)\n",
    "\n",
    "for epo in all_epochs:\n",
    "    epo.info[\"dev_head_t\"] = all_epochs[1].info[\"dev_head_t\"]\n",
    "\n",
    "epochs = mne.concatenate_epochs(all_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import numpy as np\n",
    "from utils import correlate\n",
    "\n",
    "def decod(X, y):\n",
    "    assert len(X) == len(y)\n",
    "    # define data\n",
    "    model = make_pipeline(StandardScaler(), RidgeCV(alphas=np.logspace(-1, 6, 10)))\n",
    "    cv = KFold(15, shuffle=True, random_state=0)\n",
    "\n",
    "    # fit predict\n",
    "    n, n_chans, n_times = X.shape\n",
    "    if y.ndim == 1:\n",
    "        y = np.asarray(y).reshape(y.shape[0], 1)\n",
    "    R = np.zeros((n_times, y.shape[1]))\n",
    "\n",
    "    for t in range(n_times):\n",
    "        print(\".\", end=\"\")\n",
    "        rs = []\n",
    "        # y_pred = cross_val_predict(model, X[:, :, t], y, cv=cv)\n",
    "        for train, test in cv.split(X):\n",
    "            model.fit(X[train, :, t], y[train])\n",
    "            y_pred = model.predict(X[test, :, t])\n",
    "            r = correlate(y[test], y_pred)\n",
    "            rs.append(r)\n",
    "        R[t] = np.mean(rs)\n",
    "        # R[t] = correlate(y, y_pred)\n",
    "\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d520c21",
   "metadata": {},
   "source": [
    "# LASER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54996ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_code_path\n",
    "\n",
    "run_id = 1\n",
    "CHAPTERS = {\n",
    "1: \"1-3\",\n",
    "2: \"4-6\",\n",
    "3: \"7-9\",\n",
    "4: \"10-12\",\n",
    "5: \"13-14\",\n",
    "6: \"15-19\",\n",
    "7: \"20-22\",\n",
    "8: \"23-25\",\n",
    "9: \"26-27\",\n",
    "}\n",
    "\n",
    "meta = epochs.metadata\n",
    "\n",
    "# # laser embeddings information\n",
    "dim = 1024\n",
    "embeds = np.fromfile(\n",
    "    f\"{get_code_path()}/data/laser_embeddings/emb_{CHAPTERS[int(run_id)]}.bin\",\n",
    "    dtype=np.float32,\n",
    "    count=-1,\n",
    ")\n",
    "embeds.resize(embeds.shape[0] // dim, dim)\n",
    "print(meta.shape[0])\n",
    "embeds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23935db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = epochs[]\n",
    "X = epochs.get_data()\n",
    "y = epochs.metadata.word.apply(len)\n",
    "R_vec = decod(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0203d6",
   "metadata": {},
   "source": [
    "## Word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cdd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the STIM information before decoding it (or else we'll get a 100% accuracy since the word length info is in the STIM channels)\n",
    "epochs = epochs.pick_types(meg=True, stim=False, misc=False)\n",
    "X = epochs.get_data()\n",
    "y = epochs.metadata.word.apply(len)\n",
    "R_vec = decod(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f798f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_R(R_vec.reshape(-1))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf385dc",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becba2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "epochs = epochs.pick_types(meg=True, stim=False, misc=False)\n",
    "X = epochs.get_data()\n",
    "embeddings = epochs.metadata.word.apply(lambda word: nlp(word).vector).values\n",
    "embeddings = np.array([emb for emb in embeddings])\n",
    "R_vec = decod(X, embeddings)\n",
    "R_vec = np.mean(R_vec, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_R(R_vec.reshape(-1))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c799fbc7",
   "metadata": {},
   "source": [
    "## Laser embeddings with JR baseline fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29290403",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_stop_data.shape\n",
    "baseline_starts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_starts = epochs['word_id==0'].apply_baseline((-.300, 0.))\n",
    "sent_starts.average().plot()\n",
    "\n",
    "sent_stops = epochs['is_last_word']\n",
    "bsl = (epochs.times>-.300 )*(epochs.times<=0)\n",
    "baseline_starts = sent_starts.get_data()[:, :, bsl].mean(-2)\n",
    "\n",
    "sent_stop_data = sent_stops.get_data()\n",
    "n_sentences, n_channels, n_times = sent_stop_data.shape\n",
    "sent_stop_data -= baseline_starts[:, :, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c02e3a",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ce669",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.metaepochs.metadata.word.apply(len)\n",
    "decoding_criterion = \"n_closing\"\n",
    "R_vec = decod(epochs, decoding_criterion)\n",
    "\n",
    "fig = plot_R(R_vec)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb3d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce49ba49",
   "metadata": {},
   "source": [
    "# Generate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_path, get_subjects, word_epochs, sentence_epochs\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import mne\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "mne.set_log_level(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = mne.Report()\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "evos = []\n",
    "\n",
    "# WORDS\n",
    "subjects = subjects[2]\n",
    "epochs = word_epochs(subjects)\n",
    "\n",
    "evo = epochs.average(method=\"median\")\n",
    "evos.append(evo)\n",
    "evo.plot(spatial_colors=True)\n",
    "report.add_evokeds(evo, titles=f\"Evoked for condition word  \")\n",
    "\n",
    "\n",
    "# SENTENCES\n",
    "epochs = sentence_epochs(subjects)\n",
    "\n",
    "evo = epochs.average(method=\"median\")\n",
    "evos.append(evo)\n",
    "evo.plot(spatial_colors=True)\n",
    "report.add_evokeds(evo, titles=f\"Evoked for condition sentence  \")\n",
    "\n",
    "\n",
    "evokeds = dict(sentence=evos[1], word=evos[0])\n",
    "\n",
    "fig = mne.viz.plot_compare_evokeds(evokeds, combine=\"mean\")\n",
    "\n",
    "report.add_figure(fig, title=\"Evoked response comparaison\")\n",
    "\n",
    "\n",
    "report.save(\n",
    "    f\"./figures/{task}_sentvsword_test.html\",\n",
    "    open_browser=False,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "evo = epochs.average(method=\"median\")\n",
    "evo.plot(spatial_colors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc17c75",
   "metadata": {},
   "source": [
    "# Test new functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import word_epochs\n",
    "\n",
    "sub = '3'\n",
    "\n",
    "epochs = word_epochs(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca576c",
   "metadata": {},
   "source": [
    "\n",
    "# Debug events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8123ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne_bids\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import mne\n",
    "from utils import match_list\n",
    "from dataset import mne_events\n",
    "from utils import add_syntax\n",
    "\n",
    "\n",
    "CHAPTERS = {\n",
    "    1: \"1-3\",\n",
    "    2: \"4-6\",\n",
    "    3: \"7-9\",\n",
    "    4: \"10-12\",\n",
    "    5: \"13-14\",\n",
    "    6: \"15-19\",\n",
    "    7: \"20-22\",\n",
    "    8: \"23-25\",\n",
    "    9: \"26-27\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471fd7ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "run_id = 1\n",
    "task = \"read\"\n",
    "subject = '3'\n",
    "baseline_min = -2.0\n",
    "baseline_max = 0.5\n",
    "epoch_on = 'sentence'\n",
    "reference = \"end\"\n",
    "print(f\"\\n Epoching for run {run_id}, subject: {subject}\\n\")\n",
    "bids_path = mne_bids.BIDSPath(\n",
    "    subject=subject,\n",
    "    session=\"01\",\n",
    "    task=task,\n",
    "    datatype=\"meg\",\n",
    "    root=path,\n",
    "    run=run_id,\n",
    ")\n",
    "\n",
    "raw = mne_bids.read_raw_bids(bids_path)\n",
    "raw.del_proj()  # To fix proj issues\n",
    "raw.pick_types(meg=True, stim=True)\n",
    "raw.load_data()\n",
    "raw = raw.filter(0.5, 20)\n",
    "# Generate event_file path\n",
    "event_file = path / f\"sub-{bids_path.subject}\"\n",
    "event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "event_file = event_file / \"meg\"\n",
    "event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "event_file += f\"_ses-{bids_path.session}\"\n",
    "event_file += f\"_task-{bids_path.task}\"\n",
    "event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "assert Path(event_file).exists()\n",
    "\n",
    "# read events\n",
    "meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "events = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_copy = meta.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ba504",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_syntax = get_code_path() / \"data/syntax\"\n",
    "\n",
    "# Enriching the metadata with outside files:\n",
    "meta = add_syntax(meta, path_syntax, int(run_id))\n",
    "\n",
    "# Enriching the metadata with simple operations:\n",
    "\n",
    "# end of sentence information\n",
    "end_of_sentence = [\n",
    "    True\n",
    "    if str(meta.word.iloc[i]).__contains__(\".\")\n",
    "    or str(meta.word.iloc[i]).__contains__(\"?\")\n",
    "    or str(meta.word.iloc[i]).__contains__(\"!\")\n",
    "    else False\n",
    "    for i, _ in enumerate(meta.values[:-1])\n",
    "]\n",
    "end_of_sentence.append(True)\n",
    "meta[\"sentence_end\"] = end_of_sentence\n",
    "\n",
    "# sentence start information\n",
    "list_word_start = [True]\n",
    "list_word_start_to_add = [\n",
    "    True if meta.sentence_end.iloc[i - 1] else False\n",
    "    for i in np.arange(1, meta.shape[0])\n",
    "]\n",
    "for boolean in list_word_start_to_add:\n",
    "    list_word_start.append(boolean)\n",
    "meta[\"sentence_start\"] = list_word_start\n",
    "\n",
    "# laser embeddings information\n",
    "dim = 1024\n",
    "embeds = np.fromfile(\n",
    "    f\"{get_code_path()}/data/laser_embeddings/emb_{CHAPTERS[int(run_id)]}.bin\",\n",
    "    dtype=np.float32,\n",
    "    count=-1,\n",
    ")\n",
    "embeds.resize(embeds.shape[0] // dim, dim)\n",
    "assert embeds.shape[0] == meta.shape[0]\n",
    "meta[\"laser\"] = [emb for emb in embeds]\n",
    "\n",
    "# constituent end information\n",
    "meta[\"constituent_end\"] = [\n",
    "        True if closing > 1 else False for i, closing in enumerate(meta.n_closing)]\n",
    "\n",
    "# constituent start information\n",
    "list_constituent_start = [True]\n",
    "list_constituent_start_to_add = [\n",
    "    True if meta.constituent_end.iloc[i - 1] else False\n",
    "    for i in np.arange(1, meta.shape[0])\n",
    "]\n",
    "for boolean in list_constituent_start_to_add:\n",
    "    list_constituent_start.append(boolean)\n",
    "meta[\"constituent_start\"] = list_constituent_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764fea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_length_meg = events[:, 2]\n",
    "word_len_meta = meta.word.apply(len)\n",
    "i, j = match_list(word_len_meta, word_length_meg)\n",
    "events = events[j]\n",
    "assert len(i) / meta.shape[0] > 0.8\n",
    "meta = meta.iloc[i].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c337c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[\"start\"] = events[:, 0] / raw.info[\"sfreq\"]\n",
    "meta[\"condition\"] = \"sentence\"\n",
    "meta = meta.sort_values(\"start\").reset_index(drop=True)\n",
    "meta[\"word_start\"] = meta[\"start\"]\n",
    "meta[\"word_end\"] = meta[\"word_start\"] + meta[\"duration\"]\n",
    "\n",
    "epochs = mne.Epochs(\n",
    "    raw, **mne_events(meta, raw), decim=20, tmin=baseline_min, tmax=baseline_max\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5330c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feec9e4",
   "metadata": {},
   "source": [
    "# Plotting decoding info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743045da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plot import plot_subject\n",
    "\n",
    "from dataset import get_path, get_subjects, get_code_path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"read\"\n",
    "sub = 4\n",
    "epoch_on = 'sentence'\n",
    "reference = \"end\"\n",
    "min = -4.0\n",
    "max = 0.5\n",
    "decoding_criterion = 'laser'\n",
    "path = get_code_path()\n",
    "# Format the file path\n",
    "\n",
    "# Open the pandas DataFrame containing the decoding values\n",
    "R = np.load(\n",
    "    (path) / f\"decoding/results/{task}/decoding_{decoding_criterion}_{epoch_on}_{reference}_{sub}.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42da6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad237f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.linspace(min, max, R.shape[0])  # To do better at generalizing\n",
    "fig, ax = plt.subplots(1, figsize=[6, 6])\n",
    "dec = plt.fill_between(times, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"read\"\n",
    "sub = 3\n",
    "epoch_on = 'sentence'\n",
    "reference = \"end\"\n",
    "min = -4.0\n",
    "max = 0.5\n",
    "decoding_criterion = 'laser'\n",
    "plot = plot_subject(sub, decoding_criterion, task, reference, epoch_on, min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3cdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77a76bc3",
   "metadata": {},
   "source": [
    "# Debugging ERP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750be218",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "RUN = 1\n",
    "baseline_min = -1.0\n",
    "baseline_max = 1.0\n",
    "task = \"read\"\n",
    "\n",
    "subjects = subjects[10]\n",
    "epochs2 = epoch_subjects(\n",
    "    subjects, RUN, task, path, baseline_max=baseline_max, baseline_min=baseline_min\n",
    ")\n",
    "\n",
    "epochs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56126bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs2.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044bba4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Homemade imports\n",
    "from dataset import get_path, get_subjects, epoch_subjects, epochs_slice\n",
    "from plot import plot_subject\n",
    "\n",
    "# General imports\n",
    "import numpy as np\n",
    "import mne\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "mne.set_log_level(False)\n",
    "\n",
    "# Later: integrate Hydra here as well. For now, just simple plotting of ERPS\n",
    "\n",
    "report = mne.Report()\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "RUN = 1\n",
    "baseline_min = -1.0\n",
    "baseline_max = 1.0\n",
    "task = \"read\"\n",
    "print(\"\\nSubjects for which the plotting will be done: \\n\")\n",
    "print(subjects)\n",
    "\n",
    "# DEBUG\n",
    "subjects = subjects[4]\n",
    "epochs_ = epoch_subjects(\n",
    "    subjects, RUN, task, path, baseline_max=baseline_max, baseline_min=baseline_min\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build a 3x2 plot, with for each condition (sentence, word, constituent), and for (start, end),\n",
    "# the ERP associated\n",
    "cond = [\"sentence\", \"word\", \"constituent\"]\n",
    "cases = {\"start\", \"end\"}\n",
    "\n",
    "# Plotting and adding to the report, the averaged ERPs of:\n",
    "# words, sentences and constituents, centered at the beginning and end of each\n",
    "\n",
    "\n",
    "# Need to map for:\n",
    "# - end of word,\n",
    "# - beginning of sentence (epochs[i+1] !danger limits)\n",
    "# - beginning of constituent (epochs[i+1] !same danger)\n",
    "\n",
    "evos = []\n",
    "for condi in cond:\n",
    "    for case in cases:\n",
    "        # Slice the epochs based on the epoch_criterion:\n",
    "        column_to_slice_on = f\"{condi}_{case}\"\n",
    "        if condi == \"sentence\" or condi == \"word\":  # eg: {sentence}_{end} or {word}_{start}\n",
    "            epochs = epochs_slice(epochs_, column_to_slice_on)\n",
    "        elif condi == \"constituent\":\n",
    "            epochs = epochs_slice(epochs_, column_to_slice_on, value=2, equal='sup')\n",
    "        evo = epochs.average(method=\"median\")\n",
    "        evos.append(evo)\n",
    "        evo.plot(spatial_colors=True)\n",
    "        report.add_evokeds(evo, titles=f\"Evoked for condition {column_to_slice_on}  \")\n",
    "\n",
    "evokeds = dict(sentence=evos[0], word=evos[2], constituent=evos[4])\n",
    "\n",
    "fig = mne.viz.plot_compare_evokeds(evokeds, combine=\"mean\")\n",
    "\n",
    "report.add_figure(fig, title=\"Evoked response comparaison\")\n",
    "\n",
    "\n",
    "report.save(\n",
    "    f\"./figures/{task}_ERP_all_cond.html\",\n",
    "    open_browser=False,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b61d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0419c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = epochs_slice(epochs_, 'sentence_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03bb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ae1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot import plot_subject\n",
    "\n",
    "from dataset import get_path, get_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "run = 1\n",
    "task = \"read\"\n",
    "subject = '17'\n",
    "baseline_min = -2.0\n",
    "baseline_max = 0.5\n",
    "epoch_on = 'sentence'\n",
    "reference = \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5143d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06188ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataset import epoch_data\n",
    "epo = epoch_data(\n",
    "    subject,\n",
    "    run,\n",
    "    task,\n",
    "    path,\n",
    "    baseline_min,\n",
    "    baseline_max,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54047baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import epochs_slice\n",
    "epos = epochs_slice(epo, 'sentence_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794e34b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evo = epos.average(method=\"median\")\n",
    "evo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a67a6a0",
   "metadata": {},
   "source": [
    "# GFP for sentence - epoching on sentence end and go from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade imports\n",
    "from dataset import get_path, get_subjects, epoch_runs\n",
    "from plot import plot_subject\n",
    "\n",
    "# General imports\n",
    "import numpy as np\n",
    "import mne\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from wordfreq import zipf_frequency\n",
    "from Levenshtein import editops\n",
    "\n",
    "# Tools\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "from utils import match_list, add_syntax\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa49be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "RUN = 2\n",
    "task = \"read\"\n",
    "subject = subjects[1]\n",
    "baseline_min = -4.0\n",
    "baseline_max = 0.5\n",
    "epoch_on = 'sentence'\n",
    "reference = \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1bca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = epoch_runs(\n",
    "            subject,\n",
    "            RUN,\n",
    "            task,\n",
    "            path,\n",
    "            baseline_min,\n",
    "            baseline_max,\n",
    "            epoch_on=epoch_on,\n",
    "            reference=reference,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa8af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run = 1\n",
    "from dataset import epoch_data\n",
    "epo = epoch_data(\n",
    "    subject,\n",
    "    run,\n",
    "    task,\n",
    "    path,\n",
    "    baseline_min=-0.2,\n",
    "    baseline_max=0.8,\n",
    "    filter=True,\n",
    "    epoch_on=\"word\",\n",
    "    reference=\"end\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06639bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = epo.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1596c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_code_path, get_path, mne_events\n",
    "\n",
    "run_id = RUN\n",
    "epoch_on = 'word'\n",
    "reference = \"start\"\n",
    "\n",
    "CHAPTERS = {\n",
    "    1: \"1-3\",\n",
    "    2: \"4-6\",\n",
    "    3: \"7-9\",\n",
    "    4: \"10-12\",\n",
    "    5: \"13-14\",\n",
    "    6: \"15-19\",\n",
    "    7: \"20-22\",\n",
    "    8: \"23-25\",\n",
    "    9: \"26-27\",\n",
    "}\n",
    "\n",
    "\n",
    "bids_path = mne_bids.BIDSPath(\n",
    "        subject=subject,\n",
    "        session=\"01\",\n",
    "        task=task,\n",
    "        datatype=\"meg\",\n",
    "        root=path,\n",
    "        run=RUN,\n",
    "    )\n",
    "\n",
    "raw = mne_bids.read_raw_bids(bids_path)\n",
    "raw.del_proj()  # To fix proj issues\n",
    "raw.pick_types(meg=True, stim=True)\n",
    "raw.load_data()\n",
    "raw = raw.filter(0.5, 20)\n",
    "# Generate event_file path\n",
    "event_file = path / f\"sub-{bids_path.subject}\"\n",
    "event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "event_file = event_file / \"meg\"\n",
    "event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "event_file += f\"_ses-{bids_path.session}\"\n",
    "event_file += f\"_task-{bids_path.task}\"\n",
    "event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "assert Path(event_file).exists()\n",
    "\n",
    "# read events\n",
    "meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "events = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "if (\n",
    "    bids_path.task == \"read\" and bids_path.subject == \"2\"\n",
    "):  # A trigger value bug for this subject\n",
    "    word_length_meg = (\n",
    "        events[:, 2] - 2048\n",
    "    )  # Remove first event: chapter start and remove offset\n",
    "else:\n",
    "    word_length_meg = events[:, 2]\n",
    "# Here, the trigger value encoded the word length\n",
    "# which helps us realign triggers\n",
    "# From the event file / from the MEG events\n",
    "word_len_meta = meta.word.apply(len)\n",
    "i, j = match_list(word_len_meta, word_length_meg)\n",
    "events = events[j]\n",
    "meta = meta.iloc[i].reset_index()\n",
    "print(meta.shape)\n",
    "# The start parameter will help us\n",
    "# keep the link between raw events and metadata\n",
    "meta[\"start\"] = events[:, 0] / raw.info[\"sfreq\"]\n",
    "meta[\"condition\"] = \"sentence\"\n",
    "meta = meta.sort_values(\"start\").reset_index(drop=True)\n",
    "\n",
    "# Raw LPP textual data\n",
    "path_txt = get_code_path() / \"data/txt_raw\"\n",
    "# LPP Syntax data\n",
    "path_syntax = get_code_path() / \"data/syntax\"\n",
    "\n",
    "# Enriching the metadata with outside files:\n",
    "meta = add_syntax(meta, path_syntax, int(run_id))\n",
    "print(meta.shape)\n",
    "# Add the information on the sentence ending:\n",
    "# Only works for reading: TO FIX for listening... to see with Christophe\n",
    "# Also: only works for v2 (subject 1 (me) doesn't work )\n",
    "\n",
    "# Test 1\n",
    "# end_of_sentence = [\n",
    "#     True if meta.onset.iloc[i + 1] - meta.onset.iloc[i] > 0.7 else False\n",
    "#     for i, _ in enumerate(meta.values[:-1])\n",
    "# ]\n",
    "# end_of_sentence.append(True)\n",
    "\n",
    "# Test 2\n",
    "end_of_sentence = [\n",
    "    True\n",
    "    if str(meta.word.iloc[i]).__contains__(\".\")\n",
    "    or str(meta.word.iloc[i]).__contains__(\"?\")\n",
    "    or str(meta.word.iloc[i]).__contains__(\"!\")\n",
    "    else False\n",
    "    for i, _ in enumerate(meta.values[:-1])\n",
    "]\n",
    "end_of_sentence.append(True)\n",
    "meta[\"sentence_end\"] = end_of_sentence\n",
    "\n",
    "# We are considering different cases:\n",
    "# Are we epoching on words, sentences, or constituents?\n",
    "# Different epoching for different analysis\n",
    "if epoch_on == \"word\" and reference == \"start\":\n",
    "    # Default case, so nothing to change\n",
    "    # Could be removed but kept for easy of reading\n",
    "    happy = True\n",
    "# Word end\n",
    "if epoch_on == \"word\" and reference == \"end\":\n",
    "    # Little hack: not really pretty but does the job\n",
    "    # As epoching again uses the start column, we rename it like that\n",
    "    # But it should be meta[\"end\"] instead...\n",
    "    meta[\"start\"] = [row[\"start\"] + row[\"duration\"] for i, row in meta.iterrows()]\n",
    "\n",
    "# Sentence end\n",
    "elif epoch_on == \"sentence\" and reference == \"end\":\n",
    "    # Add a LASER embeddings column for decoding\n",
    "    dim = 1024\n",
    "    embeds = np.fromfile(\n",
    "        f\"{get_code_path()}/data/laser_embeddings/emb_{CHAPTERS[int(run_id)]}.bin\",\n",
    "        dtype=np.float32,\n",
    "        count=-1,\n",
    "    )\n",
    "    embeds.resize(embeds.shape[0] // dim, dim)\n",
    "    column = \"sentence_end\"\n",
    "    value = True\n",
    "    meta = meta[meta[column] == value]\n",
    "    # TODO: create a match list between the embeds sentence and the\n",
    "    print(embeds.shape[0], meta.shape[0])\n",
    "    assert embeds.shape[0] == meta.shape[0]\n",
    "    meta[\"laser\"] = [emb for emb in embeds]\n",
    "    print(\"Added embeddings\")\n",
    "# Sentence start\n",
    "elif epoch_on == \"sentence\" and reference == \"start\":\n",
    "    # Create a sentence-start column:\n",
    "    # list_word_start = [\n",
    "    #     True\n",
    "    #     for i, is_last_word in enumerate(meta.is_last_word[:-1])\n",
    "    #     if meta.is_last_word[i + 1]\n",
    "    # ]\n",
    "    list_word_start = [True]\n",
    "    list_word_start_to_add = [\n",
    "        True if meta.sentence_end[i - 1] else False\n",
    "        for i, _ in enumerate(meta.sentence_end[1:])\n",
    "    ]\n",
    "    for boolean in list_word_start_to_add:\n",
    "        list_word_start.append(boolean)\n",
    "    meta[\"sentence_start\"] = list_word_start\n",
    "    column = \"sentence_start\"\n",
    "    value = True\n",
    "    meta = meta[meta[column] == value]\n",
    "# Constituent start\n",
    "elif epoch_on == \"constituent\" and reference == \"start\":\n",
    "    # Create a constituent-start column:\n",
    "    meta[\"constituent_start\"] = [\n",
    "        True for i, _ in enumerate(meta.is_last_word[1:]) if meta.n_closing > 1\n",
    "    ]\n",
    "    column = \"constituent_start\"\n",
    "    value = True\n",
    "    meta = meta[meta[column] == value]\n",
    "# Constituent end\n",
    "elif epoch_on == \"constituent\" and reference == \"end\":\n",
    "    # Create a constituent-start column:\n",
    "    meta[\"constituent_start\"] = [\n",
    "        True for i, _ in enumerate(meta.is_last_word[1:]) if meta.n_closing > 1\n",
    "    ]\n",
    "    column = \"constituent_start\"\n",
    "    value = True\n",
    "    meta = meta[meta[column] == value]\n",
    "epochs = mne.Epochs(\n",
    "    raw, **mne_events(meta, raw), decim=20, tmin=baseline_min, tmax=baseline_max\n",
    ")\n",
    "# epochs = epochs['kind==\"word\"']\n",
    "# epochs.metadata[\"closing\"] = epochs.metadata.closing_.fillna(0)\n",
    "epochs.load_data()\n",
    "epochs = epochs.pick_types(meg=True, stim=False, misc=False)\n",
    "data = epochs.get_data()\n",
    "\n",
    "# Scaling the data\n",
    "n_words, n_chans, n_times = data.shape\n",
    "vec = data.transpose(0, 2, 1).reshape(-1, n_chans)\n",
    "scaler = RobustScaler()\n",
    "idx = np.arange(len(vec))\n",
    "np.random.shuffle(idx)\n",
    "vec = scaler.fit(vec[idx[:20_000]]).transform(vec)\n",
    "# To try: sigmas = 7 or 15\n",
    "sigma = 7\n",
    "vec = np.clip(vec, -sigma, sigma)\n",
    "epochs._data[:, :, :] = (\n",
    "    scaler.inverse_transform(vec)\n",
    "    .reshape(n_words, n_times, n_chans)\n",
    "    .transpose(0, 2, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad079b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_2 = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "events_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d315880",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbdc744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646dde6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870d8b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a609b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995472f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255968bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785eb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c62475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f38b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a92e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52e158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685a43c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d58005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import decod\n",
    "R_vec = decod(epochs, decoding_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905586b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.fill_between(epochs.times, R_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40765d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f47780",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.diff(epochs.metadata.onset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8eb0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs2 = epoch_runs(subject, RUN, task, path, baseline_min,baseline_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e9edd",
   "metadata": {},
   "source": [
    "x \n",
    "x \n",
    "x \n",
    "x \n",
    "x \n",
    "x \n",
    "x \n",
    "x \n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482306bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.round(np.diff(epochs2.metadata.onset),3)\n",
    "unique, counts = np.unique(arr, return_counts=True)\n",
    "\n",
    "# Print unique values and their counts\n",
    "for val, count in zip(unique, counts):\n",
    "    print(f\"{val} occurs {count} times.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55740958",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.diff(epochs2.metadata.onset)>0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cbf92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddcf368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d44ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f35cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c122719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9a8ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evo = epochs.average(method=\"median\")\n",
    "evo.plot(gfp='only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dae2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epochs_(epochs, column, value):\n",
    "    meta  = epochs.metadata\n",
    "    subset = meta[meta[column]==value].level_0\n",
    "    return epochs[subset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba0ee8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs.metadata['n_closing'][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69967734",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_(epochs,'is_last_word',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0cbc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Build a 3x2 plot, with for each condition (sentence, word, constituent), and for (start, end),\n",
    "# the ERP associated\n",
    "cond = {'sentence': {'column':'is_last_word','target':True},\n",
    "        'word': {'column':'kind','target':'word'},\n",
    "        'constituent': {'column':'n_closing','target':2}}\n",
    "\n",
    "cases = {'start', 'end'}\n",
    "\n",
    "i = 1\n",
    "for condi in cond:\n",
    "    for case in cases:\n",
    "        ep = epochs_(epochs, cond[condi]['column'], cond[condi]['target'])\n",
    "        ax = fig.add_subplot(3, 2, i)\n",
    "        #ep.average().plot(gfp='only')\n",
    "        evo = ep.average(method=\"median\")\n",
    "        evo.plot(spatial_colors=True)\n",
    "        i = i + 1\n",
    "        ax.set_title(f'Plot {cond}')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3217312",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_4 = epochs_(epochs, 'n_closing', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb4772",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_4.average().plot(gfp='only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evo = epochs.average(method=\"median\")\n",
    "evo.plot(spatial_colors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5e02d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a87839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne import Epochs\n",
    "\n",
    "class CustomEpochs(Epochs):\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # Parse the key into metadata field name and value\n",
    "        field, value = key.split('==')\n",
    "        field = field.strip()\n",
    "        value = value.strip()\n",
    "\n",
    "        # Get the indices of the epochs that match the metadata query\n",
    "        indices = [i for i, metadata in enumerate(self.metadata[field]) if metadata == value]\n",
    "\n",
    "        # Return a new Epochs object containing only the matching epochs\n",
    "        return self.__class__(self._data[indices], self.events[indices], self.event_id,\n",
    "                              tmin=self.tmin, tmax=self.tmax, baseline=self.baseline,\n",
    "                              metadata=self.metadata.iloc[indices], info=self.info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_epochs = CustomEpochs(epochs, epochs.events, \"1\", -0.2, 0.8, epochs.baseline, epochs.metadata)\n",
    "\n",
    "# Get all epochs where the 'kind' metadata field is 'word':\n",
    "word_epochs = custom_epochs['kind==word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07344132",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_epochs['kind==\"word\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch\n",
    "def mne_events(meta):\n",
    "    events = np.ones((len(meta), 3), dtype=int)\n",
    "    events[:, 0] = meta.start*raw.info['sfreq']\n",
    "    return dict(events=events, metadata=meta.reset_index())\n",
    "\n",
    "epochs = mne.Epochs(raw, **mne_events(meta), decim=20, tmin=-.2, tmax=1.5, preload=True)\n",
    "epochs = epochs['kind==\"word\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_path, get_subjects, epoch_data, epoch_runs\n",
    "from utils import (\n",
    "    decod,\n",
    "    correlate,\n",
    "    match_list,\n",
    "    create_target,\n",
    "    analysis,\n",
    "    save_decoding_results,\n",
    ")\n",
    "from plot import plot_subject\n",
    "import mne_bids\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import spacy\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from wordfreq import zipf_frequency\n",
    "from Levenshtein import editops\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "RUN = 9\n",
    "task = \"read\"\n",
    "subject = subjects[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856d515",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = epoch_runs(subject, RUN, task, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0915f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(epochs.metadata).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.load_data()\n",
    "epochs = epochs['kind==\"word\"']\n",
    "epochs[\"content_word == False\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7298b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9cc25f465bce2bd58662313d1fe29f78ce66d40d6f2798a767eb1052c037478"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
