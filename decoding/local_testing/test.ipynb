{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1f3dcf",
   "metadata": {},
   "source": [
    "# Journal2Bord\n",
    "\n",
    "- 17.04: Retook on JR's pseudocode, in order to make it work with the dataset, and fix some misalignment issues (matchlist needed when re-epoching, index_resetting needed, and also the use of mne_events saves a lot of time)\n",
    "\n",
    "*Next: run it on Neurospin's workstation on a few more subjects, as it takes too much time on laptop.*\n",
    "\n",
    "- 18.04: Fixed more queries handling and general structure, and it runs on Neurospin Workstation.\n",
    "Pb: For one subject, one run, and decoding one feature (word embeddings), it takes around 1h...\n",
    "Added also the averaged on subject \n",
    "\n",
    "- 19.04: Fixed all the problems discussed during our weekly meeting (decim, meta handling, preload, etc..).\n",
    "\n",
    "Came up with a way to generate the plots for all conditions, added a better epochs window handling.\n",
    "\n",
    "- 20.04: Trying to reduce the RAM usage, by calculating the evos & scores directly, not keeping the epochs in memory.\n",
    "Created script for JeanZay, running jobs successfully now. \n",
    "Plot generated for different sliding windows, multiple subjects, all runs. \n",
    "\n",
    "- 21.04: Tried to adapt the newly parsed files for the syntax, adapted but the problem is that the parser has not been run on the correct LPP translation.. TODO contact the person in charge and rerun the parser\n",
    "\n",
    "Also, adapted script for Jean Zay: sent email for support to understand the memory faults.\n",
    "\n",
    "- 26.04: After iterating on Jean Zay: there is a problem with the way the offset of the different modalities is handled. The way it is currently done, there is a shift: the offset is the offset of the previous word/sent/const, instead of being the end of the one we care about. \n",
    "\n",
    "Next steps include:\n",
    "Ongoing / Done\n",
    "- Pausing the testing on Jean Zay and instead focusing on testing locally with first: only word on/offset and decim * 100.\n",
    "- \n",
    "- Changing the mne_events function so that we can give in the parameter sent/const/word and \n",
    "- Adding the info about the sent_id/sent_stop in the metadata using cumsum / groupby\n",
    "\n",
    "Next:\n",
    "\n",
    "- check it worked for words with the decoding\n",
    "\n",
    "## TODO important\n",
    "\n",
    "- Check if the sentence_stop makes sense: currently done by taking the max of the subgroup (groupby sentence_id) and add the duration of the last word\n",
    "\n",
    "- Ask JR about look of word onset/offset decoding\n",
    "\n",
    "TODO another time:\n",
    "- investigate the events_repeated\n",
    "- train on subset for words, and decode on other modalities ?\n",
    "- If the script runs correctly on Jean Zay, add other decoding modalities, and run them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710b125",
   "metadata": {},
   "source": [
    "# Testing new syntactic parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fc6ce",
   "metadata": {},
   "source": [
    "### Removing regex punct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ed3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(1,10):\n",
    "    \n",
    "    # Read the contents of the text file\n",
    "    with open(f\"/home/co/code/data/syntax_new_untested/run{i}_v2_0.25_0.5-tokenized.syntax.txt\", \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Define the pattern to match substrings like (PONCT 3=,)\n",
    "    pattern = r'\\(PONCT\\s\\d+.*\\)'\n",
    "    # Remove the substrings using re.sub()\n",
    "    clean_text = re.sub(pattern, '', text)\n",
    "\n",
    "    clean_text\n",
    "    # Write the cleaned text back to the file\n",
    "    with open(f\"/home/co/code/data/syntax_new_no_punct/run{i}_v2_0.25_0.5-tokenized.syntax.txt\", \"w\") as file:\n",
    "        file.write(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc766ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(SENT (VN (CLS-SUJ 0=Il) (CLO 1=y) (V 2=avait)) \\n(SENT (Ssub-MOD (CS 0=Lorsque) (Sint (VN (CLS-SUJ 1=je) (V 2=revins)) (PP-DE_OBJ (P 3=de) (NP (DET 4=mon) (NC 5=travail))))) \\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=je) (CLO-OBJ 2=l') (V 3=entendis)) (Srel-MOD (NP-SUJ (PROREL 4=qui)) (VN (V 5=parlait))))) \\n(SENT (VN (CLS-SUJ 0=tu) (ADV 1=ne) (CLR 2=t') (CLO 3=en) (V 4=souviens)) (ADV 5=donc) (ADV 6=pas) \\n(SENT (VN (V 0=Disait) (CLS-SUJ 1=il)) \\n(SENT (VN (CLS-SUJ 0=Ce) (ADV 1=n') (V 2=est)) (ADV+ (ADV 3=pas) (ADV 4=tout) (P 5=à) (NC 6=fait)) (ADV 7=ici!))\\n(SENT (NP-SUJ (DET 0=Une) (ADJ 1=autre) (NC 2=voix)) (VN (CLO-A_OBJ 3=lui) (V 4=répondit)) (ADV+ (P 5=sans) (NC 6=doute)) \\n(Ssub (CS 0=si!))\\n(Ssub (CS 0=Si!))\\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (ADV 2=bien) (NP-ATS (DET 3=le) (NC 4=jour)) \\n(SENT (VN (VN (CLS-SUJ 0=Je) (ADV 1=ne) (V 2=voyais)) (COORD (CC 3=ni) (VN (ADV 4=n') (V 5=entendais)))) (ADV 6=toujours) (NP-OBJ (NC 7=personne)) \\n(SENT (ADV 0=Pourtant) (NP-SUJ (DET 1=le) (ADJ 2=petit) (NC 3=prince)) (VN (V 4=répliqua)) (ADV+ (P 5=de) (NC 6=nouveau)) \\n(SENT \\n(SENT (ADV+ (ADV 0=Bien) (ADJ 1=sûr)) \\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=verras)) (Ssub-OBJ (ADVWH 2=où) (VN (V 3=commence)) (NP-SUJ (DET 4=ma) (NC 5=trace)) (PP-MOD (P 6=dans) (NP (DET 7=le) (NC 8=sable)))) \\n(SENT (VN (CLS-SUJ 0=Tu) (ADV 1=n') (V 2=as)) (ADV 3=qu') (PP-OBJ (P 4=à) (VPinf (VN (CLO 5=m') (CLO 6=y) (VINF 7=attendre)))) \\n(SENT (VN (CLS-SUJ 0=J') (CLO-P_OBJ 1=y) (V 2=serai)) (NP-MOD (DET 3=cette) (NC 4=nuit)) \\n(SENT (VN (CLS-SUJ 0=J') (V 1=étais)) (PP-ATS (P 2=à) (NP (DET 3=vingt) (NC 4=mètres) (PP (P+D 5=du) (NP (NC 6=mur))))) (COORD (CC 7=et) (Sint (VN (CLS-SUJ 8=je) (ADV 9=ne) (V 10=voyais)) (ADV 11=toujours) (NP-OBJ (PRO 12=rien)))) \\n(SENT (NP-SUJ (DET 0=Le) (ADJ 1=petit) (NC 2=prince)) (VN (V 3=dit)) (ADV 4=encore) \\n(SENT (VN (CLS-SUJ 0=tu) (V 1=as)) (NP-OBJ (DET 2=du) (ADJ 3=bon) (NC 4=venin)) \\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=es)) (AP-ATS (ADJ 2=sûr) (PP (P 3=de) (VPinf (VN (ADV+ (ADV 4=ne) (ADV 5=pas)) (CLO-OBJ 6=me) (VINF 7=faire) (VINF 8=souffrir)) (ADV 9=longtemps)))) \\n(SENT (VN (CLS-SUJ 0=Je) (V 1=fis)) (NP-OBJ (NC 2=halte)) \\n(SENT (ADV 0=maintenant) \\n(SENT (VN (CLS-SUJ 0=Je) (V 1=veux)) (VPinf-OBJ (VN (VINF 2=redescendre!))))\\n(SENT (ADV 0=Alors) (VN (CLS-SUJ 1=j') (V 2=abaissai)) (NP-MOD (PRO+ (PRO 3=moi) (ADJ 4=même))) (NP-OBJ (DET 5=les) (NC 6=yeux)) (PP-MOD (P 7=vers) (NP (DET 8=le) (NC 9=pied) (PP (P+D 10=du) (NP (NC 11=mur))))) \\n(SENT (VN (CLS-SUJ 0=Il) (V 1=était)) (ADV 2=là) \\n(SENT (PP-MOD (ADV 0=Tout) (P 1=en) (VPpart (VN (VPR 2=fouillant)) (NP-OBJ (DET 3=ma) (NC 4=poche)) (PP-MOD (P 5=pour) (VPinf (VN (CLO-DE_OBJ 6=en) (VINF 7=tirer)) (NP-OBJ (DET 8=mon) (NC 9=revolver)))))) \\n(SENT (VN (CLS-SUJ 0=Je) (V 1=parvins)) (PP-P_OBJ (P+D 2=au) (NP (NC 3=mur))) (AdP-MOD (ADV 4=juste) (ADV+ (P 5=à) (NC 6=temps)) (P 7=pour) (VPinf (VN (CLO-P_OBJ 8=y) (VINF 9=recevoir)) (PP-MOD (P 10=dans) (NP (DET 11=les) (NC 12=bras))) (NP-OBJ (DET 13=mon) (ADJ 14=petit) (NC 15=bonhomme) (PP (P 16=de) (NP (NC 17=prince))) \\n(SENT (AP-ATS (ADJWH 0=quelle)) (VN (V 1=est)) (NP-SUJ (DET 2=cette) (NC 3=histoire) (ADV 4=là)) \\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=parles)) (ADV 2=maintenant) (PP-MOD (P 3=avec) (NP (DET 4=les) (NC 5=serpents))) \\n(SENT (VN (CLS-SUJ 0=J') (V 1=avais) (VPP 2=défait)) (NP-OBJ (DET 3=son) (ADJ 4=éternel) (NC+ (V 5=cache) (NC 6=nez)) (PP (P 7=d') (NP (NC 8=or)))) \\n(SENT (VN (CLS-SUJ 0=Je) (CLO-A_OBJ 1=lui) (V 2=avais) (VPP 3=mouillé)) (NP-OBJ (DET 4=les) (NC 5=tempes)) (COORD (CC 6=et) (VN (CLO-OBJ 7=l') (V 8=avais) (VPP 9=fait) (VINF 10=boire))) \\n(SENT (COORD (CC 0=Et) (Sint (ADV 1=maintenant) (VN (CLS-SUJ 2=je) (ADV 3=n') (V 4=osais)) (ADV 5=plus) (VPinf-OBJ (NP-OBJ (PRO 6=rien)) (VN (CLO-A_OBJ 7=lui) (VINF 8=demander))))) \\n(SENT (VN (CLS-SUJ 0=Il) (CLO-OBJ 1=me) (V 2=regarda)) (ADV 3=gravement) (COORD (CC 4=et) (VN (CLO-OBJ 5=m') (V 6=entoura)) (NP-OBJ (DET 7=le) (NC 8=cou)) (PP-DE_OBJ (P 9=de) (NP (DET 10=ses) (NC 11=bras)))) \\n(SENT (VN (CLS-SUJ 0=Je) (V 1=sentais)) (VPinf-OBJ (VN (VINF 2=battre)) (NP-OBJ (DET 3=son) (NC 4=coeur)) (PP-MOD (P 5=comme) (NP (PRO 6=celui) (PP (P 7=d') (NP (DET 8=un) (NC 9=oiseau) (Srel (NP-SUJ (PROREL 10=qui)) (VN (V 11=meurt)))))))) \\n(SENT (VN (CLS-SUJ 0=Il) (CLO-A_OBJ 1=me) (V 2=dit)) \\n(SENT (VN (CLS-SUJ 0=je) (V 1=suis)) (AP-ATS (ADJ 2=content) (Ssub (CS 3=que) (Sint (VN (CLS-SUJ 4=tu) (VS 5=aies) (VPP 6=trouvé)) (NP-OBJ (PRO 7=ce) (Srel (NP-SUJ (PROREL 8=qui)) (VN (V 9=manquait)) (PP-A_OBJ (P 10=à) (NP (DET 11=ta) (NC 12=machine)))))))) \\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=vas)) (VPinf-OBJ (VN (VINF 2=pouvoir)) (VPinf-OBJ (VN (VINF 3=rentrer)) (PP-P_OBJ.O (P 4=chez) (PRO 5=toi)))) \\n(SENT (ADVWH 0=comment) (VN (V 1=sais) (CLS-SUJ 2=tu!)))\\n(SENT (VN (CLS-SUJ 0=Je) (V 1=venais)) (ADV 2=justement) (VPinf-OBJ (VN (CLO-A_OBJ 3=lui) (VINF 4=annoncer)) (Ssub-OBJ (CS 5=que) (Sint \\n(SENT (VN (CLS-SUJ 0=Il) (ADV 1=ne) (V 2=répondit)) (NP-OBJ (PRO 3=rien)) (PP-A_OBJ (P 4=à) (NP (DET 5=ma) (NC 6=question))) \\n(SENT (NP-SUJ (PRO 0=moi) (ADV 1=aussi)) \\n(SENT (VN (CLS-SUJ 0=c') (V 1=est)) (AdP-MOD (AdP (ADV 2=bien) (ADV 3=plus)) (ADV 4=loin)) \\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (AP-ATS (AdP (ADV 2=bien) (ADV 3=plus)) (ADJ 4=difficile)) \\n(SENT (VN (CLS-SUJ 0=Je) (CLO-OBJ 1=le) (V 2=serrais)) (PP-MOD (P 3=dans) (NP (DET 4=les) (NC 5=bras))) (PP-MOD (P 6=comme) (NP (DET 7=un) (ADJ 8=petit) (NC 9=enfant))) \\n(SENT (VN (CLS-SUJ 0=Il) (V 1=avait)) (NP-OBJ (NP-OBJ (DET 2=le) (NC 3=regard) (AP (ADJ 4=sérieux))) \\n(SENT (VN (CLS-SUJ 0=j') (V 1=ai)) (NP-OBJ (DET 2=ton) (NC 3=mouton)) \\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=j') (V 2=ai)) (NP-OBJ (DET 3=la) (NC 4=caisse) (PP (P 5=pour) (NP (DET 6=le) (NC 7=mouton)))))) \\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=j') (V 2=ai)) (NP-OBJ (DET 3=la) (NC 4=muselière))) \\n(SENT (VN (CLS-SUJ 0=J') (V 1=attendis)) (ADV 2=longtemps) \\n(SENT (VN (CLS-SUJ 0=Je) (V 1=sentais)) (Ssub-OBJ (CS 2=qu') (Sint (VN (CLS-SUJ 3=il) (CLR 4=se) (V 5=réchauffait)) (ADV+ (ADV 6=peu) (P 7=à) (ADV 8=peu)))) \\n(SENT (NP-MOD (ADJ 0=petit) (NC 1=bonhomme)) \\n(SENT (VN (CLS-SUJ 0=Il) (V 1=avait) (VPP 2=eu)) (NP-OBJ (NC 3=peur)) \\n(SENT (COORD (CC 0=Mais) (Sint (VN (CLS-SUJ 1=il) (V 2=rit)) (ADV 3=doucement))) \\n(SENT (VN (CLS-SUJ 0=j') (V 1=aurai)) (NP-OBJ (AdP (ADV 2=bien) (ADV 3=plus)) (NC 4=peur)) (NP-MOD (DET 5=ce) (NC 6=soir)) \\n(SENT (ADV+ (P 0=De) (NC 1=nouveau)) (VN (CLS-SUJ 2=je) (CLR-OBJ 3=me) (V 4=sentis)) (VPpart-ATS (VPP 5=glacé) (PP-P_OBJ (P 6=par) (NP (DET 7=le) (NC 8=sentiment) (PP (P 9=de) (NP (DET 10=l') (ADJ 11=irréparable)))))) \\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=je) (V 2=compris)) (Ssub-OBJ (CS 3=que) (Sint (VN (CLS-SUJ 4=je) (ADV 5=ne) (V 6=supportais)) (ADV 7=pas) (NP-OBJ (DET 8=l') (NC 9=idée) (PP (P 10=de) (VPinf (ADV+ (ADV 11=ne) (ADV 12=plus) (ADV 13=jamais)) (VN (VINF 14=entendre)) (NP-OBJ (DET 15=ce) (NC 16=rire))))))))) \\n(SENT (VN (CLS-SUJ 0=C') (V 1=était)) (PP-MOD (P 2=pour) (NP (PRO 3=moi))) (PP-ATS (P 4=comme) (NP (DET 5=une) (NC 6=fontaine)) (PP (P 7=dans) (NP (DET 8=le) (NC 9=désert)))) \\n(SENT (NP-MOD (ADJ 0=petit) (NC 1=bonhomme)) \\n(SENT (CC 0=Mais) (VN (CLS-SUJ 1=il) (CLO-A_OBJ 2=me) (V 3=dit)) \\n(SENT (NP-MOD (DET 0=cette) (NC 1=nuit)) \\n(SENT (NP-SUJ (DET 0=Mon) (NC 1=étoile)) (VN (CLR 2=se) (V 3=trouvera)) (PP-P_OBJ (ADV 4=juste) (P+ (P+D 5=au) (ADV 6=dessus) (P 7=de)) (NP (DET 8=l') (NC 9=endroit) (Srel (NP-MOD (PROREL 10=où)) (VN (CLS-SUJ 11=je) (V 12=suis) (VPP 13=tombé)) (NP-MOD (DET 14=l') (NC 15=année) (AP (ADJ 16=dernière)))))) \\n(SENT (NP-MOD (ADJ 0=petit) (NC 1=bonhomme)) \\n(SENT (COORD (CC 0=Mais) (Sint (VN (CLS-SUJ 1=il) (ADV 2=ne) (V 3=répondit)) (ADV 4=pas) (PP-A_OBJ (P 5=à) (NP (DET 6=ma) (NC 7=question))))) \\n(SENT (VN (CLS-SUJ 0=Il) (CLO-A_OBJ 1=me) (V 2=dit)) \\n(SENT (NP-MOD (PRO 0=ce) (Srel (NP-SUJ (PROREL 1=qui)) (VN (V 2=est)) (AP-ATS (ADJ 3=important)))) \\n(SENT (ADV+ (ADV 0=bien) (ADJ 1=sûr)) \\n(SENT (VN (CLS-SUJ 0=c') (V 1=est)) (PP-ATS (CS 2=comme) (PP (P 3=pour) (NP (DET 4=la) (NC 5=fleur)))) \\n(SENT (Ssub-MOD (CS 0=Si) (Sint (VN (CLS-SUJ 1=tu) (V 2=aimes)) (NP-OBJ (DET 3=une) (NC 4=fleur) (Srel (NP-SUJ (PROREL 5=qui)) (VN (CLR 6=se) (V 7=trouve)) (PP-P_OBJ.O (P 8=dans) (NP (DET 9=une) (NC 10=étoile))))))) \\n(SENT (NP-SUJ (ADJ 0=Toutes) (DET 1=les) (NC 2=étoiles)) (VN (V 3=sont)) (AP-ATS (ADJ 4=fleuries)) \\n(SENT (ADV+ (ADV 0=bien) (ADJ 1=sûr)) \\n(SENT (VN (CLS-SUJ 0=c') (V 1=est)) (PP-ATS (CS 2=comme) (PP (P 3=pour) (NP (DET 4=l') (NC 5=eau)))) \\n(SENT (NP-SUJ (PRO 0=Celle) (Srel (NP-OBJ (PROREL 1=que)) (VN (CLS-SUJ 2=tu) (CLO-A_OBJ 3=m') (V 4=as) (VPP 5=donnée)) (PP-A_OBJ (P 6=à) (VPinf (VN (VINF 7=boire)))))) (VN (V 8=était)) (PP-ATS (P 9=comme) (NP (DET 10=une) (NC 11=musique))) \\n(SENT (VN (CLS-SUJ 0=Tu) (CLR 1=te) (V 2=rappelles)) \\n(SENT (VN (CLS-SUJ 0=Elle) (V 1=était)) (AP-ATS (ADJ 2=bonne)) \\n(SENT (ADV+ (ADV 0=bien) (ADJ 1=sûr)) \\n(SENT (VN (CLS-SUJ 0=tu) (V 1=regarderas)) \\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (AP-ATS (ADV 2=trop) (ADJ 3=petit)) (PP-MOD (P 4=chez) (NP (PRO 5=moi))) (Ssub-MOD (CS+ (P 6=pour) (CS 7=que)) (Sint (VN (CLS-SUJ 8=je) (CLO-A_OBJ 9=te) (VS 10=montre)) (Ssub-OBJ (ADVWH-P_OBJ.LOC 11=où) (VN (CLR 12=se) (V 13=trouve)) (NP-SUJ (DET 14=la) (PRO 15=mienne))))) \\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (AP-ATS (ADJ 2=mieux)) (PP-MOD (ADV 3=comme) (NP (PRO 4=ça))) \\n(SENT (NP-MOD (DET 0=Mon) (NC 1=étoile)) \\n(SENT (ADV 0=Alors) \\n(SENT (VN (CLS-SUJ 0=Elles) (V 1=seront)) (NP-ATS (PRO 2=toutes)) (NP-ATS (DET 3=tes) (NC 4=amies)) \\n(SENT (COORD (CC+ (CC 0=Et) (ADV 1=puis)) (Sint (VN (CLS-SUJ 2=je) (V 3=vais)) (VPinf-OBJ (VN (CLO-A_OBJ 4=te) (VINF 5=faire)) (NP-OBJ (DET 6=un) (NC 7=cadeau))))) \\n(SENT (VN (CLS-SUJ 0=Il) (V 1=rit)) (ADV 2=encore) \\n(SENT (I 0=ah) \\n(SENT (NP (ADJ 0=Petit) (NC 1=bonhomme)) \\n(SENT (ADV 0=justement) (VN (CLS-SUJ 1=ce) (V 2=sera)) (NP-ATS (DET 3=mon) (NC 4=cadeau)) \\n(SENT (VN (CLS-SUJ 0=Ce) (V 1=sera)) (PP-ATS (CS 2=comme) (PP (P 3=pour) (NP (DET 4=l') (NC 5=eau)))) \\n(SENT (VPinf-OBJ (NP-OBJ (PROWH 0=que)) (VN (VINF 3=dire))) (VN (V 1=veux) (CLS-SUJ 2=tu)) \\n(SENT (NP-SUJ (DET 0=les) (NC 1=gens)) (VN (V 2=ont)) (NP-OBJ (DET 3=des) (NC 4=étoiles) (Srel (NP-SUJ (PROREL 5=qui)) (VN (ADV 6=ne) (V 7=sont)) (ADV 8=pas) (NP-ATS (DET 9=les) (ADJ 10=mêmes)))) \\n(SENT (PP-MOD (P 0=Pour) (NP (DET 1=les) (PRO 2=uns) \\n(SENT (PP-MOD (P 0=Pour) (NP (DET 1=d') (PRO 2=autres))) (VN (CLS-SUJ 3=elles) (ADV 4=ne) (V 5=sont)) (NP-ATS (PRO 6=rien)) (ADV 7=que) (NP-ATS (DET 8=de) (ADJ 9=petites) (NC 10=lumières)) \\n(SENT (PP-MOD (P 0=Pour) (NP (DET 1=d') (PRO 2=autres) \\n(SENT (PP-MOD (P 0=Pour) (NP (DET 1=mon) (NC 2=businessman))) (VN (CLS-SUJ 3=elles) (V 4=étaient)) (NP-ATS (DET+ (P 5=de) (DET 6=l')) (NC 7=or)) \\n(SENT (COORD (CC 0=Mais) (Sint (NP-SUJ (ADJ 1=toutes) (DET 2=ces) (NC 3=étoiles) (ADV 4=là)) (VN (CLR 5=se) (V 6=taisent)))) \\n(SENT (NP-MOD (PRO 0=Toi)) \\n(SENT (VPinf-OBJ (NP-OBJ (PROWH 0=que)) (VN (VINF 3=dire))) (VN (V 1=veux) (CLS-SUJ 2=tu)) \\n(SENT (CS 0=quand) (Sint (VN (CLS-SUJ 1=tu) (V 2=regarderas)) (NP-OBJ (DET 3=le) (NC 4=ciel)) \\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=auras)) \\n(SENT (COORD (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=il) (V 2=rit)) (ADV 3=encore))) \\n(SENT (COORD (CC 0=et) (Sint (Ssub-MOD (Ssub-MOD (CS 1=quand) (Sint (VN (CLS-SUJ 2=tu) (V 3=seras) (VPP 4=consolé)))) \\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=seras)) (ADV 2=toujours) (NP-ATS (DET 3=mon) (NC 4=ami)) \\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=auras)) (NP-OBJ (NC 2=envie)) (PP-DE_OBJ (P 3=de) (VPinf (VN (VINF 4=rire)) (PP-MOD (P 5=avec) (NP (PRO 6=moi))))) \\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=tu) (V 2=ouvriras)) (ADV 3=parfois) (NP-OBJ (DET 4=ta) (NC 5=fenêtre)) \\n(SENT (COORD (CC 0=Et) (Sint (NP-SUJ (DET 1=tes) (NC 2=amis)) (VN (V 3=seront) (ADV 4=bien) (VPP 5=étonnés)) (PP-DE_OBJ (P 6=de) (VPinf (VN (CLO-OBJ 7=te) (VINF 8=voir)) (VPinf-OBJ (VN (VINF 9=rire)) (PP-MOD (P 10=en) (VPpart (VN (VPR 11=regardant)) (NP-OBJ (DET 12=le) (NC 13=ciel))))))))) \\n(SENT (ADV 0=Alors) (VN (CLS-SUJ 1=tu) (CLO-A_OBJ 2=leur) (V 3=diras)) \\n(SENT (ADV 0=oui) \\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=ils) (CLO-OBJ 2=te) (V 3=croiront)) (AP-ATO (ADJ 4=fou)))) \\n(SENT (VN (CLS-SUJ 0=Je) (CLO-A_OBJ 1=t') (V 2=aurai) (VPP 3=joué)) (NP-OBJ (DET 4=un) (AP (ADV 5=bien) (ADJ 6=vilain)) (NC 7=tour)) \\n(SENT (VN (CLS-SUJ 0=ce) (V 1=sera)) (Ssub-ATS (CS+ (ADV 2=comme) (CS 3=si)) (Sint (VN (CLS-SUJ 4=je) (CLO-A_OBJ 5=t') (V 6=avais) (VPP 7=donné)) \\n(SENT (COORD (CC 0=Puis) (Sint (VN (CLS-SUJ 1=il) (V 2=redevint)) (AP-ATS (ADJ 3=sérieux)))) \\n(SENT (NP (DET 0=cette) (NC 1=nuit)) \\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=sais)) \\n(SENT (VN (ADV 0=Ne) (V 1=viens)) (ADV 2=pas) \\n(SENT (VN (CLS-SUJ 0=je) (ADV 1=ne) (CLO-OBJ 2=te) (V 3=quitterai)) (ADV 4=pas) \\n(SENT (VN (CLS-SUJ 0=j') (V 1=aurai)) (NP-OBJ (DET 2=l') (NC 3=air)) (PP-DE_OBJ (P 4=d') (VPinf (VN (VINF 5=avoir)) (ADV 6=mal))) \\n(SENT (VN (CLS-SUJ 0=J') (V 1=aurai)) (ADV+ (DET 2=un) (ADV 3=peu)) (NP-OBJ (DET 4=l') (NC 5=air)) (PP-DE_OBJ (P 6=de) (VPinf (VN (VINF 7=mourir)))) \\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (PP-ATS (P 2=comme) (NP (PRO 3=ça))) \\n(SENT (VN (ADV 0=Ne) (V 1=viens)) (ADV 2=pas) (VPinf-OBJ (VN (VINF 3=voir)) (NP-OBJ (PRO 4=ça))) (Sint-MOD \\n(SENT (VN (CLS-SUJ 0=je) (ADV 1=ne) (CLO-OBJ 2=te) (V 3=quitterai)) (ADV 4=pas) \\n(SENT (COORD (CC 0=Mais) (Sint (VN (CLS-SUJ 1=il) (V 2=était)) (AP-ATS (ADJ 3=soucieux)))) \\n(SENT (VN (CLS-SUJ 0=je) (CLO-A_OBJ 1=te) (V 2=dis)) (NP-OBJ (PRO 3=ça)) \\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (PP-ATS (P+ (P 2=à) (NC 3=cause)) (ADV 4=aussi) (PP (P+D+ (P+D 5=du)) (NP (NC 6=serpent)))) \\n(SENT (VN (CLS-SUJ 0=Il) (ADV 1=ne) (V 2=faut)) (ADV 3=pas) (Ssub-OBJ (CS 4=qu') (Sint (VN (CLS-SUJ 5=il) (CLO-OBJ 6=te) (VS 7=morde)))) \\n(SENT (NP (DET 0=Les) (NC 1=serpents)) \\n(SENT (NP-SUJ (PRO 0=Ça)) (VN (V 1=peut)) (VPinf-OBJ (VN (VINF 2=mordre)) (PP-MOD (P 3=pour) (NP (DET 4=le) (NC 5=plaisir)))) \\n(SENT (VN (CLS-SUJ 0=je) (ADV 1=ne) (CLO-OBJ 2=te) (V 3=quitterai)) (ADV 4=pas) \\n(SENT (COORD (CC 0=Mais) (Sint (NP-SUJ (PRO+ (DET 1=quelque) (NC 2=chose))) (VN (CLO-OBJ 3=le) (V 4=rassura)))) \\n(SENT (VN (CLS-SUJ 0=c') (V 1=est)) (AP-ATS (ADJ 2=vrai)) (Ssub-OBJ (CS 3=qu') (Sint (VN (CLS-SUJ 4=ils) (ADV 5=n') (V 6=ont)) (ADV 7=plus) (NP-OBJ (DET 8=de) (NC 9=venin)) (PP-MOD (P 10=pour) (NP (DET 11=la) (ADJ 12=seconde) (NC 13=morsure))))) \\n(SENT (NP-MOD (DET 0=Cette) (NC 1=nuit) (ADV 2=là)) (VN (CLS-SUJ 3=je) (ADV 4=ne) (CLO-OBJ 5=le) (V 6=vis)) (ADV 7=pas) (VPinf-OBJ (VN (CLR 8=se) (VINF 9=mettre)) (PP-P_OBJ (P 10=en) (NP (NC 11=route)))) \\n(SENT (VN (CLS-SUJ 0=Il) (CLR 1=s') (V 2=était) (VPP 3=évadé)) (PP-MOD (P 4=sans) (NP (NC 5=bruit))) \\n(SENT (Ssub-MOD (CS 0=Quand) (Sint (VN (CLS-SUJ 1=je) (V 2=réussis)) (PP-A_OBJ (P 3=à) (VPinf (VN (CLO-OBJ 4=le) (VINF 5=rejoindre)))))) (VN (VN (CLS-SUJ 6=il) (V 7=marchait)) (AP-ATO (ADJ 8=décidé))) \\n(SENT (VN (CLS-SUJ 0=Il) (CLO-A_OBJ 1=me) (V 2=dit)) (ADV 3=seulement) \\n(SENT (I 0=ah) \\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=es)) (ADV 2=là) \\n(SENT (COORD (CC 0=Mais) (VN (CLS-SUJ 1=il) (CLR 2=se) (V 3=tourmenta)) (ADV 4=encore)) \\n(SENT (VN (CLS-SUJ 0=tu) (V 1=as) (VPP 2=eu)) (NP-OBJ (NC 3=tort)) \\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=auras)) (NP-OBJ (DET+ (DET 2=de) (DET 3=la)) (NC 4=peine)) \\n(SENT (VN (CLS-SUJ 0=J') (V 1=aurai)) (NP-OBJ (DET 2=l') (NC 3=air)) (PP-DE_OBJ (P 4=d') (VPinf (VN (VINF 5=être)) (AP-ATS (ADJ 6=mort)))) (COORD (CC 7=et) (Sint (VN (CLS-SUJ 8=ce) (ADV 9=ne) (V 10=sera)) (ADV 11=pas) (AP-ATS (ADJ 12=vrai)))) \\n(SENT (VN (CLS-SUJ 0=tu) (V 1=comprends)) \\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (AdP-MOD (ADV 2=trop) (ADV 3=loin)) \\n(SENT (VN (CLS-SUJ 0=Je) (ADV 1=ne) (V 2=peux)) (ADV 3=pas) (VPinf-OBJ (VN (VINF 4=emporter)) (NP-OBJ (DET 5=ce) (NC 6=corps) (ADV 7=là))) \\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (AP-ATS (ADV 2=trop) (ADJ 3=lourd)) \\n(SENT (PRO 0=Moi) (VN (CLS-SUJ 1=je) (CLR 2=me) (V 3=taisais)) \\n(SENT (COORD (CC 0=mais) (Sint (VN (CLS-SUJ 1=ce) (V 2=sera)) (PP-ATS (P 3=comme) (NP (DET 4=une) (ADJ 5=vieille) (NC 6=écorce) (VPpart (VPP 7=abandonnée)))))) \\n(SENT (VN (CLS-SUJ 0=Ce) (ADV 1=n') (V 2=est)) (ADV 3=pas) (AP-ATS (ADJ 4=triste)) (NP-MOD (DET 5=les) (ADJ 6=vieilles) (NC 7=écorces)) \\n(SENT (VN (CLS-SUJ 0=Il) (CLR-OBJ 1=se) (V 2=découragea)) (ADV+ (DET 3=un) (ADV 4=peu)) \\n(SENT (COORD (CC 0=Mais) (Sint (VN (CLS-SUJ 1=il) (V 2=fit)) (ADV 3=encore) (NP-OBJ (DET 4=un) (NC 5=effort)))) \\n(SENT (VN (CLS-SUJ 0=ce) (V 1=sera)) (AP-ATS (ADJ 2=gentil)) \\n(SENT (PRO 0=Moi) (ADV 1=aussi) \\n(SENT (NP-SUJ (ADJ 0=Toutes) (DET 1=les) (NC 2=étoiles)) (VN (V 3=seront)) (NP-ATS (DET 4=des) (NC 5=puits) (PP (P 6=avec) (NP (DET 7=une) (NC 8=poulie) (VPpart (VPP 9=rouillée))))) \\n(SENT (NP-SUJ (ADJ 0=Toutes) (DET 1=les) (NC 2=étoiles)) (VN (CLO 3=me) (V 4=verseront)) (PP-A_OBJ (P 5=à) (VPinf (VN (VINF 6=boire)))) \\n(SENT (VN (CLS-SUJ 0=ce) (V 1=sera)) (AP-ATS (ADV 2=tellement) (ADJ 3=amusant)) \\n(SENT (VN (CLS-SUJ 0=Tu) (V 1=auras)) (NP-OBJ (DET+ (DET 2=cinq) (DET 3=cents)) (NC 4=millions) (PP (P 5=de) (NP (NC 6=grelots)))) \\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=il) (CLR 2=se) (V 3=tut)) (ADV 4=aussi) \\n(SENT (VN (CLS-SUJ 0=c') (V 1=est)) (ADV 2=là) \\n(SENT (VN (VIMP 0=Laisse) (CLO-A_OBJ 1=moi)) (VPinf-OBJ (VN (VINF 2=faire)) (NP-OBJ (DET 3=un) (NC 4=pas) (AP (ADV 5=tout) (ADJ 6=seul)))) \\n(SENT (COORD (CC 0=Et) (VN (CLS-SUJ 1=il) (CLR 2=s') (V 3=assit)) (Ssub-MOD (CS+ (CS 4=parce) (CS 5=qu')) (Sint (VN (CLS-SUJ 6=il) (V 7=avait)) (NP-OBJ (NC 8=peur))))) \\n(SENT (VN (CLS-SUJ 0=Il) (V 1=dit)) (ADV 2=encore) \\n(SENT (VN (CLS-SUJ 0=tu) (V 1=sais)) \\n(SENT (NP (DET 0=Ma) (NC 1=fleur)) \\n(SENT (VN (CLS-SUJ 0=J') (V 2=suis)) (AP-ATS (CLO-DEP 1=en) (ADJ 3=responsable)) \\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=elle) (V 2=est)) (AP-ATS (ADV 3=tellement) (ADJ 4=faible)))) \\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=elle) (V 2=est)) (AP-ATS (ADV 3=tellement) (ADJ 4=naïve)))) \\n(SENT (VN (CLS-SUJ 0=Elle) (V 1=a)) (NP-OBJ (DET 2=quatre) (NC 3=épines) (PP (P 4=de) (NP (PRO+ (PRO 5=rien) (P+D 6=du) (PRO 7=tout))))) (PP-MOD (P 8=pour) (VPinf (VN (CLO-OBJ 9=la) (VINF 10=protéger)) (PP-MOD (P 11=contre) (NP (DET 12=le) (NC 13=monde))))) \\n(SENT (NP-MOD (PRO 0=Moi)) (VN (CLS-SUJ 1=je) (CLR 2=m') (V 3=assis)) (Ssub-MOD (CS+ (CS 4=parce) (CS 5=que)) (Sint (VN (CLS-SUJ 6=je) (ADV 7=ne) (V 8=pouvais)) (ADV 9=plus) (VPinf-OBJ (VN (CLO-OBJ 10=me) (VINF 11=tenir)) (AP-ATO (ADJ 12=debout))))) \\n(SENT (VN (CLS-SUJ 0=Il) (V 1=dit)) \\n(SENT (VN (V 0=voilà)) \\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (NP-ATS (PRO 2=tout)) \\n(SENT (VN (CLS-SUJ 0=Il) (V 1=hésita)) (ADV 2=encore) (ADV+ (DET 3=un) (ADV 4=peu)) \\n(SENT (VN (CLS-SUJ 0=Il) (V 1=fit)) (NP-OBJ (DET 2=un) (NC 3=pas)) \\n(SENT (PRO 0=Moi) (VN (CLS-SUJ 1=je) (ADV 2=ne) (V 3=pouvais)) (ADV 4=pas) (VPinf-OBJ (VN (VINF 5=bouger))) \\n(SENT (VN (CLS-SUJ 0=Il) (ADV 1=n') (CLO 2=y) (V 3=eut)) (NP-OBJ (PRO 4=rien)) (ADV 5=qu') (NP-OBJ (DET 6=un) (NC 7=éclair) (AP (ADJ 8=jaune))) (PP-MOD (P+ (P 9=près) (P 10=de)) (NP (DET 11=sa) (NC 12=cheville))) \\n(SENT (VN (CLS-SUJ 0=Il) (V 1=demeura)) (NP-MOD (DET 2=un) (NC 3=instant)) (AP-ATS (ADJ 4=immobile)) \\n(SENT (VN (CLS-SUJ 0=Il) (ADV 1=ne) (V 2=cria)) (ADV 3=pas) \\n(SENT (VN (CLS-SUJ 0=Il) (V 1=tomba)) (ADV 2=doucement) (Ssub-MOD (CS 3=comme) (Sint (VN (V 4=tombe)) (NP-SUJ (DET 5=un) (NC 6=arbre)))) \\n(SENT (NP-SUJ (PRO 0=Ça)) (VN (ADV 1=ne) (V 2=fit)) (AdP-MOD (ADV 3=même) (ADV 4=pas)) (NP-OBJ (DET 5=de) (NC 6=bruit)) \\n(SENT (COORD (CC 0=Et) (Sint (ADV 1=maintenant) (ADV+ (ADV 2=bien) (ADJ 3=sûr)) \\n(SENT (VN (CLS-SUJ 0=Je) (ADV 1=n') (V 2=ai) (AdP (ADV 3=jamais) (ADV 4=encore)) (VPP 5=raconté)) (NP-OBJ (DET 6=cette) (NC 7=histoire)) \\n(SENT (NP-SUJ (DET 0=Les) (NC 1=camarades) (Srel (NP-SUJ (PROREL 2=qui)) (VN (CLO-OBJ 3=m') (V 4=ont) (VPP 5=revu)))) (VN (V 6=ont) (VPP 7=été)) (AP-ATS (ADV 8=bien) (ADJ 9=contents) (PP (P 10=de) (VPinf (VN (CLO-OBJ 11=me) (VINF 12=revoir)) (AP-ATO (ADJ 13=vivant))))) \\n(SENT (VN (CLS-SUJ 0=J') (V 1=étais)) (AP-ATS (ADJ 2=triste)) (COORD (CC 3=mais) (VN (CLS-SUJ 4=je) (CLO-A_OBJ 5=leur) (V 6=disais))) \\n(SENT (VN (CLS-SUJ 0=c') (V 1=est)) (NP-ATS (DET 2=la) (NC 3=fatigue)) \\n(SENT (CC+ (CLS 0=C') (V 1=est) (P 2=à) (VINF 3=dire)) \\n(SENT (ADV+ (ADV 0=Pas) (ADV+ (ADV 1=tout) (P 2=à) (NC 3=fait))) \\n(SENT (COORD (CC 0=Mais) (Sint (VN (CLS-SUJ 1=je) (V 2=sais)) (ADV 3=bien) (Ssub-OBJ (CS 4=qu') (Sint (VN (CLS-SUJ 5=il) (V 6=est) (VPP 7=revenu)) (PP-A_OBJ (P 8=à) (NP (DET 9=sa) (NC 10=planète))))) \\n(SENT (VN (CLS-SUJ 0=Ce) (ADV 1=n') (V 2=était)) (ADV 3=pas) (NP-ATS (DET 4=un) (NC 5=corps) (AP (ADV 6=tellement) (ADJ 7=lourd))) \\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=j') (V 2=aime)) (NP-MOD (DET 3=la) (NC 4=nuit)) (VPinf-OBJ (VN (VINF 5=écouter)) (NP-OBJ (DET 6=les) (NC 7=étoiles))))) \\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (PP-ATS (P 2=comme) (NP (DET+ (DET 3=cinq) (DET 4=cents)) (NC 5=millions) (PP (P 6=de) (NP (NC 7=grelots))))) \\n(SENT (COORD (CC 0=Mais) (Sint (VN (V 1=voilà)) (Ssub-OBJ (CS 2=qu') (Sint (VN (CLS-SUJ 3=il) (CLR 4=se) (V 5=passe)) (NP-OBJ (PRO+ (DET 6=quelque) (NC 7=chose)) (PP (P 8=d') (AP (ADJ 9=extraordinaire)))))))) \\n(SENT (NP-MOD (DET 0=La) (NC 1=muselière) (Srel (NP-OBJ (PROREL 2=que)) (VN (CLS-SUJ 3=j') (V 4=ai) (VPP 5=dessinée)) (PP-MOD (P 6=pour) (NP (DET 7=le) (ADJ 8=petit) (NC 9=prince))))) \\n(SENT (VN (CLS-SUJ 0=Il) (ADV 1=n') (V 2=aura) (ADV 3=jamais) (VPP 4=pu)) (VPinf-OBJ (VN (CLO-OBJ 5=l') (VINF 6=attacher)) (PP-A_OBJ (P+D 7=au) (NP (NC 8=mouton)))) \\n(SENT (ADV 0=Alors) (VN (CLS-SUJ 1=je) (CLR 2=me) (V 3=demande)) \\n(SENT (NP-OBJ (PROWH 0=que)) (VN (CLR 1=s') (V 2=est) (CLS-SUJ 3=il) (VPP 4=passé)) (PP-MOD.LOC (P 5=sur) (NP (DET 6=sa) (NC 7=planète))) \\n(SENT (ADV+ (V 0=Peut) (VINF 1=être)) (Ssub (CS+ (ADV 2=bien) (CS 3=que)) (Sint (NP-SUJ (DET 4=le) (NC 5=mouton)) (VN (V 6=a) (VPP 7=mangé)) (NP-OBJ (DET 8=la) (NC 9=fleur)))) \\n(SENT (ADV 0=Tantôt) (VN (CLS-SUJ 1=je) (CLR-A_OBJ 2=me) (V 3=dis)) \\n(SENT (ADV 0=sûrement) (ADV 1=non!))\\n(SENT (NP-SUJ (DET 0=Le) (ADJ 1=petit) (NC 2=prince)) (VN (V 3=enferme)) (NP-OBJ (DET 4=sa) (NC 5=fleur)) (NP-MOD (ADJ 6=toutes) (DET 7=les) (NC 8=nuits)) (PP-P_OBJ.O (P 9=sous) (NP (DET 10=son) (NC 11=globe) (PP (P 12=de) (NP (NC 13=verre))))) \\n(SENT (COORD (CC 0=Et) (Sint (NP-SUJ (ADJ 1=toutes) (DET 2=les) (NC 3=étoiles)) (VN (V 4=rient)) (ADV 5=doucement))) \\n(SENT (ADV 0=Tantôt) (VN (CLS-SUJ 1=je) (CLR-A_OBJ 2=me) (V 3=dis)) \\n(SENT (VN (CLS-SUJ 0=on) (V 1=est) (VPP 2=distrait)) (NP-MOD (DET 3=une) (NC 4=fois) (COORD (CC 5=ou) (NP (DET 6=l') (PRO 7=autre)))) \\n(SENT (VN (CLS-SUJ 0=Il) (V 1=a) (VPP 2=oublié)) \\n(SENT (ADV 0=Alors) (NP-SUJ (DET 1=les) (NC 2=grelots)) (VN (CLR 3=se) (V 4=changent)) (NP-MOD (PRO 5=tous)) (PP-P_OBJ.O (P 6=en) (NP (NC 7=larmes))) \\n(SENT (VN (CLS-SUJ 0=C') (V 1=est)) (ADV 2=là) (NP-ATS (DET 3=un) (AP (ADV 4=bien) (ADJ 5=grand)) (NC 6=mystère)) \\n(SENT (PP-MOD (P 0=Pour) (NP (PRO 1=vous) (Srel (NP-SUJ (PROREL 2=qui)) (VN (V 3=aimez)) (ADV 4=aussi) (NP-OBJ (DET 5=le) (ADJ 6=petit) (NC 7=prince))))) \\n(SENT (VN (VIMP 0=Regardez)) (NP-OBJ (DET 1=le) (NC 2=ciel)) \\n(SENT (VN (VIMP 0=Demandez) (CLR 1=vous)) \\n(SENT (NP-SUJ (DET 0=le) (NC 1=mouton) (AdP-MOD (ADV 2=oui) (COORD (CC 3=ou) (ADV 4=non)))) (VN (V 5=a) (CLS-SUJ 6=t) (CLS-SUJ 7=il) (VPP 8=mangé)) (NP-OBJ (DET 9=la) (NC 10=fleur)) \\n(SENT (COORD (CC 0=Et) (Sint (VN (CLS-SUJ 1=vous) (V 2=verrez)) (Ssub-OBJ (CS 3=comme) (Sint (NP-SUJ (PRO 4=tout)) (VN (V 5=change)))))) \\n(SENT (COORD (CC 0=Et) (Sint (NP-SUJ (DET 1=aucune) (ADJ 2=grande) (NC 3=personne)) (VN (ADV 4=ne) (V 5=comprendra)) (ADV 6=jamais) (Ssub-OBJ (CS 7=que) (Sint (NP-SUJ (PRO 8=ça)) (VN (V 9=a)) (NP-OBJ (ADV 10=tellement) (P 11=d') (NC 12=importance)))))) \\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4454fd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoching for run 1, subject: 5\n",
      "\n",
      "Opening raw data file /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-01_meg.fif...\n",
      "    Read a total of 13 projection items:\n",
      "        grad_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v6 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v7 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v8 (1 x 306)  idle\n",
      "    Range : 89000 ... 554999 =     89.000 ...   554.999 secs\n",
      "Ready.\n",
      "Reading events from /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-01_events.tsv.\n",
      "Reading channel info from /home/co/data/LPP_MEG_visual/sub-5/ses-01/meg/sub-5_ses-01_task-read_run-01_channels.tsv.\n",
      "Using 4 HPI coils: 293 307 314 321 Hz\n",
      "Not fully anonymizing info - keeping his_id, sex, and hand info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: This file contains raw Internal Active Shielding data. It may be distorted. Elekta recommends it be run through MaxFilter to produce reliable results. Consider closing the file and running MaxFilter on the data.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: Omitted 128 annotation(s) that were outside data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/home/co/workspace_LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: The unit for channel(s) STI001, STI002, STI003, STI004, STI005, STI006, STI007, STI008, STI009, STI010, STI011, STI012, STI013, STI014, STI015, STI016, STI101, STI201, STI301 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1468 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "Reading 0 ... 465999  =      0.000 ...   465.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 20 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 20.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 306 out of 306 | elapsed:   25.0s finished\n"
     ]
    }
   ],
   "source": [
    "def mne_events(meta, raw, start, level):\n",
    "    if start=='onset':\n",
    "        events = np.ones((len(meta), 3), dtype=int)\n",
    "        events[:, 0] = meta.start * raw.info[\"sfreq\"]\n",
    "        return dict(events=events, metadata=meta.reset_index())\n",
    "    elif start=='offset':\n",
    "        events = np.ones((len(meta), 3), dtype=int)\n",
    "        events[:, 0] = (meta.start+meta.duration) * raw.info[\"sfreq\"]\n",
    "        return dict(events=events, metadata=meta.reset_index())\n",
    "        \n",
    "    else:\n",
    "        print('start should be either onset or offset')\n",
    "        return 0\n",
    "from dataset import read_raw, get_subjects, get_path\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "all_evos = []\n",
    "all_scores = []\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 1\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "levels = ('word')\n",
    "starts = ('onset')\n",
    "            \n",
    "for subject in subjects[2:3]:\n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    # Dict init\n",
    "    for start in starts: \n",
    "            for level in levels:\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "        meta['word_stop'] = meta.start + meta.duration\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sentence stop info\n",
    "        meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "        for s, d in meta.groupby('sentence_id'):\n",
    "            meta.loc[d.index, 'sent_word_id'] = range(len(d))\n",
    "            meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'sentence_stop'] = d.start.max() # TO Verify!\n",
    "            \n",
    "        # Adding the constituents stop info\n",
    "        meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "        for s, d in meta.groupby('constituent_id'):\n",
    "            meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'constituent_stop'] = d.start.max() # TO Verify!\n",
    "            meta.loc[d.index, 'const_word_id'] = range(len(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5795b81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>constituent_id</th>\n",
       "      <th>n_closing</th>\n",
       "      <th>const_word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lorsque</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>javais</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>six</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ans</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jai</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vu</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>une</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fois</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>une</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>magnifique</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>image</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dans</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>un</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>livre</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sur</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>la</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>forêt</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>vierge</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>qui</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sappelait</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>histoires</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>vécues</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ça</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>représentait</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>un</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>serpent</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>boa</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>qui</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>avalait</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>un</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>fauve</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Voilà</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>la</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>copie</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>du</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>dessin</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>On</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>disait</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>dans</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>le</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>livre</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>les</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>serpents</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>boas</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>avalent</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>leur</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>proie</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tout</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>entière</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>sans</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  constituent_id  n_closing  const_word_id\n",
       "0        Lorsque               0          1            0.0\n",
       "1         javais               0          1            1.0\n",
       "2            six               0          2            2.0\n",
       "3            ans               1          1            0.0\n",
       "4            jai               1          1            1.0\n",
       "5             vu               1          1            2.0\n",
       "6            une               1          1            3.0\n",
       "7           fois               1          1            4.0\n",
       "8            une               1          2            5.0\n",
       "9     magnifique               2          1            0.0\n",
       "10         image               2          1            1.0\n",
       "11          dans               2          2            2.0\n",
       "12            un               3          1            0.0\n",
       "13         livre               3          1            1.0\n",
       "14           sur               3          1            2.0\n",
       "15            la               3          2            3.0\n",
       "16         forêt               4          1            0.0\n",
       "17        vierge               4          1            1.0\n",
       "18           qui               4          1            2.0\n",
       "19     sappelait               4          1            3.0\n",
       "20     histoires               4          1            4.0\n",
       "21        vécues               4          1            5.0\n",
       "22            Ça               4          1            6.0\n",
       "23  représentait               4          2            7.0\n",
       "24            un               5          1            0.0\n",
       "25       serpent               5          1            1.0\n",
       "26           boa               5          2            2.0\n",
       "27           qui               6          1            0.0\n",
       "28       avalait               6          6            1.0\n",
       "29            un               6          2            2.0\n",
       "30         fauve               6          2            3.0\n",
       "31         Voilà               7          1            0.0\n",
       "32            la               7          1            1.0\n",
       "33         copie               7          1            2.0\n",
       "34            du               7          1            3.0\n",
       "35        dessin               7          2            4.0\n",
       "36            On               8          1            0.0\n",
       "37        disait               8          1            1.0\n",
       "38          dans               8          4            2.0\n",
       "39            le               8          2            3.0\n",
       "40         livre               8          2            4.0\n",
       "41           les               9          1            0.0\n",
       "42      serpents               9          1            1.0\n",
       "43          boas               9          1            2.0\n",
       "44       avalent               9          4            3.0\n",
       "45          leur               9          2            4.0\n",
       "46         proie              10          1            0.0\n",
       "47          tout              10          2            1.0\n",
       "48       entière              11          1            0.0\n",
       "49          sans              11          1            1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[['word', 'constituent_id','n_closing','const_word_id']][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9799f565",
   "metadata": {},
   "source": [
    "# Initial Plotting for ERPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572b284",
   "metadata": {},
   "source": [
    "### 0.5\n",
    "\n",
    "Just the evoked, for all conditions, all subjects:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb924f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It is currently being done by keeping all the epochs in memory: might want to do like in the #1, and generate the evo\n",
    "# or score from the epochs (for a subject), and from there try\n",
    "# To find a way to, starting with an array of evoked, average them!!!\n",
    "from dataset import read_raw, get_subjects, get_path, mne_events\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "all_evos = []\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 9\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "# Dict init\n",
    "for start in ('onset', 'offset'): \n",
    "        for level in ('word', 'constituent', 'sentence'):\n",
    "            epoch_key = f'{level}_{start}'\n",
    "            dict_epochs[epoch_key] = [] \n",
    "            \n",
    "for subject in subjects[2:5]:\n",
    "    all_epochs = []\n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "\n",
    "        # Word end\n",
    "        meta['word_offset'] = True\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Sent stop\n",
    "        meta['next_word_id'] = meta['word_id'].shift(-1)\n",
    "        meta['sentence_offset'] = meta.apply(lambda x: True if x['word_id'] > x['next_word_id'] else False, axis=1)\n",
    "        meta['sentence_offset'].fillna(False, inplace=True)\n",
    "        meta.drop('next_word_id', axis=1, inplace=True)\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: True if x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1 else False, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "\n",
    "        # Const stop\n",
    "        meta['next_closing'] = meta['n_closing'].shift(-1)\n",
    "        meta['constituent_offset'] = meta.apply(lambda x: True if x['n_closing'] > x['next_closing'] else False, axis=1)\n",
    "        meta['constituent_offset'].fillna(False, inplace=True)\n",
    "        meta.drop('next_closing', axis=1, inplace=True)\n",
    "\n",
    "        for start in ('onset', 'offset'): \n",
    "            # for level in ('word', 'constituent', 'sentence'):\n",
    "            for level in ('sentence', 'constituent', 'word'):\n",
    "                # Select only the rows containing the True for the conditions (sentence_end, etc..)\n",
    "                sel = meta.query(f'{level}_{start}==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "                # TODO check variance as well for sentences\n",
    "                # Matchlist events and meta\n",
    "                # So that we can epoch now that's we've sliced our metadata\n",
    "                i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "                sel = sel.reset_index().loc[j]\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw), decim = 10,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop',\n",
    "                                            preload=True)  # n_words OR n_constitutent OR n_sentences\n",
    "                epoch_key = f'{level}_{start}'\n",
    "            \n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "\n",
    "# Once we have the dict of epochs per condition full, we can concatenate them, and fix the dev_head             \n",
    "for start_ in ('onset', 'offset'): \n",
    "    for level_ in ('word', 'constituent', 'sentence'):\n",
    "        epoch_key = f'{level_}_{start_}'\n",
    "        all_epochs_chosen = dict_epochs[epoch_key]\n",
    "        # Concatenate epochs\n",
    "        for epo in all_epochs_chosen:\n",
    "            epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "        dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n",
    "            \n",
    "dict_evos = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "# Dict init\n",
    "for start in ('onset', 'offset'): \n",
    "        for level in ('word', 'constituent', 'sentence'):\n",
    "            epoch_key = f'{level}_{start}'\n",
    "            dict_evos[epoch_key] = [] \n",
    "\n",
    "# Now that we have all the epochs, rerun the plotting / decoding on averaged\n",
    "for start in ('onset', 'offset'): \n",
    "        for level in ('word', 'constituent', 'sentence'):  \n",
    "            epoch_key = f'{level}_{start}'\n",
    "            epochs = dict_epochs[epoch_key]\n",
    "            # mean\n",
    "            evo = epochs.copy().pick_types(meg=True).average(method='median')\n",
    "            dict_evos[epoch_key] = evo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740163c2",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e5db5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for level in ('word', 'constituent', 'sentence'):\n",
    "    for start in ('onset', 'offset'):        \n",
    "            epoch_key = f'{level}_{start}'\n",
    "            print(f\"Plotting for: {epoch_key}\")\n",
    "            dict_evos[epoch_key].plot(gfp=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb01a54",
   "metadata": {},
   "source": [
    "### #1\n",
    "First plot: 3x2 plot, that shows:\n",
    "- From the onset, and offset of {word, constituent, sentence}:\n",
    "\n",
    "The evoked potential linked to it, as well as the decoding of the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38245f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mne_events(meta, raw, start, level):\n",
    "    if start=='onset':\n",
    "        events = np.ones((len(meta), 3), dtype=int)\n",
    "        events[:, 0] = meta.start * raw.info[\"sfreq\"]\n",
    "        return dict(events=events, metadata=meta.reset_index())\n",
    "    elif start=='offset':\n",
    "        \"\"\"\n",
    "        It should generalize, no need for different cases?\n",
    "        if level == 'word':\n",
    "            events = np.ones((len(meta), 3), dtype=int)\n",
    "            events[:, 0] = (meta.start+meta.duration) * raw.info[\"sfreq\"]\n",
    "            return dict(events=events, metadata=meta.reset_index())\n",
    "        elif level == 'sentence':\n",
    "            events = np.ones((len(meta), 3), dtype=int)\n",
    "            events[:, 0] = (meta.start+meta.duration) * raw.info[\"sfreq\"]\n",
    "            return dict(events=events, metadata=meta.reset_index())\n",
    "\n",
    "        else:\n",
    "            print('hi')\n",
    "            # Fill\n",
    "        \"\"\"\n",
    "        events = np.ones((len(meta), 3), dtype=int)\n",
    "        events[:, 0] = (meta.start+meta.duration) * raw.info[\"sfreq\"]\n",
    "        return dict(events=events, metadata=meta.reset_index())\n",
    "        \n",
    "    else:\n",
    "        print('start should be either onset or offset')\n",
    "        return 0\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15915c86",
   "metadata": {},
   "source": [
    "# Test on words offset only # Done\n",
    "\n",
    "# Generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fb212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataset import read_raw, get_subjects, get_path\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "all_evos = []\n",
    "all_scores = []\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 9\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "levels = ('word','constituent','sentence')\n",
    "starts = ('onset', 'offset')\n",
    "            \n",
    "for subject in subjects[2:6]:\n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    # Dict init\n",
    "    for start in starts: \n",
    "            for level in levels:\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "        meta['word_stop'] = meta.start + meta.duration\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sentence stop info\n",
    "        meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "        for s, d in meta.groupby('sentence_id'):\n",
    "            meta.loc[d.index, 'sent_word_id'] = range(len(d))\n",
    "            meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'sentence_stop'] = d.start.max() # TO Verify!\n",
    "            \n",
    "        # Adding the constituents stop info\n",
    "        meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "        for s, d in meta.groupby('constituent_id'):\n",
    "            meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'constituent_stop'] = d.start.max() # TO Verify!\n",
    "            meta.loc[d.index, 'const_word_id'] = range(len(d))\n",
    "\n",
    "        for start in starts: \n",
    "            # for level in ('word', 'constituent', 'sentence'):\n",
    "            # for level in ('sentence', 'constituent', 'word'):\n",
    "            for level in levels:\n",
    "                \n",
    "                # Select only the rows containing the True for the conditions\n",
    "                # Simplified to only get for the onset: sentence onset epochs, constituent onset epochs,etc\n",
    "                sel = meta.query(f'{level}_onset==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "                # TODO check variance as well for sentences\n",
    "                # Matchlist events and meta\n",
    "                # So that we can epoch now that's we've sliced our metadata\n",
    "                i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "                sel = sel.reset_index().loc[j]\n",
    "                # Making sure there is not hidden bug when matching\n",
    "                assert sel.shape[0] > 0.8 *  (meta.query(f'{level}_onset==True')).shape[0]\n",
    "\n",
    "                # Epoching from the metadata having all onset events: if the start=Offset, the mne events\n",
    "                # Function will epoch on the offset of each level instead of the onset\n",
    "                # TODO: add adaptative baseline\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop', # check event repeated\n",
    "                                            preload=True,\n",
    "                                                baseline=None)  # n_words OR n_constitutent OR n_sentences\n",
    "                epoch_key = f'{level}_{start}'\n",
    "\n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "            \n",
    "    # Once we have the dict of epochs per condition full (epoching for each run for a subject)\n",
    "    # we can concatenate them, and fix the dev_head             \n",
    "    for start_ in starts: \n",
    "        for level_ in levels:\n",
    "            epoch_key = f'{level_}_{start_}'\n",
    "            all_epochs_chosen = dict_epochs[epoch_key]\n",
    "            # Concatenate epochs\n",
    "\n",
    "            for epo in all_epochs_chosen:\n",
    "                epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "            dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n",
    "\n",
    "    # Now that we have all the epochs, rerun the plotting / decoding on averaged\n",
    "    for start in starts: \n",
    "        for level in levels:\n",
    "            epoch_key = f'{level}_{start}'\n",
    "            epochs = dict_epochs[epoch_key]\n",
    "            # mean\n",
    "            evo = epochs.copy().pick_types(meg=True).average(method='median')\n",
    "            all_evos.append(dict(subject=subject, evo=evo, start=start, level=level))\n",
    "\n",
    "\n",
    "            # decoding word emb\n",
    "            epochs = epochs.load_data().pick_types(meg=True, stim=False, misc=False)\n",
    "            X = epochs.get_data()\n",
    "            embeddings = epochs.metadata.word.apply(lambda word: nlp(word).vector).values\n",
    "            embeddings = np.array([emb for emb in embeddings])\n",
    "            R_vec = decod_xy(X, embeddings)\n",
    "            scores = np.mean(R_vec, axis=1)\n",
    "\n",
    "            for t, score in enumerate(scores):\n",
    "                all_scores.append(dict(subject=subject, score=score, start=start, level=level, t=epochs.times[t]))\n",
    "\n",
    "all_scores = pd.DataFrame(all_scores)\n",
    "all_evos = pd.DataFrame(all_evos)\n",
    "\n",
    "all_scores.to_csv('./score.csv')\n",
    "all_evos.to_csv('./evos.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e983491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores = pd.DataFrame(all_scores)\n",
    "all_evos = pd.DataFrame(all_evos)\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(16, 10), dpi=80)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2)\n",
    "\n",
    "for axes_, level in zip( axes, levels):  \n",
    "    for ax, start in zip( axes_, starts):  \n",
    "        cond1 = all_scores.level==f'{level}'\n",
    "        cond2 = all_scores.start==f'{start}'\n",
    "        data = all_scores[ cond1 & cond2]\n",
    "        y = []\n",
    "        x = []\n",
    "        for s, t in data.groupby('t'):\n",
    "            score_avg = t.score.mean()\n",
    "            y.append(score_avg)\n",
    "            x.append(s)\n",
    "\n",
    "        ax.plot(x,y)\n",
    "        ax.set_title(f'{level} {start}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67755eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel[['word','start']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11151285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import read_raw, get_subjects, get_path\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "all_evos = []\n",
    "all_scores = []\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 9\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "\n",
    "            \n",
    "for subject in subjects[2]:\n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    # Dict init\n",
    "    for start in ('onset', 'offset'): \n",
    "            for level in ('word', 'constituent', 'sentence'):\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "    for run in range(1,2):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "        meta['word_stop'] = meta.start + meta.duration\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sentence stop info\n",
    "        meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "        for s, d in meta.groupby('sentence_id'):\n",
    "            meta.loc[d.index, 'sent_word_id'] = range(len(d))\n",
    "            meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'sentence_stop'] = d.start.max()\n",
    "            \n",
    "        # Adding the constituents stop info\n",
    "        meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "        for s, d in meta.groupby('constituent_id'):\n",
    "            meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'constituent_stop'] = d.start.max()\n",
    "            meta.loc[d.index, 'const_word_id'] = range(len(d))\n",
    "        print(meta.head(50))\n",
    "        \n",
    "        for start in ('onset', 'offset'): \n",
    "            # for level in ('word', 'constituent', 'sentence'):\n",
    "            # for level in ('sentence', 'constituent', 'word'):\n",
    "            level = 'word'\n",
    "                \n",
    "            # Select only the rows containing the True for the conditions\n",
    "            # Simplified to only get for the onset: sentence onset epochs, constituent onset epochs,etc\n",
    "            sel = meta.query(f'{level}_onset==True')\n",
    "            assert sel.shape[0] > 10  #\n",
    "            # TODO check variance as well for sentences\n",
    "            # Matchlist events and meta\n",
    "            # So that we can epoch now that's we've sliced our metadata\n",
    "            i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "            sel = sel.reset_index().loc[j]\n",
    "            # Making sure there is not hidden bug when matching\n",
    "            assert sel.shape[0] > 0.8 *  (meta.query(f'{level}_onset==True')).shape[0]\n",
    "\n",
    "            # Epoching from the metadata having all onset events: if the start=Offset, the mne events\n",
    "            # Function will epoch on the offset of each level instead of the onset\n",
    "            # TODO: add adaptative baseline\n",
    "            epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
    "                                 tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                   tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                     event_repeated = 'drop', # check event repeated\n",
    "                                        preload=True,\n",
    "                                            baseline=None)  # n_words OR n_constitutent OR n_sentences\n",
    "            epoch_key = f'{level}_{start}'\n",
    "\n",
    "            dict_epochs[epoch_key].append(epochs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9906493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs.copy().pick_types(meg=True).average(method='median').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed860c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import read_raw, get_subjects, get_path, mne_events\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "all_evos = []\n",
    "all_scores = []\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 9\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "\n",
    "\n",
    "            \n",
    "for subject in subjects[2:10]:\n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    # Dict init\n",
    "    for start in ('onset', 'offset'): \n",
    "            for level in ('word', 'constituent', 'sentence'):\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: True if x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1 else False, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sequence\n",
    "        meta['sequence_id'] = np.cumsum(meta.is_last_word.shift(1, fill_value=False))\n",
    "        for s, d in meta.groupby('sequence_id'):\n",
    "            meta.loc[d.index, 'word_id'] = range(len(d))\n",
    "        \n",
    "\n",
    "        for start in ('onset', 'offset'): \n",
    "            # for level in ('word', 'constituent', 'sentence'):\n",
    "            for level in ('sentence', 'constituent', 'word'):\n",
    "                # Select only the rows containing the True for the conditions (sentence_end, etc..)\n",
    "                sel = meta.query(f'{level}_{start}==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "                # TODO check variance as well for sentences\n",
    "                # Matchlist events and meta\n",
    "                # So that we can epoch now that's we've sliced our metadata\n",
    "                i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "                sel = sel.reset_index().loc[j]\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw), decim = 100,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop', # check event repeated\n",
    "                                            preload=True)  # n_words OR n_constitutent OR n_sentences\n",
    "                epoch_key = f'{level}_{start}'\n",
    "            \n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "        \n",
    "            \n",
    "    # Once we have the dict of epochs per condition full (epoching for each run for a subject)\n",
    "    # we can concatenate them, and fix the dev_head             \n",
    "    for start_ in ('onset', 'offset'): \n",
    "        for level_ in ('word', 'constituent', 'sentence'):\n",
    "            epoch_key = f'{level_}_{start_}'\n",
    "            all_epochs_chosen = dict_epochs[epoch_key]\n",
    "            # Concatenate epochs\n",
    "            for epo in all_epochs_chosen:\n",
    "                epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "            dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n",
    "\n",
    "    # Now that we have all the epochs, rerun the plotting / decoding on averaged\n",
    "    for start in ('onset', 'offset'): \n",
    "            for level in ('word', 'constituent', 'sentence'):  \n",
    "                epoch_key = f'{level}_{start}'\n",
    "                epochs = dict_epochs[epoch_key]\n",
    "                # mean\n",
    "                evo = epochs.copy().pick_types(meg=True).average(method='median')\n",
    "                all_evos.append(dict(subject=subject, evo=evo, start=start, level=level))\n",
    "\n",
    "\n",
    "                # decoding word emb\n",
    "                epochs = epochs.load_data().pick_types(meg=True, stim=False, misc=False)\n",
    "                X = epochs.get_data()\n",
    "                embeddings = epochs.metadata.word.apply(lambda word: nlp(word).vector).values\n",
    "                embeddings = np.array([emb for emb in embeddings])\n",
    "                R_vec = decod_xy(X, embeddings)\n",
    "                scores = np.mean(R_vec, axis=1)\n",
    "\n",
    "                for t, score in enumerate(scores):\n",
    "                    all_scores.append(dict(subject=subject, score=score, start=start, level=level, t=epochs.times[t]))\n",
    "\n",
    "all_scores = pd.DataFrame(all_scores,index=False)\n",
    "all_evos = pd.DataFrame(all_evos,index=False)\n",
    "\n",
    "all_scores.to_csv('./score.csv')\n",
    "all_evos.to_csv('./evos.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = pd.DataFrame(all_scores)\n",
    "all_evos = pd.DataFrame(all_evos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores.query('level==\"sentence\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(16, 10), dpi=80)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2)\n",
    "\n",
    "for axes_, level in zip(axes, ('word', 'constituent', 'sentence')):\n",
    "    for ax, start in zip( axes_, ('onset', 'offset')):  \n",
    "        cond1 = all_scores.level==f'{level}'\n",
    "        cond2 = all_scores.start==f'{start}'\n",
    "        data = all_scores[ cond1 & cond2]\n",
    "        print(data.shape)\n",
    "        x = data['t']\n",
    "        y = data['score']\n",
    "        \n",
    "        ax.plot(x,y)\n",
    "        ax.set_title(f'{level} {start}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd270ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67f9be",
   "metadata": {},
   "source": [
    "###  #3\n",
    "Now the same idea, but iterating on the targeted decoding: {word embedding, sentence embedding, etc..}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34178be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b72a766",
   "metadata": {},
   "source": [
    "### #4 \n",
    "Now baselined on offset, no matter whether it's on onset or offset window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c77b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "824fb580",
   "metadata": {},
   "source": [
    "### #5 \n",
    "Submitit-compatible version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1494a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install submitit\n",
    "import submitit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621358ab",
   "metadata": {},
   "source": [
    "# Jitter test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c5154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3507de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne_bids\n",
    "import mne\n",
    "from pathlib import Path\n",
    "subject = '5'\n",
    "run_id = 1\n",
    "path = get_path(\"LPP_read\")\n",
    "task = \"read\"\n",
    "print(f\"\\n Epoching for run {run_id}, subject: {subject}\\n\")\n",
    "bids_path = mne_bids.BIDSPath(\n",
    "    subject=subject,\n",
    "    session=\"01\",\n",
    "    task=task,\n",
    "    datatype=\"meg\",\n",
    "    root=path,\n",
    "    run=run_id,\n",
    ")\n",
    "\n",
    "raw = mne_bids.read_raw_bids(bids_path)\n",
    "raw.del_proj()  # To fix proj issues\n",
    "raw.pick_types(meg=True, stim=True)\n",
    "\n",
    "# Generate event_file path\n",
    "event_file = path / f\"sub-{bids_path.subject}\"\n",
    "event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "event_file = event_file / \"meg\"\n",
    "event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "event_file += f\"_ses-{bids_path.session}\"\n",
    "event_file += f\"_task-{bids_path.task}\"\n",
    "event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "assert Path(event_file).exists()\n",
    "\n",
    "events = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "events_  = events[:,0] / raw.info[\"sfreq\"]\n",
    "diffs = np.diff(events_)\n",
    "x,y = np.unique(diffs, return_counts=True)\n",
    "plt.plot(x,y)\n",
    "plt.xlim([0.2,0.3])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa654f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw.copy().pick_types(meg=False, stim=True).plot(start=50, duration=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4566943",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = mne.find_events(raw, stim_channel=\"STI002\", shortest_event=1)\n",
    "events_  = events[:,0] / raw.info[\"sfreq\"]\n",
    "diffs = np.diff(events_)\n",
    "x,y = np.unique(diffs, return_counts=True)\n",
    "plt.plot(x,y)\n",
    "plt.xlim([0.2,0.4])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "meta = epochs.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb0fef",
   "metadata": {},
   "source": [
    "# Testing decoding more and more difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import word_epochs_debug, get_path, get_subjects, sentence_epochs_debug\n",
    "from utils import decod\n",
    "from plot import plot_R\n",
    "import mne\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "subjects = subjects[:2]\n",
    "\n",
    "# WORDS\n",
    "all_epochs = []\n",
    "for sub in subjects:\n",
    "\n",
    "    epochs = sentence_epochs_debug(sub, 3)\n",
    "    all_epochs.append(epochs)\n",
    "\n",
    "for epo in all_epochs:\n",
    "    epo.info[\"dev_head_t\"] = all_epochs[1].info[\"dev_head_t\"]\n",
    "\n",
    "epochs = mne.concatenate_epochs(all_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import numpy as np\n",
    "from utils import correlate\n",
    "\n",
    "def decod(X, y):\n",
    "    assert len(X) == len(y)\n",
    "    # define data\n",
    "    model = make_pipeline(StandardScaler(), RidgeCV(alphas=np.logspace(-1, 6, 10)))\n",
    "    cv = KFold(15, shuffle=True, random_state=0)\n",
    "\n",
    "    # fit predict\n",
    "    n, n_chans, n_times = X.shape\n",
    "    if y.ndim == 1:\n",
    "        y = np.asarray(y).reshape(y.shape[0], 1)\n",
    "    R = np.zeros((n_times, y.shape[1]))\n",
    "\n",
    "    for t in range(n_times):\n",
    "        print(\".\", end=\"\")\n",
    "        rs = []\n",
    "        # y_pred = cross_val_predict(model, X[:, :, t], y, cv=cv)\n",
    "        for train, test in cv.split(X):\n",
    "            model.fit(X[train, :, t], y[train])\n",
    "            y_pred = model.predict(X[test, :, t])\n",
    "            r = correlate(y[test], y_pred)\n",
    "            rs.append(r)\n",
    "        R[t] = np.mean(rs)\n",
    "        # R[t] = correlate(y, y_pred)\n",
    "\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d520c21",
   "metadata": {},
   "source": [
    "# LASER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54996ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_code_path\n",
    "\n",
    "run_id = 1\n",
    "CHAPTERS = {\n",
    "1: \"1-3\",\n",
    "2: \"4-6\",\n",
    "3: \"7-9\",\n",
    "4: \"10-12\",\n",
    "5: \"13-14\",\n",
    "6: \"15-19\",\n",
    "7: \"20-22\",\n",
    "8: \"23-25\",\n",
    "9: \"26-27\",\n",
    "}\n",
    "\n",
    "meta = epochs.metadata\n",
    "\n",
    "# # laser embeddings information\n",
    "dim = 1024\n",
    "embeds = np.fromfile(\n",
    "    f\"{get_code_path()}/data/laser_embeddings/emb_{CHAPTERS[int(run_id)]}.bin\",\n",
    "    dtype=np.float32,\n",
    "    count=-1,\n",
    ")\n",
    "embeds.resize(embeds.shape[0] // dim, dim)\n",
    "print(meta.shape[0])\n",
    "embeds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23935db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = epochs[]\n",
    "X = epochs.get_data()\n",
    "y = epochs.metadata.word.apply(len)\n",
    "R_vec = decod(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0203d6",
   "metadata": {},
   "source": [
    "## Word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cdd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the STIM information before decoding it (or else we'll get a 100% accuracy since the word length info is in the STIM channels)\n",
    "epochs = epochs.pick_types(meg=True, stim=False, misc=False)\n",
    "X = epochs.get_data()\n",
    "y = epochs.metadata.word.apply(len)\n",
    "R_vec = decod(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f798f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_R(R_vec.reshape(-1))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf385dc",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becba2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "epochs = epochs.pick_types(meg=True, stim=False, misc=False)\n",
    "X = epochs.get_data()\n",
    "embeddings = epochs.metadata.word.apply(lambda word: nlp(word).vector).values\n",
    "embeddings = np.array([emb for emb in embeddings])\n",
    "R_vec = decod(X, embeddings)\n",
    "R_vec = np.mean(R_vec, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_R(R_vec.reshape(-1))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c799fbc7",
   "metadata": {},
   "source": [
    "## Laser embeddings with JR baseline fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29290403",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_stop_data.shape\n",
    "baseline_starts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_starts = epochs['word_id==0'].apply_baseline((-.300, 0.))\n",
    "sent_starts.average().plot()\n",
    "\n",
    "sent_stops = epochs['is_last_word']\n",
    "bsl = (epochs.times>-.300 )*(epochs.times<=0)\n",
    "baseline_starts = sent_starts.get_data()[:, :, bsl].mean(-2)\n",
    "\n",
    "sent_stop_data = sent_stops.get_data()\n",
    "n_sentences, n_channels, n_times = sent_stop_data.shape\n",
    "sent_stop_data -= baseline_starts[:, :, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c02e3a",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ce669",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.metaepochs.metadata.word.apply(len)\n",
    "decoding_criterion = \"n_closing\"\n",
    "R_vec = decod(epochs, decoding_criterion)\n",
    "\n",
    "fig = plot_R(R_vec)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb3d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce49ba49",
   "metadata": {},
   "source": [
    "# Generate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_path, get_subjects, word_epochs, sentence_epochs\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import mne\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "mne.set_log_level(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = mne.Report()\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "evos = []\n",
    "\n",
    "# WORDS\n",
    "subjects = subjects[2]\n",
    "epochs = word_epochs(subjects)\n",
    "\n",
    "evo = epochs.average(method=\"median\")\n",
    "evos.append(evo)\n",
    "evo.plot(spatial_colors=True)\n",
    "report.add_evokeds(evo, titles=f\"Evoked for condition word  \")\n",
    "\n",
    "\n",
    "# SENTENCES\n",
    "epochs = sentence_epochs(subjects)\n",
    "\n",
    "evo = epochs.average(method=\"median\")\n",
    "evos.append(evo)\n",
    "evo.plot(spatial_colors=True)\n",
    "report.add_evokeds(evo, titles=f\"Evoked for condition sentence  \")\n",
    "\n",
    "\n",
    "evokeds = dict(sentence=evos[1], word=evos[0])\n",
    "\n",
    "fig = mne.viz.plot_compare_evokeds(evokeds, combine=\"mean\")\n",
    "\n",
    "report.add_figure(fig, title=\"Evoked response comparaison\")\n",
    "\n",
    "\n",
    "report.save(\n",
    "    f\"./figures/{task}_sentvsword_test.html\",\n",
    "    open_browser=False,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "evo = epochs.average(method=\"median\")\n",
    "evo.plot(spatial_colors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc17c75",
   "metadata": {},
   "source": [
    "# Test new functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import word_epochs\n",
    "\n",
    "sub = '3'\n",
    "\n",
    "epochs = word_epochs(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca576c",
   "metadata": {},
   "source": [
    "\n",
    "# Debug events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8123ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne_bids\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import mne\n",
    "from utils import match_list\n",
    "from dataset import mne_events\n",
    "from utils import add_syntax\n",
    "\n",
    "\n",
    "CHAPTERS = {\n",
    "    1: \"1-3\",\n",
    "    2: \"4-6\",\n",
    "    3: \"7-9\",\n",
    "    4: \"10-12\",\n",
    "    5: \"13-14\",\n",
    "    6: \"15-19\",\n",
    "    7: \"20-22\",\n",
    "    8: \"23-25\",\n",
    "    9: \"26-27\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471fd7ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "run_id = 1\n",
    "task = \"read\"\n",
    "subject = '3'\n",
    "baseline_min = -2.0\n",
    "baseline_max = 0.5\n",
    "epoch_on = 'sentence'\n",
    "reference = \"end\"\n",
    "print(f\"\\n Epoching for run {run_id}, subject: {subject}\\n\")\n",
    "bids_path = mne_bids.BIDSPath(\n",
    "    subject=subject,\n",
    "    session=\"01\",\n",
    "    task=task,\n",
    "    datatype=\"meg\",\n",
    "    root=path,\n",
    "    run=run_id,\n",
    ")\n",
    "\n",
    "raw = mne_bids.read_raw_bids(bids_path)\n",
    "raw.del_proj()  # To fix proj issues\n",
    "raw.pick_types(meg=True, stim=True)\n",
    "raw.load_data()\n",
    "raw = raw.filter(0.5, 20)\n",
    "# Generate event_file path\n",
    "event_file = path / f\"sub-{bids_path.subject}\"\n",
    "event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "event_file = event_file / \"meg\"\n",
    "event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "event_file += f\"_ses-{bids_path.session}\"\n",
    "event_file += f\"_task-{bids_path.task}\"\n",
    "event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "assert Path(event_file).exists()\n",
    "\n",
    "# read events\n",
    "meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "events = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_copy = meta.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ba504",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_syntax = get_code_path() / \"data/syntax\"\n",
    "\n",
    "# Enriching the metadata with outside files:\n",
    "meta = add_syntax(meta, path_syntax, int(run_id))\n",
    "\n",
    "# Enriching the metadata with simple operations:\n",
    "\n",
    "# end of sentence information\n",
    "end_of_sentence = [\n",
    "    True\n",
    "    if str(meta.word.iloc[i]).__contains__(\".\")\n",
    "    or str(meta.word.iloc[i]).__contains__(\"?\")\n",
    "    or str(meta.word.iloc[i]).__contains__(\"!\")\n",
    "    else False\n",
    "    for i, _ in enumerate(meta.values[:-1])\n",
    "]\n",
    "end_of_sentence.append(True)\n",
    "meta[\"sentence_end\"] = end_of_sentence\n",
    "\n",
    "# sentence start information\n",
    "list_word_start = [True]\n",
    "list_word_start_to_add = [\n",
    "    True if meta.sentence_end.iloc[i - 1] else False\n",
    "    for i in np.arange(1, meta.shape[0])\n",
    "]\n",
    "for boolean in list_word_start_to_add:\n",
    "    list_word_start.append(boolean)\n",
    "meta[\"sentence_start\"] = list_word_start\n",
    "\n",
    "# laser embeddings information\n",
    "dim = 1024\n",
    "embeds = np.fromfile(\n",
    "    f\"{get_code_path()}/data/laser_embeddings/emb_{CHAPTERS[int(run_id)]}.bin\",\n",
    "    dtype=np.float32,\n",
    "    count=-1,\n",
    ")\n",
    "embeds.resize(embeds.shape[0] // dim, dim)\n",
    "assert embeds.shape[0] == meta.shape[0]\n",
    "meta[\"laser\"] = [emb for emb in embeds]\n",
    "\n",
    "# constituent end information\n",
    "meta[\"constituent_end\"] = [\n",
    "        True if closing > 1 else False for i, closing in enumerate(meta.n_closing)]\n",
    "\n",
    "# constituent start information\n",
    "list_constituent_start = [True]\n",
    "list_constituent_start_to_add = [\n",
    "    True if meta.constituent_end.iloc[i - 1] else False\n",
    "    for i in np.arange(1, meta.shape[0])\n",
    "]\n",
    "for boolean in list_constituent_start_to_add:\n",
    "    list_constituent_start.append(boolean)\n",
    "meta[\"constituent_start\"] = list_constituent_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764fea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_length_meg = events[:, 2]\n",
    "word_len_meta = meta.word.apply(len)\n",
    "i, j = match_list(word_len_meta, word_length_meg)\n",
    "events = events[j]\n",
    "assert len(i) / meta.shape[0] > 0.8\n",
    "meta = meta.iloc[i].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c337c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[\"start\"] = events[:, 0] / raw.info[\"sfreq\"]\n",
    "meta[\"condition\"] = \"sentence\"\n",
    "meta = meta.sort_values(\"start\").reset_index(drop=True)\n",
    "meta[\"word_start\"] = meta[\"start\"]\n",
    "meta[\"word_end\"] = meta[\"word_start\"] + meta[\"duration\"]\n",
    "\n",
    "epochs = mne.Epochs(\n",
    "    raw, **mne_events(meta, raw), decim=20, tmin=baseline_min, tmax=baseline_max\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5330c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feec9e4",
   "metadata": {},
   "source": [
    "# Plotting decoding info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743045da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plot import plot_subject\n",
    "\n",
    "from dataset import get_path, get_subjects, get_code_path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"read\"\n",
    "sub = 4\n",
    "epoch_on = 'sentence'\n",
    "reference = \"end\"\n",
    "min = -4.0\n",
    "max = 0.5\n",
    "decoding_criterion = 'laser'\n",
    "path = get_code_path()\n",
    "# Format the file path\n",
    "\n",
    "# Open the pandas DataFrame containing the decoding values\n",
    "R = np.load(\n",
    "    (path) / f\"decoding/results/{task}/decoding_{decoding_criterion}_{epoch_on}_{reference}_{sub}.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42da6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad237f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.linspace(min, max, R.shape[0])  # To do better at generalizing\n",
    "fig, ax = plt.subplots(1, figsize=[6, 6])\n",
    "dec = plt.fill_between(times, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"read\"\n",
    "sub = 3\n",
    "epoch_on = 'sentence'\n",
    "reference = \"end\"\n",
    "min = -4.0\n",
    "max = 0.5\n",
    "decoding_criterion = 'laser'\n",
    "plot = plot_subject(sub, decoding_criterion, task, reference, epoch_on, min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3cdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77a76bc3",
   "metadata": {},
   "source": [
    "# Debugging ERP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750be218",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "RUN = 1\n",
    "baseline_min = -1.0\n",
    "baseline_max = 1.0\n",
    "task = \"read\"\n",
    "\n",
    "subjects = subjects[10]\n",
    "epochs2 = epoch_subjects(\n",
    "    subjects, RUN, task, path, baseline_max=baseline_max, baseline_min=baseline_min\n",
    ")\n",
    "\n",
    "epochs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56126bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs2.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044bba4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Homemade imports\n",
    "from dataset import get_path, get_subjects, epoch_subjects, epochs_slice\n",
    "from plot import plot_subject\n",
    "\n",
    "# General imports\n",
    "import numpy as np\n",
    "import mne\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "mne.set_log_level(False)\n",
    "\n",
    "# Later: integrate Hydra here as well. For now, just simple plotting of ERPS\n",
    "\n",
    "report = mne.Report()\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "RUN = 1\n",
    "baseline_min = -1.0\n",
    "baseline_max = 1.0\n",
    "task = \"read\"\n",
    "print(\"\\nSubjects for which the plotting will be done: \\n\")\n",
    "print(subjects)\n",
    "\n",
    "# DEBUG\n",
    "subjects = subjects[4]\n",
    "epochs_ = epoch_subjects(\n",
    "    subjects, RUN, task, path, baseline_max=baseline_max, baseline_min=baseline_min\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build a 3x2 plot, with for each condition (sentence, word, constituent), and for (start, end),\n",
    "# the ERP associated\n",
    "cond = [\"sentence\", \"word\", \"constituent\"]\n",
    "cases = {\"start\", \"end\"}\n",
    "\n",
    "# Plotting and adding to the report, the averaged ERPs of:\n",
    "# words, sentences and constituents, centered at the beginning and end of each\n",
    "\n",
    "\n",
    "# Need to map for:\n",
    "# - end of word,\n",
    "# - beginning of sentence (epochs[i+1] !danger limits)\n",
    "# - beginning of constituent (epochs[i+1] !same danger)\n",
    "\n",
    "evos = []\n",
    "for condi in cond:\n",
    "    for case in cases:\n",
    "        # Slice the epochs based on the epoch_criterion:\n",
    "        column_to_slice_on = f\"{condi}_{case}\"\n",
    "        if condi == \"sentence\" or condi == \"word\":  # eg: {sentence}_{end} or {word}_{start}\n",
    "            epochs = epochs_slice(epochs_, column_to_slice_on)\n",
    "        elif condi == \"constituent\":\n",
    "            epochs = epochs_slice(epochs_, column_to_slice_on, value=2, equal='sup')\n",
    "        evo = epochs.average(method=\"median\")\n",
    "        evos.append(evo)\n",
    "        evo.plot(spatial_colors=True)\n",
    "        report.add_evokeds(evo, titles=f\"Evoked for condition {column_to_slice_on}  \")\n",
    "\n",
    "evokeds = dict(sentence=evos[0], word=evos[2], constituent=evos[4])\n",
    "\n",
    "fig = mne.viz.plot_compare_evokeds(evokeds, combine=\"mean\")\n",
    "\n",
    "report.add_figure(fig, title=\"Evoked response comparaison\")\n",
    "\n",
    "\n",
    "report.save(\n",
    "    f\"./figures/{task}_ERP_all_cond.html\",\n",
    "    open_browser=False,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b61d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0419c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = epochs_slice(epochs_, 'sentence_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03bb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ae1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot import plot_subject\n",
    "\n",
    "from dataset import get_path, get_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "run = 1\n",
    "task = \"read\"\n",
    "subject = '17'\n",
    "baseline_min = -2.0\n",
    "baseline_max = 0.5\n",
    "epoch_on = 'sentence'\n",
    "reference = \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5143d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06188ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataset import epoch_data\n",
    "epo = epoch_data(\n",
    "    subject,\n",
    "    run,\n",
    "    task,\n",
    "    path,\n",
    "    baseline_min,\n",
    "    baseline_max,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54047baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import epochs_slice\n",
    "epos = epochs_slice(epo, 'sentence_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794e34b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evo = epos.average(method=\"median\")\n",
    "evo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a67a6a0",
   "metadata": {},
   "source": [
    "# GFP for sentence - epoching on sentence end and go from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade imports\n",
    "from dataset import get_path, get_subjects, epoch_runs\n",
    "from plot import plot_subject\n",
    "\n",
    "# General imports\n",
    "import numpy as np\n",
    "import mne\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from wordfreq import zipf_frequency\n",
    "from Levenshtein import editops\n",
    "\n",
    "# Tools\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "from utils import match_list, add_syntax\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa49be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "RUN = 2\n",
    "task = \"read\"\n",
    "subject = subjects[1]\n",
    "baseline_min = -4.0\n",
    "baseline_max = 0.5\n",
    "epoch_on = 'sentence'\n",
    "reference = \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1bca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = epoch_runs(\n",
    "            subject,\n",
    "            RUN,\n",
    "            task,\n",
    "            path,\n",
    "            baseline_min,\n",
    "            baseline_max,\n",
    "            epoch_on=epoch_on,\n",
    "            reference=reference,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa8af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run = 1\n",
    "from dataset import epoch_data\n",
    "epo = epoch_data(\n",
    "    subject,\n",
    "    run,\n",
    "    task,\n",
    "    path,\n",
    "    baseline_min=-0.2,\n",
    "    baseline_max=0.8,\n",
    "    filter=True,\n",
    "    epoch_on=\"word\",\n",
    "    reference=\"end\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06639bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = epo.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1596c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_code_path, get_path, mne_events\n",
    "\n",
    "run_id = RUN\n",
    "epoch_on = 'word'\n",
    "reference = \"start\"\n",
    "\n",
    "CHAPTERS = {\n",
    "    1: \"1-3\",\n",
    "    2: \"4-6\",\n",
    "    3: \"7-9\",\n",
    "    4: \"10-12\",\n",
    "    5: \"13-14\",\n",
    "    6: \"15-19\",\n",
    "    7: \"20-22\",\n",
    "    8: \"23-25\",\n",
    "    9: \"26-27\",\n",
    "}\n",
    "\n",
    "\n",
    "bids_path = mne_bids.BIDSPath(\n",
    "        subject=subject,\n",
    "        session=\"01\",\n",
    "        task=task,\n",
    "        datatype=\"meg\",\n",
    "        root=path,\n",
    "        run=RUN,\n",
    "    )\n",
    "\n",
    "raw = mne_bids.read_raw_bids(bids_path)\n",
    "raw.del_proj()  # To fix proj issues\n",
    "raw.pick_types(meg=True, stim=True)\n",
    "raw.load_data()\n",
    "raw = raw.filter(0.5, 20)\n",
    "# Generate event_file path\n",
    "event_file = path / f\"sub-{bids_path.subject}\"\n",
    "event_file = event_file / f\"ses-{bids_path.session}\"\n",
    "event_file = event_file / \"meg\"\n",
    "event_file = str(event_file / f\"sub-{bids_path.subject}\")\n",
    "event_file += f\"_ses-{bids_path.session}\"\n",
    "event_file += f\"_task-{bids_path.task}\"\n",
    "event_file += f\"_run-{bids_path.run}_events.tsv\"\n",
    "assert Path(event_file).exists()\n",
    "\n",
    "# read events\n",
    "meta = pd.read_csv(event_file, sep=\"\\t\")\n",
    "events = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "if (\n",
    "    bids_path.task == \"read\" and bids_path.subject == \"2\"\n",
    "):  # A trigger value bug for this subject\n",
    "    word_length_meg = (\n",
    "        events[:, 2] - 2048\n",
    "    )  # Remove first event: chapter start and remove offset\n",
    "else:\n",
    "    word_length_meg = events[:, 2]\n",
    "# Here, the trigger value encoded the word length\n",
    "# which helps us realign triggers\n",
    "# From the event file / from the MEG events\n",
    "word_len_meta = meta.word.apply(len)\n",
    "i, j = match_list(word_len_meta, word_length_meg)\n",
    "events = events[j]\n",
    "meta = meta.iloc[i].reset_index()\n",
    "print(meta.shape)\n",
    "# The start parameter will help us\n",
    "# keep the link between raw events and metadata\n",
    "meta[\"start\"] = events[:, 0] / raw.info[\"sfreq\"]\n",
    "meta[\"condition\"] = \"sentence\"\n",
    "meta = meta.sort_values(\"start\").reset_index(drop=True)\n",
    "\n",
    "# Raw LPP textual data\n",
    "path_txt = get_code_path() / \"data/txt_raw\"\n",
    "# LPP Syntax data\n",
    "path_syntax = get_code_path() / \"data/syntax\"\n",
    "\n",
    "# Enriching the metadata with outside files:\n",
    "meta = add_syntax(meta, path_syntax, int(run_id))\n",
    "print(meta.shape)\n",
    "# Add the information on the sentence ending:\n",
    "# Only works for reading: TO FIX for listening... to see with Christophe\n",
    "# Also: only works for v2 (subject 1 (me) doesn't work )\n",
    "\n",
    "# Test 1\n",
    "# end_of_sentence = [\n",
    "#     True if meta.onset.iloc[i + 1] - meta.onset.iloc[i] > 0.7 else False\n",
    "#     for i, _ in enumerate(meta.values[:-1])\n",
    "# ]\n",
    "# end_of_sentence.append(True)\n",
    "\n",
    "# Test 2\n",
    "end_of_sentence = [\n",
    "    True\n",
    "    if str(meta.word.iloc[i]).__contains__(\".\")\n",
    "    or str(meta.word.iloc[i]).__contains__(\"?\")\n",
    "    or str(meta.word.iloc[i]).__contains__(\"!\")\n",
    "    else False\n",
    "    for i, _ in enumerate(meta.values[:-1])\n",
    "]\n",
    "end_of_sentence.append(True)\n",
    "meta[\"sentence_end\"] = end_of_sentence\n",
    "\n",
    "# We are considering different cases:\n",
    "# Are we epoching on words, sentences, or constituents?\n",
    "# Different epoching for different analysis\n",
    "if epoch_on == \"word\" and reference == \"start\":\n",
    "    # Default case, so nothing to change\n",
    "    # Could be removed but kept for easy of reading\n",
    "    happy = True\n",
    "# Word end\n",
    "if epoch_on == \"word\" and reference == \"end\":\n",
    "    # Little hack: not really pretty but does the job\n",
    "    # As epoching again uses the start column, we rename it like that\n",
    "    # But it should be meta[\"end\"] instead...\n",
    "    meta[\"start\"] = [row[\"start\"] + row[\"duration\"] for i, row in meta.iterrows()]\n",
    "\n",
    "# Sentence end\n",
    "elif epoch_on == \"sentence\" and reference == \"end\":\n",
    "    # Add a LASER embeddings column for decoding\n",
    "    dim = 1024\n",
    "    embeds = np.fromfile(\n",
    "        f\"{get_code_path()}/data/laser_embeddings/emb_{CHAPTERS[int(run_id)]}.bin\",\n",
    "        dtype=np.float32,\n",
    "        count=-1,\n",
    "    )\n",
    "    embeds.resize(embeds.shape[0] // dim, dim)\n",
    "    column = \"sentence_end\"\n",
    "    value = True\n",
    "    meta = meta[meta[column] == value]\n",
    "    # TODO: create a match list between the embeds sentence and the\n",
    "    print(embeds.shape[0], meta.shape[0])\n",
    "    assert embeds.shape[0] == meta.shape[0]\n",
    "    meta[\"laser\"] = [emb for emb in embeds]\n",
    "    print(\"Added embeddings\")\n",
    "# Sentence start\n",
    "elif epoch_on == \"sentence\" and reference == \"start\":\n",
    "    # Create a sentence-start column:\n",
    "    # list_word_start = [\n",
    "    #     True\n",
    "    #     for i, is_last_word in enumerate(meta.is_last_word[:-1])\n",
    "    #     if meta.is_last_word[i + 1]\n",
    "    # ]\n",
    "    list_word_start = [True]\n",
    "    list_word_start_to_add = [\n",
    "        True if meta.sentence_end[i - 1] else False\n",
    "        for i, _ in enumerate(meta.sentence_end[1:])\n",
    "    ]\n",
    "    for boolean in list_word_start_to_add:\n",
    "        list_word_start.append(boolean)\n",
    "    meta[\"sentence_start\"] = list_word_start\n",
    "    column = \"sentence_start\"\n",
    "    value = True\n",
    "    meta = meta[meta[column] == value]\n",
    "# Constituent start\n",
    "elif epoch_on == \"constituent\" and reference == \"start\":\n",
    "    # Create a constituent-start column:\n",
    "    meta[\"constituent_start\"] = [\n",
    "        True for i, _ in enumerate(meta.is_last_word[1:]) if meta.n_closing > 1\n",
    "    ]\n",
    "    column = \"constituent_start\"\n",
    "    value = True\n",
    "    meta = meta[meta[column] == value]\n",
    "# Constituent end\n",
    "elif epoch_on == \"constituent\" and reference == \"end\":\n",
    "    # Create a constituent-start column:\n",
    "    meta[\"constituent_start\"] = [\n",
    "        True for i, _ in enumerate(meta.is_last_word[1:]) if meta.n_closing > 1\n",
    "    ]\n",
    "    column = \"constituent_start\"\n",
    "    value = True\n",
    "    meta = meta[meta[column] == value]\n",
    "epochs = mne.Epochs(\n",
    "    raw, **mne_events(meta, raw), decim=20, tmin=baseline_min, tmax=baseline_max\n",
    ")\n",
    "# epochs = epochs['kind==\"word\"']\n",
    "# epochs.metadata[\"closing\"] = epochs.metadata.closing_.fillna(0)\n",
    "epochs.load_data()\n",
    "epochs = epochs.pick_types(meg=True, stim=False, misc=False)\n",
    "data = epochs.get_data()\n",
    "\n",
    "# Scaling the data\n",
    "n_words, n_chans, n_times = data.shape\n",
    "vec = data.transpose(0, 2, 1).reshape(-1, n_chans)\n",
    "scaler = RobustScaler()\n",
    "idx = np.arange(len(vec))\n",
    "np.random.shuffle(idx)\n",
    "vec = scaler.fit(vec[idx[:20_000]]).transform(vec)\n",
    "# To try: sigmas = 7 or 15\n",
    "sigma = 7\n",
    "vec = np.clip(vec, -sigma, sigma)\n",
    "epochs._data[:, :, :] = (\n",
    "    scaler.inverse_transform(vec)\n",
    "    .reshape(n_words, n_times, n_chans)\n",
    "    .transpose(0, 2, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad079b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_2 = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1)\n",
    "events_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d315880",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbdc744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646dde6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870d8b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a609b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995472f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255968bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785eb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c62475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f38b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a92e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52e158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685a43c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d58005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import decod\n",
    "R_vec = decod(epochs, decoding_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905586b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.fill_between(epochs.times, R_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40765d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f47780",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.diff(epochs.metadata.onset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8eb0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs2 = epoch_runs(subject, RUN, task, path, baseline_min,baseline_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e9edd",
   "metadata": {},
   "source": [
    "x \n",
    "x \n",
    "x \n",
    "x \n",
    "x \n",
    "x \n",
    "x \n",
    "x \n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482306bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.round(np.diff(epochs2.metadata.onset),3)\n",
    "unique, counts = np.unique(arr, return_counts=True)\n",
    "\n",
    "# Print unique values and their counts\n",
    "for val, count in zip(unique, counts):\n",
    "    print(f\"{val} occurs {count} times.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55740958",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.diff(epochs2.metadata.onset)>0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cbf92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddcf368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d44ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f35cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c122719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9a8ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evo = epochs.average(method=\"median\")\n",
    "evo.plot(gfp='only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dae2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epochs_(epochs, column, value):\n",
    "    meta  = epochs.metadata\n",
    "    subset = meta[meta[column]==value].level_0\n",
    "    return epochs[subset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba0ee8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs.metadata['n_closing'][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69967734",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_(epochs,'is_last_word',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0cbc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Build a 3x2 plot, with for each condition (sentence, word, constituent), and for (start, end),\n",
    "# the ERP associated\n",
    "cond = {'sentence': {'column':'is_last_word','target':True},\n",
    "        'word': {'column':'kind','target':'word'},\n",
    "        'constituent': {'column':'n_closing','target':2}}\n",
    "\n",
    "cases = {'start', 'end'}\n",
    "\n",
    "i = 1\n",
    "for condi in cond:\n",
    "    for case in cases:\n",
    "        ep = epochs_(epochs, cond[condi]['column'], cond[condi]['target'])\n",
    "        ax = fig.add_subplot(3, 2, i)\n",
    "        #ep.average().plot(gfp='only')\n",
    "        evo = ep.average(method=\"median\")\n",
    "        evo.plot(spatial_colors=True)\n",
    "        i = i + 1\n",
    "        ax.set_title(f'Plot {cond}')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3217312",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_4 = epochs_(epochs, 'n_closing', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb4772",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_4.average().plot(gfp='only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evo = epochs.average(method=\"median\")\n",
    "evo.plot(spatial_colors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5e02d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a87839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne import Epochs\n",
    "\n",
    "class CustomEpochs(Epochs):\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # Parse the key into metadata field name and value\n",
    "        field, value = key.split('==')\n",
    "        field = field.strip()\n",
    "        value = value.strip()\n",
    "\n",
    "        # Get the indices of the epochs that match the metadata query\n",
    "        indices = [i for i, metadata in enumerate(self.metadata[field]) if metadata == value]\n",
    "\n",
    "        # Return a new Epochs object containing only the matching epochs\n",
    "        return self.__class__(self._data[indices], self.events[indices], self.event_id,\n",
    "                              tmin=self.tmin, tmax=self.tmax, baseline=self.baseline,\n",
    "                              metadata=self.metadata.iloc[indices], info=self.info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_epochs = CustomEpochs(epochs, epochs.events, \"1\", -0.2, 0.8, epochs.baseline, epochs.metadata)\n",
    "\n",
    "# Get all epochs where the 'kind' metadata field is 'word':\n",
    "word_epochs = custom_epochs['kind==word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07344132",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_epochs['kind==\"word\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch\n",
    "def mne_events(meta):\n",
    "    events = np.ones((len(meta), 3), dtype=int)\n",
    "    events[:, 0] = meta.start*raw.info['sfreq']\n",
    "    return dict(events=events, metadata=meta.reset_index())\n",
    "\n",
    "epochs = mne.Epochs(raw, **mne_events(meta), decim=20, tmin=-.2, tmax=1.5, preload=True)\n",
    "epochs = epochs['kind==\"word\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_path, get_subjects, epoch_data, epoch_runs\n",
    "from utils import (\n",
    "    decod,\n",
    "    correlate,\n",
    "    match_list,\n",
    "    create_target,\n",
    "    analysis,\n",
    "    save_decoding_results,\n",
    ")\n",
    "from plot import plot_subject\n",
    "import mne_bids\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import spacy\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from wordfreq import zipf_frequency\n",
    "from Levenshtein import editops\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "RUN = 9\n",
    "task = \"read\"\n",
    "subject = subjects[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856d515",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = epoch_runs(subject, RUN, task, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0915f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(epochs.metadata).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.load_data()\n",
    "epochs = epochs['kind==\"word\"']\n",
    "epochs[\"content_word == False\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7298b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9cc25f465bce2bd58662313d1fe29f78ce66d40d6f2798a767eb1052c037478"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
