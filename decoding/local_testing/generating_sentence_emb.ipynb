{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aebc7b4",
   "metadata": {},
   "source": [
    "# New Approach:\n",
    "\n",
    "For both:\n",
    "- Sentence embeddings\n",
    "- Constituent embeddings\n",
    "\n",
    "Generate the embedding by iterating through them, instead of generating it from the whole txt file and chunking after.\n",
    "\n",
    "In order to get the right chunking:\n",
    "- get the metadata format from a normal analysis,\n",
    "- get the frontiers of constituents / sentences from it, and generate the embeddings from there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eec524",
   "metadata": {},
   "source": [
    "## Testing metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e46dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import decod_xy, mne_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e717995e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoching for run 1, subject: 3\n",
      "\n",
      "Opening raw data file /home/is153802/data/LPP_MEG_visual/sub-3/ses-01/meg/sub-3_ses-01_task-read_run-01_meg.fif...\n",
      "    Read a total of 13 projection items:\n",
      "        grad_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v6 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v7 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v8 (1 x 306)  idle\n",
      "    Range : 36000 ... 517999 =     36.000 ...   517.999 secs\n",
      "Ready.\n",
      "Reading channel info from /home/is153802/data/LPP_MEG_visual/sub-3/ses-01/meg/sub-3_ses-01_task-read_run-01_channels.tsv.\n",
      "Using 4 HPI coils: 293 307 314 321 Hz\n",
      "Not fully anonymizing info - keeping his_id, sex, and hand info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: This file contains raw Internal Active Shielding data. It may be distorted. Elekta recommends it be run through MaxFilter to produce reliable results. Consider closing the file and running MaxFilter on the data.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: Did not find any events.tsv associated with sub-3_ses-01_task-read_run-01.\n",
      "\n",
      "The search_str was \"/home/is153802/data/LPP_MEG_visual/sub-3/**/meg/sub-3_ses-01*events.tsv\"\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:51: RuntimeWarning: The unit for channel(s) STI001, STI002, STI003, STI004, STI005, STI006, STI007, STI008, STI009, STI010, STI011, STI012, STI013, STI014, STI015, STI016, STI101, STI201, STI301 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Iterating on runs, building the metadata and re-epoching\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,runs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 37\u001b[0m     raw, meta_, events \u001b[38;5;241m=\u001b[39m read_raw(subject, run, events_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m     meta \u001b[38;5;241m=\u001b[39m meta_\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Metadata update\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/decoding/local_testing/dataset.py:63\u001b[0m, in \u001b[0;36mread_raw\u001b[0;34m(subject, run_id, events_return)\u001b[0m\n\u001b[1;32m     61\u001b[0m event_file \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_task-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbids_path\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m event_file \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_run-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbids_path\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_events.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Path(event_file)\u001b[38;5;241m.\u001b[39mexists()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# read events\u001b[39;00m\n\u001b[1;32m     66\u001b[0m meta \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(event_file, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dataset import read_raw, get_subjects, get_path\n",
    "from utils import decod_xy, mne_events\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "all_evos = []\n",
    "all_scores = []\n",
    "path = get_path(\"visual\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "runs = 2\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "levels = ('word','constituent','sentence')\n",
    "starts = ('onset', 'offset')\n",
    "      \n",
    "# Iterate on subjects to epochs, and mean later\n",
    "for subject in subjects[2:3]:\n",
    "    \n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    \n",
    "    # Initialization of the dictionary\n",
    "    for start in starts: \n",
    "            for level in levels:\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "                \n",
    "    # Iterating on runs, building the metadata and re-epoching\n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        \n",
    "        # Metadata update\n",
    "        meta['word_onset'] = True\n",
    "        meta['word_stop'] = meta.start + meta.duration\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sentence stop info\n",
    "        meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "        for s, d in meta.groupby('sentence_id'):\n",
    "            meta.loc[d.index, 'sent_word_id'] = range(len(d))\n",
    "            meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'sentence_stop'] = d.start.max()\n",
    "            \n",
    "        # Adding the constituents stop info\n",
    "        meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "        for s, d in meta.groupby('constituent_id'):\n",
    "            meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'constituent_stop'] = d.start.max()\n",
    "            meta.loc[d.index, 'const_word_id'] = range(len(d))\n",
    "\n",
    "        for start in starts: \n",
    "            for level in levels:\n",
    "                # Select only the rows containing the True for the conditions\n",
    "                # Simplified to only get for the onset: sentence onset epochs, constituent onset epochs,etc\n",
    "                sel = meta.query(f'{level}_onset==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "                # TODO check variance as well for sentences\n",
    "                # Matchlist events and meta\n",
    "                # So that we can epoch now that's we've sliced our metadata\n",
    "                i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "                sel = sel.reset_index().loc[j]\n",
    "                # Making sure there is not hidden bug when matching\n",
    "                assert sel.shape[0] > 0.8 *  (meta.query(f'{level}_onset==True')).shape[0]\n",
    "\n",
    "                # Epoching from the metadata having all onset events: if the start=Offset, the mne events\n",
    "                # Function will epoch on the offset of each level instead of the onset\n",
    "                # TODO: add adaptative baseline\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop',\n",
    "                                            preload=True,\n",
    "                                                baseline=None)\n",
    "                epoch_key = f'{level}_{start}'\n",
    "\n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "            \n",
    "    # Once we have the dict of epochs per condition full (epoching for each run for a subject)\n",
    "    # we can concatenate them, and fix the dev_head             \n",
    "    for start_ in starts: \n",
    "        for level_ in levels:\n",
    "            epoch_key = f'{level_}_{start_}'\n",
    "            all_epochs_chosen = dict_epochs[epoch_key]\n",
    "            # Concatenate epochs\n",
    "\n",
    "            for epo in all_epochs_chosen:\n",
    "                epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "            dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c6d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3074153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc45e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcd86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abc4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac3c322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c50e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Open the events files to get the metadata, and then generate the txt file from there\n",
    "for run in np.arange(1,10):\n",
    "\n",
    "    file = f'/home/co/data/LPP_MEG_auditory/sub-{sub}/ses-01/meg/sub-{sub}_ses-01_task-read_run-0{run}_events.tsv'\n",
    "\n",
    "    # Load the TSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "\n",
    "    # Keep track of the previous onset value\n",
    "    prev_onset = None\n",
    "\n",
    "    # Open the output file for writing\n",
    "    with open(f'run{run}.txt', 'w') as output_file:\n",
    "\n",
    "        # Loop through each row in the DataFrame\n",
    "        for i, row in df.iterrows():\n",
    "\n",
    "            # Get the onset value for this row\n",
    "            onset = row['onset']\n",
    "\n",
    "            # If this is the first row, or the onset difference with the previous row is less than 0.7, append the current column to the output\n",
    "            if ((row.word).__contains__(\".\")\n",
    "                or (row.word).__contains__(\"?\")\n",
    "                or (row.word).__contains__(\"!\")):\n",
    "                output_file.write(row['word'] +'\\n')\n",
    "                \n",
    "\n",
    "            # Otherwise, start a new line in the output file\n",
    "            else:\n",
    "                \n",
    "                output_file.write(row['word'] + ' ')\n",
    "\n",
    "            # Remember the onset value for the next iteration\n",
    "            prev_onset = onset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52942a",
   "metadata": {},
   "source": [
    "## LASER embeddings for sentences (easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a57c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the metadata, go from sentence end to sentence end, \n",
    "# regroup all the words each time, create a txt file from it,\n",
    "# And run LASER on it, generating an associated txt file containing the embeddings\n",
    "# finally, add these embeddings to the metadata \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d90e41",
   "metadata": {},
   "source": [
    "# Previous Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7019f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: generate the run{i}.txt file to input to LASER\n",
    "\n",
    "# What was done previously: chunk the txt file raw by actual sentence (based on ., ?, !, etc..)\n",
    "# Problem: the metadata in epochs (sentence_end calculated using the word onset difference) doesn't match, as there are\n",
    "# Offsets that happen sometimes not at the end of sentences\n",
    "\n",
    "# Solution: temporary: generate the line chunking for LASER by word onset difference from the metadata file\n",
    "# Final: it will only work for read modality: for audio, an option could be to replicate the metadata file\n",
    "# => supposing the shape of both metadata files are the same, we can add the sentence_end column to the audio one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a97b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Open the events files to get the metadata, and then generate the txt file from there\n",
    "for run in np.arange(1,10):\n",
    "\n",
    "    file = f'/home/co/data/BIDS_lecture/sub-{sub}/ses-01/meg/sub-{sub}_ses-01_task-read_run-0{run}_events.tsv'\n",
    "\n",
    "\n",
    "\n",
    "    # Load the TSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "\n",
    "    # Keep track of the previous onset value\n",
    "    prev_onset = None\n",
    "\n",
    "    # Open the output file for writing\n",
    "    with open(f'run{run}.txt', 'w') as output_file:\n",
    "\n",
    "        # Loop through each row in the DataFrame\n",
    "        for i, row in df.iterrows():\n",
    "\n",
    "            # Get the onset value for this row\n",
    "            onset = row['onset']\n",
    "\n",
    "            # If this is the first row, or the onset difference with the previous row is less than 0.7, append the current column to the output\n",
    "            if ((row.word).__contains__(\".\")\n",
    "                or (row.word).__contains__(\"?\")\n",
    "                or (row.word).__contains__(\"!\")):\n",
    "                output_file.write(row['word'] +'\\n')\n",
    "                \n",
    "\n",
    "            # Otherwise, start a new line in the output file\n",
    "            else:\n",
    "                \n",
    "                output_file.write(row['word'] + ' ')\n",
    "\n",
    "            # Remember the onset value for the next iteration\n",
    "            prev_onset = onset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620a449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "path = Path('/home/is153802/github/LASER/tasks/embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c561d791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LASER=/home/is153802/github/LASER\n"
     ]
    }
   ],
   "source": [
    "%env LASER=/home/is153802/github/LASER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aedc33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-24 22:09:01,159 | INFO | embed | spm_model: /home/is153802/github/LASER/laser2.spm\n",
      "2023-03-24 22:09:01,159 | INFO | embed | spm_cvocab: /home/is153802/github/LASER/laser2.cvocab\n",
      "2023-03-24 22:09:01,159 | INFO | embed | loading encoder: /home/is153802/github/LASER/laser2.pt\n",
      "2023-03-24 22:09:01,705 | INFO | preprocess | SPM processing run1.txt  \n",
      "2023-03-24 22:09:01,828 | INFO | embed | encoding /tmp/tmpild7c4y1/spm to /home/is153802/code/data/laser_embeddings/emb_1-3.bin\n",
      "2023-03-24 22:09:02,481 | INFO | embed | encoded 134 sentences in 0s\n",
      "2023-03-24 22:09:04,769 | INFO | embed | spm_model: /home/is153802/github/LASER/laser2.spm\n",
      "2023-03-24 22:09:04,769 | INFO | embed | spm_cvocab: /home/is153802/github/LASER/laser2.cvocab\n",
      "2023-03-24 22:09:04,769 | INFO | embed | loading encoder: /home/is153802/github/LASER/laser2.pt\n",
      "2023-03-24 22:09:05,307 | INFO | preprocess | SPM processing run2.txt  \n",
      "2023-03-24 22:09:05,425 | INFO | embed | encoding /tmp/tmp_nei1fhw/spm to /home/is153802/code/data/laser_embeddings/emb_4-6.bin\n",
      "2023-03-24 22:09:06,141 | INFO | embed | encoded 136 sentences in 0s\n",
      "2023-03-24 22:09:08,366 | INFO | embed | spm_model: /home/is153802/github/LASER/laser2.spm\n",
      "2023-03-24 22:09:08,366 | INFO | embed | spm_cvocab: /home/is153802/github/LASER/laser2.cvocab\n",
      "2023-03-24 22:09:08,366 | INFO | embed | loading encoder: /home/is153802/github/LASER/laser2.pt\n",
      "2023-03-24 22:09:08,913 | INFO | preprocess | SPM processing run3.txt  \n",
      "2023-03-24 22:09:09,033 | INFO | embed | encoding /tmp/tmpzj3s4nx4/spm to /home/is153802/code/data/laser_embeddings/emb_7-9.bin\n",
      "2023-03-24 22:09:10,062 | INFO | embed | encoded 183 sentences in 1s\n",
      "2023-03-24 22:09:12,334 | INFO | embed | spm_model: /home/is153802/github/LASER/laser2.spm\n",
      "2023-03-24 22:09:12,334 | INFO | embed | spm_cvocab: /home/is153802/github/LASER/laser2.cvocab\n",
      "2023-03-24 22:09:12,334 | INFO | embed | loading encoder: /home/is153802/github/LASER/laser2.pt\n",
      "2023-03-24 22:09:12,880 | INFO | preprocess | SPM processing run4.txt  \n",
      "2023-03-24 22:09:12,975 | INFO | embed | encoding /tmp/tmpvarbms0y/spm to /home/is153802/code/data/laser_embeddings/emb_10-12.bin\n",
      "2023-03-24 22:09:13,719 | INFO | embed | encoded 174 sentences in 0s\n",
      "2023-03-24 22:09:15,988 | INFO | embed | spm_model: /home/is153802/github/LASER/laser2.spm\n",
      "2023-03-24 22:09:15,988 | INFO | embed | spm_cvocab: /home/is153802/github/LASER/laser2.cvocab\n",
      "2023-03-24 22:09:15,988 | INFO | embed | loading encoder: /home/is153802/github/LASER/laser2.pt\n",
      "2023-03-24 22:09:16,527 | INFO | preprocess | SPM processing run5.txt  \n",
      "2023-03-24 22:09:16,644 | INFO | embed | encoding /tmp/tmpd8jhxakn/spm to /home/is153802/code/data/laser_embeddings/emb_13-14.bin\n",
      "2023-03-24 22:09:17,258 | INFO | embed | encoded 177 sentences in 0s\n",
      "2023-03-24 22:09:19,511 | INFO | embed | spm_model: /home/is153802/github/LASER/laser2.spm\n",
      "2023-03-24 22:09:19,511 | INFO | embed | spm_cvocab: /home/is153802/github/LASER/laser2.cvocab\n",
      "2023-03-24 22:09:19,511 | INFO | embed | loading encoder: /home/is153802/github/LASER/laser2.pt\n",
      "2023-03-24 22:09:20,056 | INFO | preprocess | SPM processing run6.txt  \n",
      "2023-03-24 22:09:20,159 | INFO | embed | encoding /tmp/tmpfhxfnh46/spm to /home/is153802/code/data/laser_embeddings/emb_15-19.bin\n",
      "2023-03-24 22:09:20,875 | INFO | embed | encoded 210 sentences in 0s\n",
      "2023-03-24 22:09:23,158 | INFO | embed | spm_model: /home/is153802/github/LASER/laser2.spm\n",
      "2023-03-24 22:09:23,159 | INFO | embed | spm_cvocab: /home/is153802/github/LASER/laser2.cvocab\n",
      "2023-03-24 22:09:23,159 | INFO | embed | loading encoder: /home/is153802/github/LASER/laser2.pt\n",
      "2023-03-24 22:09:23,702 | INFO | preprocess | SPM processing run7.txt  \n",
      "2023-03-24 22:09:23,821 | INFO | embed | encoding /tmp/tmptupnjqxt/spm to /home/is153802/code/data/laser_embeddings/emb_20-22.bin\n",
      "2023-03-24 22:09:24,453 | INFO | embed | encoded 192 sentences in 0s\n",
      "2023-03-24 22:09:26,745 | INFO | embed | spm_model: /home/is153802/github/LASER/laser2.spm\n",
      "2023-03-24 22:09:26,745 | INFO | embed | spm_cvocab: /home/is153802/github/LASER/laser2.cvocab\n",
      "2023-03-24 22:09:26,745 | INFO | embed | loading encoder: /home/is153802/github/LASER/laser2.pt\n",
      "2023-03-24 22:09:27,310 | INFO | preprocess | SPM processing run8.txt  \n",
      "2023-03-24 22:09:27,428 | INFO | embed | encoding /tmp/tmp0id7fc1n/spm to /home/is153802/code/data/laser_embeddings/emb_23-25.bin\n",
      "2023-03-24 22:09:28,072 | INFO | embed | encoded 142 sentences in 0s\n",
      "2023-03-24 22:09:30,356 | INFO | embed | spm_model: /home/is153802/github/LASER/laser2.spm\n",
      "2023-03-24 22:09:30,356 | INFO | embed | spm_cvocab: /home/is153802/github/LASER/laser2.cvocab\n",
      "2023-03-24 22:09:30,356 | INFO | embed | loading encoder: /home/is153802/github/LASER/laser2.pt\n",
      "2023-03-24 22:09:30,897 | INFO | preprocess | SPM processing run9.txt  \n",
      "2023-03-24 22:09:31,013 | INFO | embed | encoding /tmp/tmpmv4efnm1/spm to /home/is153802/code/data/laser_embeddings/emb_26-27.bin\n",
      "2023-03-24 22:09:31,658 | INFO | embed | encoded 196 sentences in 0s\n"
     ]
    }
   ],
   "source": [
    "CHAPTERS = {\n",
    "        1: \"1-3\",\n",
    "        2: \"4-6\",\n",
    "        3: \"7-9\",\n",
    "        4: \"10-12\",\n",
    "        5: \"13-14\",\n",
    "        6: \"15-19\",\n",
    "        7: \"20-22\",\n",
    "        8: \"23-25\",\n",
    "        9: \"26-27\",\n",
    "    }\n",
    "\n",
    "for run in np.arange(1,10):\n",
    "    ch = CHAPTERS[run]\n",
    "    txt_file = f\"/home/is153802/code/data/txt_laser/run{run}.txt\"\n",
    "    emb_file = f\"/home/is153802/code/data/laser_embeddings/emb_{ch}.bin\"\n",
    "    !bash /home/is153802/github/LASER/tasks/embed/embed.sh {txt_file} {emb_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3719b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
