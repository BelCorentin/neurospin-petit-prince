{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550c03f6",
   "metadata": {},
   "source": [
    "# Word Const Sent decoding \n",
    "\n",
    "Todo: \n",
    "\n",
    "Integrate this:\n",
    "\n",
    "```sent_starts = epochs['word_id==0'].apply_baseline((-.300, 0.))\n",
    "sent_starts.average().plot()\n",
    "\n",
    "sent_stops = epochs['is_last_word']\n",
    "bsl = (epochs.times>-.300 )*(epochs.times<=0)\n",
    "baseline_starts = sent_starts.get_data()[:, :, bsl].mean(-2)\n",
    "\n",
    "sent_stop_data = sent_stops.get_data()\n",
    "n_sentences, n_channels, n_times = sent_stop_data.shape\n",
    "sent_stop_data -= baseline_starts[:, :, None]```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b8d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import read_raw, get_subjects, get_path, add_embeddings\n",
    "from utils import decod_xy, mne_events\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "\n",
    "modality = \"auditory\"\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "all_evos = []\n",
    "all_scores = []\n",
    "path = get_path(modality)\n",
    "subjects = get_subjects(path)\n",
    "runs = 2\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "levels = ('word','constituent','sentence')\n",
    "starts = ('onset', 'offset')\n",
    "      \n",
    "# Iterate on subjects to epochs, and mean later\n",
    "for subject in subjects[2:3]:\n",
    "    \n",
    "    dict_epochs = dict() # DICT containing epochs grouped by conditions (start x level)\n",
    "    \n",
    "    # Initialization of the dictionary\n",
    "    for start in starts: \n",
    "            for level in levels:\n",
    "                epoch_key = f'{level}_{start}'\n",
    "                dict_epochs[epoch_key] = [] \n",
    "                \n",
    "    # Iterating on runs, building the metadata and re-epoching\n",
    "    for run in range(1,runs+1):\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True, modality=modality)\n",
    "        meta = meta_.copy()\n",
    "        \n",
    "        # Metadata update\n",
    "        meta['word_onset'] = True\n",
    "        meta['word_stop'] = meta.start + meta.duration\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sentence stop info\n",
    "        meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "        for s, d in meta.groupby('sentence_id'):\n",
    "            meta.loc[d.index, 'sent_word_id'] = range(len(d))\n",
    "            meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'sentence_stop'] = d.start.max()\n",
    "            \n",
    "        # Adding the constituents stop info\n",
    "        meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "        for s, d in meta.groupby('constituent_id'):\n",
    "            meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "            meta.loc[d.index, 'constituent_stop'] = d.start.max()\n",
    "            meta.loc[d.index, 'const_word_id'] = range(len(d))\n",
    "        \n",
    "        # Adding embeddings info\n",
    "        meta = add_embeddings(meta, run, 'constituent')\n",
    "        meta = add_embeddings(meta, run, 'sentence')\n",
    "        for start in starts: \n",
    "            for level in levels:\n",
    "                # Select only the rows containing the True for the conditions\n",
    "                # Simplified to only get for the onset: sentence onset epochs, constituent onset epochs,etc\n",
    "                start = 'onset' # DEBUG\n",
    "                sel = meta.query(f'{level}_{start}==True')\n",
    "                assert sel.shape[0] > 10  #\n",
    "                \n",
    "                # Do we need to do that ???\n",
    "                \"\"\"\n",
    "                # Matchlist events and meta\n",
    "                # So that we can epoch now that's we've sliced our metadata\n",
    "                if modality == 'auditory':\n",
    "                    word_events = events[events[:, 2] > 1]\n",
    "                    meg_delta = np.round(np.diff(word_events[:, 0]/raw.info['sfreq']))\n",
    "                    meta_delta = np.round(np.diff(sel.onset.values))\n",
    "                    i, j = match_list(meg_delta, meta_delta)\n",
    "\n",
    "                # For auditory, we match on the time difference between triggers\n",
    "                elif modality == \"visual\":\n",
    "\n",
    "                    i, j = match_list(events[:, 2], sel.wlength)\n",
    "                    assert len(i) > (0.9 * len(events))\n",
    "                    assert (events[i, 2] == sel.loc[j].wlength).mean() > 0.95\n",
    "                sel = sel.reset_index().loc[j]\n",
    "                # Making sure there is not hidden bug when matching\n",
    "                assert sel.shape[0] > 0.5 *  (meta.query(f'{level}_onset==True')).shape[0]\n",
    "                \"\"\"\n",
    "                \n",
    "                # Epoching from the metadata having all onset events: if the start=Offset, the mne events\n",
    "                # Function will epoch on the offset of each level instead of the onset\n",
    "                # TODO: add adaptative baseline\n",
    "                epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
    "                                     tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                                       tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                         event_repeated = 'drop',\n",
    "                                            preload=True,\n",
    "                                                baseline=None)\n",
    "                epoch_key = f'{level}_{start}'\n",
    "\n",
    "                dict_epochs[epoch_key].append(epochs)\n",
    "            \n",
    "    # Once we have the dict of epochs per condition full (epoching for each run for a subject)\n",
    "    # we can concatenate them, and fix the dev_head             \n",
    "    for start_ in starts: \n",
    "        for level_ in levels:\n",
    "            start_ = 'onset' # DEBUG\n",
    "            epoch_key = f'{level_}_{start_}'\n",
    "            all_epochs_chosen = dict_epochs[epoch_key]\n",
    "            # Concatenate epochs\n",
    "\n",
    "            for epo in all_epochs_chosen:\n",
    "                epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "            dict_epochs[epoch_key] = mne.concatenate_epochs(all_epochs_chosen)\n",
    "            \n",
    "    # All epochs -> Decoding and generate evoked potentials\n",
    "    for start in starts: \n",
    "        for level in levels:\n",
    "            epoch_key = f'{level}_{start}'\n",
    "            epochs = dict_epochs[epoch_key]\n",
    "            # mean\n",
    "            evo = epochs.copy().pick_types(meg=True).average(method='median')\n",
    "            all_evos.append(dict(subject=subject, evo=evo, start=start, level=level))\n",
    "\n",
    "\n",
    "            # decoding word emb\n",
    "            epochs = epochs.load_data().pick_types(meg=True, stim=False, misc=False)\n",
    "            X = epochs.get_data()\n",
    "            embeddings = epochs.metadata[f'embeds_{level}']\n",
    "            #embeddings = epochs.metadata.word.apply(lambda word: nlp(word).vector).values\n",
    "            #embeddings = np.array([emb for emb in embeddings])\n",
    "            R_vec = decod_xy(X, embeddings)\n",
    "            scores = np.mean(R_vec, axis=1)\n",
    "\n",
    "            for t, score in enumerate(scores):\n",
    "                all_scores.append(dict(subject=subject, score=score, start=start, level=level, t=epochs.times[t]))\n",
    "\n",
    "all_scores = pd.DataFrame(all_scores)\n",
    "all_evos = pd.DataFrame(all_evos)\n",
    "\n",
    "all_scores.to_csv('./score.csv')\n",
    "all_evos.to_csv('./evos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = pd.DataFrame(all_scores)\n",
    "all_evos = pd.DataFrame(all_evos)\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(16, 10), dpi=80)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2)\n",
    "\n",
    "for axes_, level in zip( axes, levels):  \n",
    "    for ax, start in zip( axes_, starts):  \n",
    "        cond1 = all_scores.level==f'{level}'\n",
    "        cond2 = all_scores.start==f'{start}'\n",
    "        data = all_scores[ cond1 & cond2]\n",
    "        y = []\n",
    "        x = []\n",
    "        for s, t in data.groupby('t'):\n",
    "            score_avg = t.score.mean()\n",
    "            y.append(score_avg)\n",
    "            x.append(s)\n",
    "\n",
    "        ax.plot(x,y)\n",
    "        ax.set_title(f'{level} {start}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
