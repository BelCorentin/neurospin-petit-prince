"""
The role of this script is to:

1)
Transform the raw data organization generated by the MEG, 
that does not follow the same rules:
ex: some files are named sub_blocX_raw.fif where some others are sub_rX_raw.fif

The point will be to format everything under a stable and normalized format:
BIDS which will simplify postprocessing.

2)
Run the MNE_BIDS_PIPELINE in order to run a Maxwell filter on the raw data:
the raw data generated by Elekta needs to have this step done before going
any further.

3)
We'll need to add the text annotation / word events in 
the different subject folders for other analysis.
The reference is the fMRI events file from:
https://openneuro.org/datasets/ds003643/versions/1.0.4

"""

###### Imports ######

from __future__ import annotations
import pandas as pd
import os 
import re 
import mne
from pathlib import Path
from mne_bids import BIDSPath, write_raw_bids  

##### 1) Raw format to BIDS #####

###  CONST ###
BASE_PATH = Path('/home/is153802/workspace_LPP/data/MEG/LPP/')
BIDS_PATH = BASE_PATH / 'LPP_bids'
RAW_DATA_PATH = BASE_PATH / 'raw'
PROC_DATA_PATH = BASE_PATH / 'derivatives' / 'final_all'
TASK = 'listen'

""" 

Each folder / fif file name format is different, so depending on its format, we need
different processes to extract the run, subject, etc..

Different types: 
SA_bloc82_raw.fif
sf_r1_raw.fif
rt220104_partie9.fif
bloc2_raw.fif

The objective is to get all the unnormalized file names to follow the same pattern:
in this case, we'll choose the most common: the sub_r{run_number}_raw.fif format

Once it's done, it is possible to iterate on the file names to create the corresponding
BIDS Datasets

In the next acquisitions, we'll make sure to respect this normalized format:
rX_raw.fif
"""

# For each of these folders, go into the sub folder (that has the name of a subject)
for folder in RAW_DATA_PATH.iterdir():
    sub_dir = RAW_DATA_PATH / folder
    # for sub in sub_dir.iterdir():
    for sub in os.listdir(sub_dir):

        # Get the list of runs
        run_dir = sub_dir / sub
        for file in os.listdir(run_dir):
        # for file in run_dir.iterdir():
            # Use a regular expression to get the run number in the file name
            # assert file.name.startswith('_r')
            # assert file.name.endswith('_raw')
            file = str(file)
            try:
                run = re.search(r"_r([^']*)_raw.fif", file).group(1)
            # Two cases: filenames is sub_r{run_number}_raw or sub_run{run_number}_raw
            # so it's we are in the second case, ignore the first re and keep the 2nd result
                if len(run) > 2:
                    run = re.search(r"_run([^']*)_raw.fif", file).group(1)
            except:
                print(f"No run found for file: {file}")
                continue

            # Check if the BIDS dataset already exists:
            sub = str(sub)
            fname = f"sub-{sub}/ses-01/meg/sub-{sub}_ses-01_task-{TASK}_run-0{run}_meg.fif"
            if (BIDS_PATH / fname).exists():
                print(f"The file {fname} already exists: not created again.")
                continue
            # Open the raw file
            raw = mne.io.read_raw_fif(run_dir /file, allow_maxshield=True)

            # Create a BIDS path with the correct parameters 
            bids_path = BIDSPath(subject=sub, session='01', run='0'+str(run), datatype='meg', root=BIDS_PATH)
            bids_path.task = TASK

            # Write the BIDS path from the raw file
            write_raw_bids(raw, bids_path=bids_path, overwrite=True)


### Do the same for the empty room:

if not (BASE_PATH / 'sub_emptyroom/' / 'ses-20220628').exists():
    raw_er = '/home/is153802/workspace_LPP/data/MEG/LPP/empty_room/220628/empty_raw.fif'
    er_raw = mne.io.read_raw_fif(raw_er,allow_maxshield = True)

    er_raw.info['line_freq'] = 60
    er_date = er_raw.info['meas_date'].strftime('%Y%m%d')
    print(er_date)

    er_bids_path = BIDSPath(subject='emptyroom', session=er_date,
                            task='noise', root=BIDS_PATH)
    write_raw_bids(er_raw, er_bids_path, overwrite=True)

##### 2) Running the Maxwell filter #####

#output = os.popen('python ./mne_preproc/run.py --config=./mne_preproc/custom_config_LPP.py --steps=preprocessing/1').read()
#print(f'\n Output of BIDS pipeline is: {output}\n')

##### 3) Add the events to the newly created derivates/final_all folder that is preprocessed #####


# Transforming the csv file from the fMRI LPP experiment into events file to be fed to the BIDS format


## Formatting to be read by the MEG-MASC script

df = pd.read_csv('./annotations/annotation FR lppFR_word_information.csv')

df_ = pd.DataFrame(df['onset'])

df_['duration'] = df['offset'] - df['onset']

df_['trial_type'] = [{"kind":"word",'word':df.loc[i,'word'].replace("'","")} for i in range(df_.shape[0])]

df_.to_csv('./annotations/annotation_processed.tsv',sep='\t',index=False)

## Segmenting these annotations into different files for different runs

df = pd.read_csv('./annotations/annotation_processed.tsv',sep='\t')

df1 = df.iloc[0:1632,:]
df1.to_csv('./annotations/annotation_processed1.tsv',sep='\t',index=False)

# print(df1)
df2 = df.iloc[1632:3419,:]
df2.to_csv('./annotations/annotation_processed2.tsv',sep='\t',index=False)

# print(df2)
df3 = df.iloc[3419:5295,:]
df3.to_csv('./annotations/annotation_processed3.tsv',sep='\t',index=False)

# print(df3)
df4 = df.iloc[5295:6945,:]
df4.to_csv('./annotations/annotation_processed4.tsv',sep='\t',index=False)

# print(df4)
df5 = df.iloc[6945:8472,:]
df5.to_csv('./annotations/annotation_processed5.tsv',sep='\t',index=False)

# print(df5)
df6 = df.iloc[8472:10330,:]
df6.to_csv('./annotations/annotation_processed6.tsv',sep='\t',index=False)

# print(df6)
df7 = df.iloc[10330:12042,:]
df7.to_csv('./annotations/annotation_processed7.tsv',sep='\t',index=False)

# print(df7)
df8 = df.iloc[12042:13581,:]
df8.to_csv('./annotations/annotation_processed8.tsv',sep='\t',index=False)

# print(df8)
df9 = df.iloc[13581:15391,:]
df9.to_csv('./annotations/annotation_processed9.tsv',sep='\t',index=False)


## Putting the generated annotation files (one for each run) in the correct directories: the processed one and the regular one

path_raw = '~/workspace_LPP/data/MEG/LPP/LPP_bids'
path_proc = '/home/is153802/workspace_LPP/data/MEG/LPP/derivatives/final_all'

for sub in os.listdir(path_proc):
    if sub.__contains__('sub-'):
        SUBJ_PATH = PROC_DATA_PATH / f'{sub}/ses-01/meg'
        files = os.listdir(SUBJ_PATH)
        print(f'files : {files}')
        for file in files:

            try:
                run = re.search(r"_run-0([^']*)_proc-filt_raw.fif", file).group(1)
                print("File for which an events one will be created: "+file)
            except:
                continue
            annot = f'~/workspace_LPP/code/neurospin-petit-prince/bids_formatting/annotations/annotation_processed{run}.tsv'
            df = pd.read_csv(annot,sep='\t')
            df.to_csv(f'{SUBJ_PATH}/{sub}_ses-01_task-{TASK}_run-0{run}_events.tsv',sep='\t')
            print(f"File created:  + {sub}_ses-01_task-{TASK}_run-0{run}_events.tsv")
            

print(f"\n \n ***************************************************\
\n Script finished!\n \
***************************************************\
\n Folder created: \n For bids: {BIDS_PATH} \n For Preproc: {path_proc}")


#### TODO 

# Add the files like participants.tsv & co







