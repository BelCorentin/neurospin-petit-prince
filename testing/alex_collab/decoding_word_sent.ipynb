{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44909105",
   "metadata": {},
   "source": [
    "# Attempt to decode word end / sentence end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6a4c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mne_events(meta, raw, start, level):\n",
    "    if start=='onset':\n",
    "        events = np.ones((len(meta), 3), dtype=int)\n",
    "        events[:, 0] = meta.start * raw.info[\"sfreq\"]\n",
    "        return dict(events=events, metadata=meta.reset_index())\n",
    "    elif start=='offset':\n",
    "        \"\"\"\n",
    "        It should generalize, no need for different cases?\n",
    "        if level == 'word':\n",
    "            events = np.ones((len(meta), 3), dtype=int)\n",
    "            events[:, 0] = (meta.start+meta.duration) * raw.info[\"sfreq\"]\n",
    "            return dict(events=events, metadata=meta.reset_index())\n",
    "        elif level == 'sentence':\n",
    "            events = np.ones((len(meta), 3), dtype=int)\n",
    "            events[:, 0] = (meta.start+meta.duration) * raw.info[\"sfreq\"]\n",
    "            return dict(events=events, metadata=meta.reset_index())\n",
    "\n",
    "        else:\n",
    "            print('hi')\n",
    "            # Fill\n",
    "        \"\"\"\n",
    "        events = np.ones((len(meta), 3), dtype=int)\n",
    "        events[:, 0] = (meta.start+meta.duration) * raw.info[\"sfreq\"]\n",
    "        return dict(events=events, metadata=meta.reset_index())\n",
    "        \n",
    "    else:\n",
    "        print('start should be either onset or offset')\n",
    "        return 0\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b60479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoching for run 1, subject: 3\n",
      "\n",
      "Opening raw data file /home/is153802/data/LPP_MEG_visual/sub-3/ses-01/meg/sub-3_ses-01_task-read_run-01_meg.fif...\n",
      "    Read a total of 13 projection items:\n",
      "        grad_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v6 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v7 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v8 (1 x 306)  idle\n",
      "    Range : 36000 ... 517999 =     36.000 ...   517.999 secs\n",
      "Ready.\n",
      "Reading events from /home/is153802/data/LPP_MEG_visual/sub-3/ses-01/meg/sub-3_ses-01_task-read_run-01_events.tsv.\n",
      "Reading channel info from /home/is153802/data/LPP_MEG_visual/sub-3/ses-01/meg/sub-3_ses-01_task-read_run-01_channels.tsv.\n",
      "Using 4 HPI coils: 293 307 314 321 Hz\n",
      "Not fully anonymizing info - keeping his_id, sex, and hand info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/testing/alex_collab/dataset.py:51: RuntimeWarning: This file contains raw Internal Active Shielding data. It may be distorted. Elekta recommends it be run through MaxFilter to produce reliable results. Consider closing the file and running MaxFilter on the data.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/testing/alex_collab/dataset.py:51: RuntimeWarning: Omitted 81 annotation(s) that were outside data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/testing/alex_collab/dataset.py:51: RuntimeWarning: The unit for channel(s) STI001, STI002, STI003, STI004, STI005, STI006, STI007, STI008, STI009, STI010, STI011, STI012, STI013, STI014, STI015, STI016, STI101, STI201, STI301 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigger channel has a non-zero initial value of 8 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "1466 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "Reading 0 ... 481999  =      0.000 ...   481.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 20 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 20.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 306 out of 306 | elapsed:    6.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Adding metadata with 23 columns\n",
      "1405 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1405 events and 1301 original time points (prior to decimation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73460/3989308597.py:71: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
      "/tmp/ipykernel_73460/3989308597.py:71: RuntimeWarning: The measurement information indicates a low-pass frequency of 20.0 Hz. The decim=100 parameter will result in a sampling frequency of 10.0 Hz, which can cause aliasing artifacts.\n",
      "  epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 bad epochs dropped\n",
      "\n",
      " Epoching for run 2, subject: 3\n",
      "\n",
      "Opening raw data file /home/is153802/data/LPP_MEG_visual/sub-3/ses-01/meg/sub-3_ses-01_task-read_run-02_meg.fif...\n",
      "    Read a total of 13 projection items:\n",
      "        grad_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        grad_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v1 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v2 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v3 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v4 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v5 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v6 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v7 (1 x 306)  idle\n",
      "        mag_ssp_upright.fif : PCA-v8 (1 x 306)  idle\n",
      "    Range : 14000 ... 532999 =     14.000 ...   532.999 secs\n",
      "Ready.\n",
      "Reading events from /home/is153802/data/LPP_MEG_visual/sub-3/ses-01/meg/sub-3_ses-01_task-read_run-02_events.tsv.\n",
      "Reading channel info from /home/is153802/data/LPP_MEG_visual/sub-3/ses-01/meg/sub-3_ses-01_task-read_run-02_channels.tsv.\n",
      "Using 4 HPI coils: 293 307 314 321 Hz\n",
      "Not fully anonymizing info - keeping his_id, sex, and hand info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/testing/alex_collab/dataset.py:51: RuntimeWarning: This file contains raw Internal Active Shielding data. It may be distorted. Elekta recommends it be run through MaxFilter to produce reliable results. Consider closing the file and running MaxFilter on the data.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/testing/alex_collab/dataset.py:51: RuntimeWarning: Omitted 99 annotation(s) that were outside data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/testing/alex_collab/dataset.py:51: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n",
      "/mnt/localdrive/workspace-LPP/code/neurospin-petit-prince/testing/alex_collab/dataset.py:51: RuntimeWarning: The unit for channel(s) STI001, STI002, STI003, STI004, STI005, STI006, STI007, STI008, STI009, STI010, STI011, STI012, STI013, STI014, STI015, STI016, STI101, STI201, STI301 has changed from V to NA.\n",
      "  raw = mne_bids.read_raw_bids(bids_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1607 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n",
      "Reading 0 ... 518999  =      0.000 ...   518.999 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 20 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 20.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz)\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "from dataset import read_raw, get_subjects, get_path\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 9\n",
    "\n",
    "level = 'word' # Sentence or word\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "start = 'offset'\n",
    "\n",
    "subject = subjects[2]\n",
    "all_epochs = []\n",
    "for run in range(1,runs+1):\n",
    "\n",
    "    # Set the metadata for this case\n",
    "    raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "    meta = meta_.copy()\n",
    "    # Metadata update\n",
    "    # Word start\n",
    "    meta['word_onset'] = True\n",
    "    meta['word_stop'] = meta.start + meta.duration\n",
    "\n",
    "    # Sent start\n",
    "    meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "    # Const start\n",
    "    meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "    meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "    meta['constituent_onset'].fillna(False, inplace=True)\n",
    "    meta.drop('prev_closing', axis=1, inplace=True)\n",
    "\n",
    "    # Adding the sentence stop info\n",
    "    meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "    for s, d in meta.groupby('sentence_id'):\n",
    "        meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "\n",
    "    # Adding the constituents stop info\n",
    "    meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "    for s, d in meta.groupby('constituent_id'):\n",
    "        meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "\n",
    "\n",
    "\n",
    "    # Select either the sentence end to decode, or the word end\n",
    "    sel = meta.query(f'{level}_onset==True')\n",
    "    assert sel.shape[0] > 10  #\n",
    "    # TODO check variance as well for sentences\n",
    "    # Matchlist events and meta\n",
    "    # So that we can epoch now that's we've sliced our metadata\n",
    "    i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "    sel = sel.reset_index().loc[j]\n",
    "    # Making sure there is not hidden bug when matching\n",
    "    assert sel.shape[0] > 0.8 *  (meta.query(f'{level}_onset==True')).shape[0]\n",
    "\n",
    "    # Epoching from the metadata having all onset events: if the start=Offset, the mne events\n",
    "    # Function will epoch on the offset of each level instead of the onset\n",
    "    # TODO: add adaptative baseline\n",
    "    epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
    "                         tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                           tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                             event_repeated = 'drop', # check event repeated\n",
    "                                preload=True,\n",
    "                                    baseline=None)  # n_words OR n_constitutent OR n_sentences\n",
    "    epoch_key = f'{level}_{start}'\n",
    "\n",
    "    all_epochs.append(epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7abc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epo in all_epochs:\n",
    "    epo.info[\"dev_head_t\"] = all_epochs[1].info[\"dev_head_t\"]\n",
    "\n",
    "all_epochs = mne.concatenate_epochs(all_epochs)\n",
    "\n",
    "# Decode\n",
    "epochs = epochs.load_data().pick_types(meg=True, stim=False, misc=False)\n",
    "X = epochs.get_data()\n",
    "y = epochs.metadata.word\n",
    "\"\"\"\n",
    "R_vec = decod_xy(X, embeddings)\n",
    "scores = np.mean(R_vec, axis=1)\n",
    "\n",
    "for t, score in enumerate(scores):\n",
    "    all_scores.append(dict(subject=subject, score=score, start=start, level=level, t=epochs.times[t]))\n",
    "\n",
    "all_scores = pd.DataFrame(all_scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "# Decode function for the sole purpose of word end decoding\n",
    "\n",
    "def correlate(X, Y):\n",
    "    if X.ndim == 1:\n",
    "        X = np.array(X)[:, None]\n",
    "    if Y.ndim == 1:\n",
    "        Y = np.array(Y)[:, None]\n",
    "    X = X - X.mean(0)\n",
    "    Y = Y - Y.mean(0)\n",
    "\n",
    "    SX2 = (X**2).sum(0) ** 0.5\n",
    "    SY2 = (Y**2).sum(0) ** 0.5\n",
    "    SXY = (X * Y).sum(0)\n",
    "    return SXY / (SX2 * SY2)\n",
    "\n",
    "\n",
    "def decod_xy_word_end(X, y):\n",
    "    assert len(X) == len(y)\n",
    "    # define data\n",
    "    model = make_pipeline(StandardScaler(), RidgeCV(alphas=np.logspace(-3, 8, 10)))\n",
    "    cv = KFold(5, shuffle=True, random_state=0)\n",
    "\n",
    "    # fit predict\n",
    "    n, n_chans = X.shape\n",
    "    if y.ndim == 1:\n",
    "        y = np.asarray(y).reshape(y.shape[0], 1)\n",
    "    R = np.zeros((y.shape[1]))\n",
    "\n",
    "    print(\".\", end=\"\")\n",
    "    rs = []\n",
    "    scores = []\n",
    "\n",
    "        # y_pred = cross_val_predict(model, X[:, :, t], y, cv=cv)\n",
    "    for train, test in cv.split(X):\n",
    "        model.fit(X[train, :], y[train])\n",
    "        y_pred = model.predict(X[test, :])\n",
    "        r = correlate(y[test], y_pred)\n",
    "        score = model.score(X[test, :], y[test])\n",
    "        rs.append(r)\n",
    "        scores.append(score)\n",
    "    score = np.mean(scores)\n",
    "    R = np.mean(rs)\n",
    "        # R[t] = correlate(y, y_pred)\n",
    "\n",
    "    return R, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "X = X.reshape((-1,306))\n",
    "# The 4th timepoint is the timestamp of the end of the word\n",
    "y = np.zeros(A.shape[0])\n",
    "y[3::14] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fbb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_vec, score = decod_xy_word_end(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd4466",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303c708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
