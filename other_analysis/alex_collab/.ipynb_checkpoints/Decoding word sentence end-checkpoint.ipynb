{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44909105",
   "metadata": {},
   "source": [
    "# Attempt to decode word end / sentence end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de064747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import read_raw, get_subjects, get_path\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 3\n",
    "\n",
    "level = 'word' # Sentence or word\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "start = 'onset'\n",
    "\n",
    "for subject in subjects[2:3]:\n",
    "    all_epochs = []\n",
    "    for run in range(1,runs+1):\n",
    "        \n",
    "        # Set the metadata for this case\n",
    "        raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "        meta = meta_.copy()\n",
    "        # Metadata update\n",
    "        # Word start\n",
    "        meta['word_onset'] = True\n",
    "        meta['word_stop'] = meta.start + meta.duration\n",
    "\n",
    "        # Sent start\n",
    "        meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "        # Const start\n",
    "        meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "        meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "        meta['constituent_onset'].fillna(False, inplace=True)\n",
    "        meta.drop('prev_closing', axis=1, inplace=True)\n",
    "        \n",
    "        # Adding the sentence stop info\n",
    "        meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "        for s, d in meta.groupby('sentence_id'):\n",
    "            meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "            \n",
    "        # Adding the constituents stop info\n",
    "        meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "        for s, d in meta.groupby('constituent_id'):\n",
    "            meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Select either the sentence end to decode, or the word end\n",
    "        sel = meta.query(f'{level}_onset==True')\n",
    "        assert sel.shape[0] > 10  #\n",
    "        # TODO check variance as well for sentences\n",
    "        # Matchlist events and meta\n",
    "        # So that we can epoch now that's we've sliced our metadata\n",
    "        i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "        sel = sel.reset_index().loc[j]\n",
    "        # Making sure there is not hidden bug when matching\n",
    "        assert sel.shape[0] > 0.8 *  (meta.query(f'{level}_onset==True')).shape[0]\n",
    "\n",
    "        # Epoching from the metadata having all onset events: if the start=Offset, the mne events\n",
    "        # Function will epoch on the offset of each level instead of the onset\n",
    "        # TODO: add adaptative baseline\n",
    "        epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
    "                             tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                               tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                                 event_repeated = 'drop', # check event repeated\n",
    "                                    preload=True,\n",
    "                                        baseline=None)  # n_words OR n_constitutent OR n_sentences\n",
    "        epoch_key = f'{level}_{start}'\n",
    "\n",
    "        all_epochs.append(epochs)\n",
    "        \n",
    "    for epo in all_epochs:\n",
    "        epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "        all_epochs = mne.concatenate_epochs(all_epochs)\n",
    "        \n",
    "    # Decode\n",
    "    epochs = epochs.load_data().pick_types(meg=True, stim=False, misc=False)\n",
    "    X = epochs.get_data()\n",
    "    y = epochs.metadata.\n",
    "    R_vec = decod_xy(X, embeddings)\n",
    "    scores = np.mean(R_vec, axis=1)\n",
    "\n",
    "    for t, score in enumerate(scores):\n",
    "        all_scores.append(dict(subject=subject, score=score, start=start, level=level, t=epochs.times[t]))\n",
    "\n",
    "    all_scores = pd.DataFrame(all_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b60479",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/data_path.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m match_list\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLPP_read\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m subjects \u001b[38;5;241m=\u001b[39m get_subjects(path)\n\u001b[1;32m     13\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/git/neurospin-petit-prince/testing/alex_collab/dataset.py:197\u001b[0m, in \u001b[0;36mget_path\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_path\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLPP_read\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    196\u001b[0m     path_file \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./../../data/data_path.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    198\u001b[0m         data \u001b[38;5;241m=\u001b[39m Path(f\u001b[38;5;241m.\u001b[39mreadlines()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLPP_read\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;66;03m# TASK = \"read\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/data_path.txt'"
     ]
    }
   ],
   "source": [
    "from dataset import read_raw, get_subjects, get_path\n",
    "from utils import decod_xy\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import match_list\n",
    "import spacy\n",
    "\n",
    "path = get_path(\"LPP_read\")\n",
    "subjects = get_subjects(path)\n",
    "task = \"read\"\n",
    "# Debug\n",
    "runs = 3\n",
    "\n",
    "level = 'word' # Sentence or word\n",
    "\n",
    "epoch_windows = {\"word\": {\"onset_min\": -0.3, \"onset_max\": 1.0, \"offset_min\": -1.0, \"offset_max\": 0.3},\n",
    "                  \"constituent\": {\"offset_min\": -2.0, \"offset_max\": 0.5, \"onset_min\": -0.5, \"onset_max\": 2.0},\n",
    "                  \"sentence\": {\"offset_min\": -4.0, \"offset_max\": 1.0, \"onset_min\": -1.0, \"onset_max\": 4.0}}\n",
    "start = 'onset'\n",
    "\n",
    "subject = subjects[2]\n",
    "all_epochs = []\n",
    "for run in range(1,runs+1):\n",
    "\n",
    "    # Set the metadata for this case\n",
    "    raw, meta_, events = read_raw(subject, run, events_return = True)\n",
    "    meta = meta_.copy()\n",
    "    # Metadata update\n",
    "    # Word start\n",
    "    meta['word_onset'] = True\n",
    "    meta['word_stop'] = meta.start + meta.duration\n",
    "\n",
    "    # Sent start\n",
    "    meta['sentence_onset'] = meta.word_id == 0\n",
    "\n",
    "    # Const start\n",
    "    meta['prev_closing'] = meta['n_closing'].shift(1)\n",
    "    meta['constituent_onset'] = meta.apply(lambda x: x['prev_closing'] > x['n_closing'] and x['n_closing'] == 1, axis=1)\n",
    "    meta['constituent_onset'].fillna(False, inplace=True)\n",
    "    meta.drop('prev_closing', axis=1, inplace=True)\n",
    "\n",
    "    # Adding the sentence stop info\n",
    "    meta['sentence_id'] = np.cumsum(meta.sentence_onset)\n",
    "    for s, d in meta.groupby('sentence_id'):\n",
    "        meta.loc[d.index, 'sentence_start'] = d.start.min()\n",
    "\n",
    "    # Adding the constituents stop info\n",
    "    meta['constituent_id'] = np.cumsum(meta.constituent_onset)\n",
    "    for s, d in meta.groupby('constituent_id'):\n",
    "        meta.loc[d.index, 'constituent_start'] = d.start.min()\n",
    "\n",
    "\n",
    "\n",
    "    # Select either the sentence end to decode, or the word end\n",
    "    sel = meta.query(f'{level}_onset==True')\n",
    "    assert sel.shape[0] > 10  #\n",
    "    # TODO check variance as well for sentences\n",
    "    # Matchlist events and meta\n",
    "    # So that we can epoch now that's we've sliced our metadata\n",
    "    i, j = match_list(events[:, 2], sel.word.apply(len))\n",
    "    sel = sel.reset_index().loc[j]\n",
    "    # Making sure there is not hidden bug when matching\n",
    "    assert sel.shape[0] > 0.8 *  (meta.query(f'{level}_onset==True')).shape[0]\n",
    "\n",
    "    # Epoching from the metadata having all onset events: if the start=Offset, the mne events\n",
    "    # Function will epoch on the offset of each level instead of the onset\n",
    "    # TODO: add adaptative baseline\n",
    "    epochs = mne.Epochs(raw, **mne_events(sel, raw ,start=start, level=level), decim = 100,\n",
    "                         tmin = epoch_windows[f'{level}'][f'{start}_min'],\n",
    "                           tmax = epoch_windows[f'{level}'][f'{start}_max'],\n",
    "                             event_repeated = 'drop', # check event repeated\n",
    "                                preload=True,\n",
    "                                    baseline=None)  # n_words OR n_constitutent OR n_sentences\n",
    "    epoch_key = f'{level}_{start}'\n",
    "\n",
    "    all_epochs.append(epochs)\n",
    "\n",
    "for epo in all_epochs:\n",
    "    epo.info[\"dev_head_t\"] = all_epochs_chosen[1].info[\"dev_head_t\"]\n",
    "\n",
    "    all_epochs = mne.concatenate_epochs(all_epochs)\n",
    "\n",
    "# Decode\n",
    "epochs = epochs.load_data().pick_types(meg=True, stim=False, misc=False)\n",
    "X = epochs.get_data()\n",
    "y = epochs.metadata.word\n",
    "\"\"\"\n",
    "R_vec = decod_xy(X, embeddings)\n",
    "scores = np.mean(R_vec, axis=1)\n",
    "\n",
    "for t, score in enumerate(scores):\n",
    "    all_scores.append(dict(subject=subject, score=score, start=start, level=level, t=epochs.times[t]))\n",
    "\n",
    "all_scores = pd.DataFrame(all_scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43dd22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
