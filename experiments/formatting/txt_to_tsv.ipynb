{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194613b6",
   "metadata": {},
   "source": [
    "# Formatting the LPP txt files\n",
    "\n",
    "In this notebook, we'll go from the raw LPP txt files, to a word-based csv file, doing the following steps:\n",
    "- Tokenizing the natural language by words\n",
    "- Remove the blank space between a word and :\n",
    "- Adding capital letters at the beginning of a sentence\n",
    "- Remove the blank space between - and the following word (dialogue) \n",
    "\n",
    "Different versions will be built:\n",
    "\n",
    "- A first one with 300 ms words + 50 ms black screen. End of sentence delay of 200 ms\n",
    "\n",
    "- A second one with 250 ms words + 50 ms black screen. End of sentence delay of 500 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b09fbd",
   "metadata": {},
   "source": [
    "### Bash commands preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880e75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For :\n",
    "!perl -pi.bak -e 's/ :/:/g' *.txt\n",
    "\n",
    "# For dash\n",
    "!perl -pi.bak -e 's/- /-/g' *.txt\n",
    "\n",
    "# Word tokenizing\n",
    "!for f in `seq 1 9` ; do sed 's/ /\\n/g' text_french_run$f.txt | awk 'length($0) > 0 ' > new_test_run$f.txt; done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43dd9b",
   "metadata": {},
   "source": [
    "### Python commands tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b56b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb761f",
   "metadata": {},
   "source": [
    "## First version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3bdd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_screen = 0.05\n",
    "word_duration = 0.30\n",
    "word_bs = black_screen + word_duration\n",
    "\n",
    "for i in np.arange(1,10):\n",
    "    with open(f'./text_lpp/new_test_run{i}.txt') as temp_file:\n",
    "\n",
    "        lpp = temp_file.read().splitlines() \n",
    "\n",
    "\n",
    "    df = pd.DataFrame(lpp)\n",
    "    next_cap = False\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # First word\n",
    "        if index == 0:\n",
    "            df.at[index,0] = str(row.str.capitalize()[0])\n",
    "        if next_cap == True:\n",
    "            df.at[index,0] = str(row.str.capitalize()[0])\n",
    "        if str(row).__contains__('.') or str(row).__contains__('?') or str(row).__contains__('!'):\n",
    "            next_cap = True\n",
    "        else:\n",
    "            next_cap = False\n",
    "\n",
    "    df.columns = ['word']\n",
    "    end = (df.shape[0] * word_bs) + 0.7\n",
    "    df['onset'] = np.arange(0.7, end, word_bs)\n",
    "    df['duration'] = np.ones(df.shape[0]) * word_duration\n",
    "    \n",
    "    df.to_csv(f'./txt_clean/run{i}_clean.tsv', sep='\\t', index=False)\n",
    "    \n",
    "    \n",
    "    # Create a dataframe where the duration of the black screen after the end of the sentence is longer.\n",
    "\n",
    "    df_sentence_end = pd.DataFrame(columns = df.columns, data = copy.deepcopy(df.values))\n",
    "    end_of_sentence_delay = 0.2\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if str(row.word).__contains__('.') or str(row.word).__contains__('?') or str(row.word).__contains__('!'):\n",
    "            # df_sentence_end.at[index, 'onset'] = row.onset + end_of_sentence_delay # Add the delay from this line\n",
    "            # And for every next onset\n",
    "            for j in np.arange(index+1, df.shape[0]):\n",
    "                df_sentence_end.at[j, 'onset'] = df_sentence_end.at[j, 'onset'] + end_of_sentence_delay\n",
    "                ww = df_sentence_end.at[j, 'word']\n",
    "\n",
    "    \n",
    "    df_sentence_end.to_csv(f'./v1/run{i}_v1_word_0.3_end_sentence_0.2.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92562f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create for decoding adding a dict\n",
    "for i in np.arange(1,10):\n",
    "    df_clean = pd.read_csv(f'./txt_clean/run{i}_clean.tsv',sep='\\t')\n",
    "    df_clean['trial_type'] = [{} for i in np.arange(df_clean.shape[0])]\n",
    "    for index, row in df_clean.iterrows():\n",
    "        clean_word = str(row.word).translate(str.maketrans('', '', string.punctuation))\n",
    "        dict_word =  {'kind':'word','word':clean_word}\n",
    "        df_clean.at[index, 'trial_type'] = dict_word\n",
    "    df_clean.to_csv(f'./decoding_tsv_v1/run{i}_v1.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf25101",
   "metadata": {},
   "source": [
    "## Second version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc5e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_screen = 0.05\n",
    "word_duration = 0.25\n",
    "end_of_sentence_delay = 0.5\n",
    "word_bs = black_screen + word_duration\n",
    "end_of_chapter_duration = 2\n",
    "\n",
    "dict_end_chapter = {\n",
    "    1:[433,1087],\n",
    "    2:[737,1400],\n",
    "    3:[710,1345],\n",
    "    4:[1090,1357],\n",
    "    5:[753],\n",
    "    6:[716,951,1271,1555],\n",
    "    7:[200,1278],\n",
    "    8:[95,703],\n",
    "    9:[1357],\n",
    "}\n",
    "\n",
    "for i in np.arange(1,10):\n",
    "    with open(f'./text_lpp/new_test_run{i}.txt') as temp_file:\n",
    "\n",
    "        lpp = temp_file.read().splitlines() \n",
    "\n",
    "\n",
    "    df = pd.DataFrame(lpp)\n",
    "    next_cap = False\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # First word\n",
    "        if index == 0:\n",
    "            df.at[index,0] = str(row.str.capitalize()[0])\n",
    "        if next_cap == True:\n",
    "            df.at[index,0] = str(row.str.capitalize()[0])\n",
    "        if str(row).__contains__('.') or str(row).__contains__('?') or str(row).__contains__('!'):\n",
    "            next_cap = True\n",
    "        else:\n",
    "            next_cap = False\n",
    "\n",
    "    df.columns = ['word']\n",
    "    end = (df.shape[0] * word_bs) + 0.7\n",
    "    df['onset'] = np.arange(0.7, end, word_bs)\n",
    "    df['duration'] = np.ones(df.shape[0]) * word_duration\n",
    "    \n",
    "    \n",
    "    \n",
    "    df.to_csv(f'./txt_clean/run{i}_clean.tsv', sep='\\t', index=False)\n",
    "    \n",
    "    \n",
    "    # Create a dataframe where the duration of the black screen after the end of the sentence is longer.\n",
    "\n",
    "    df_sentence_end = pd.DataFrame(columns = df.columns, data = copy.deepcopy(df.values))\n",
    "    for index, row in df.iterrows():\n",
    "        if str(row.word).__contains__('.') or str(row.word).__contains__('?') or str(row.word).__contains__('!'):\n",
    "            # df_sentence_end.at[index, 'onset'] = row.onset + end_of_sentence_delay # Add the delay from this line\n",
    "            # And for every next onset\n",
    "            for j in np.arange(index+1, df.shape[0]):\n",
    "                df_sentence_end.at[j, 'onset'] = df_sentence_end.at[j, 'onset'] + end_of_sentence_delay\n",
    "                ww = df_sentence_end.at[j, 'word']\n",
    "        if index+2 in dict_end_chapter[i]:\n",
    "            print(f'Adding 2s after the word {row.word} \\n')\n",
    "            for j in np.arange(index+1, df.shape[0]):\n",
    "                df_sentence_end.at[j, 'onset'] = df_sentence_end.at[j, 'onset'] + end_of_chapter_duration\n",
    "                ww = df_sentence_end.at[j, 'word']\n",
    "\n",
    "    \n",
    "    df_sentence_end.to_csv(f'./v2/run{i}_v2_0.25_0.5.tsv',sep='\\t',index=False)\n",
    "    \n",
    "# Create for decoding adding a dict\n",
    "for i in np.arange(1,10):\n",
    "    df_clean = pd.read_csv(f'./v2/run{i}_v2_0.25_0.5.tsv',sep='\\t')\n",
    "    df_clean['trial_type'] = [{} for i in np.arange(df_clean.shape[0])]\n",
    "    for index, row in df_clean.iterrows():\n",
    "        clean_word = str(row.word).translate(str.maketrans('', '', string.punctuation))\n",
    "        dict_word =  {'kind':'word','word':clean_word}\n",
    "        df_clean.at[index, 'trial_type'] = dict_word\n",
    "    df_clean.to_csv(f'./decoding_tsv_v2/run{i}_v2.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03c05cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2s after the word raisonnable... \n",
      "\n",
      "Adding 2s after the word prince. \n",
      "\n",
      "Adding 2s after the word vieillir. \n",
      "\n",
      "Adding 2s after the word l'urgence. \n",
      "\n",
      "Adding 2s after the word larmes! \n",
      "\n",
      "Adding 2s after the word l'aimer.\" \n",
      "\n",
      "Adding 2s after the word voyage. \n",
      "\n",
      "Adding 2s after the word voyage. \n",
      "\n",
      "Adding 2s after the word voyage. \n",
      "\n",
      "Adding 2s after the word fleur. \n",
      "\n",
      "Adding 2s after the word an. \n",
      "\n",
      "Adding 2s after the word turent. \n",
      "\n",
      "Adding 2s after the word fleur. \n",
      "\n",
      "Adding 2s after the word pleura. \n",
      "\n",
      "Adding 2s after the word souvenir. \n",
      "\n",
      "Adding 2s after the word fontaine...\" \n",
      "\n",
      "Adding 2s after the word jour. \n",
      "\n",
      "Adding 2s after the word sable. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "black_screen = 0.05\n",
    "word_duration = 0.25\n",
    "end_of_sentence_delay = 0.5\n",
    "word_bs = black_screen + word_duration\n",
    "end_of_chapter_duration = 2\n",
    "\n",
    "dict_end_chapter = {\n",
    "    1:[433,1087],\n",
    "    2:[737,1400],\n",
    "    3:[710,1345],\n",
    "    4:[1090,1357],\n",
    "    5:[753],\n",
    "    6:[716,951,1271,1555],\n",
    "    7:[200,1278],\n",
    "    8:[95,703],\n",
    "    9:[1357]\n",
    "}\n",
    "\n",
    "for i in np.arange(1,10):\n",
    "    with open(f'./text_lpp/new_test_run{i}.txt') as temp_file:\n",
    "\n",
    "        lpp = temp_file.read().splitlines() \n",
    "\n",
    "\n",
    "    df = pd.DataFrame(lpp)\n",
    "    next_cap = False\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # First word\n",
    "        if index == 0:\n",
    "            df.at[index,0] = str(row.str.capitalize()[0])\n",
    "        if next_cap == True:\n",
    "            df.at[index,0] = str(row.str.capitalize()[0])\n",
    "        if str(row).__contains__('.') or str(row).__contains__('?') or str(row).__contains__('!'):\n",
    "            next_cap = True\n",
    "        else:\n",
    "            next_cap = False\n",
    "\n",
    "    df.columns = ['word']\n",
    "    end = (df.shape[0] * word_bs) + 0.7\n",
    "    df['onset'] = np.arange(0.7, end, word_bs)\n",
    "    df['duration'] = np.ones(df.shape[0]) * word_duration\n",
    "    \n",
    "    df_sentence_end = pd.DataFrame(columns = df.columns, data = copy.deepcopy(df.values))\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "                \n",
    "        if index+2 in dict_end_chapter[i]:\n",
    "            print(f'Adding 2s after the word {row.word} \\n')\n",
    "            for j in np.arange(index+1, df.shape[0]):\n",
    "                df_sentence_end.at[j, 'onset'] = df_sentence_end.at[j, 'onset'] + end_of_chapter_duration\n",
    "                ww = df_sentence_end.at[j, 'word']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6db9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f35ebef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[433, 1087]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_end_chapter[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f99e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
