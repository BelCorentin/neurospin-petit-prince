{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praat-textgrids\n",
      "  Downloading praat_textgrids-1.4.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: praat-textgrids\n",
      "Successfully installed praat-textgrids-1.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# If needed \n",
    "# !pip install praat-textgrids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textgrids\n",
    "import mne\n",
    "import mne_bids\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0fbfe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset is : 0.9228799999999997\n",
      "Offset is : -0.012119999999999909\n",
      "Offset is : 0.027880000000000127\n",
      "Offset is : 0.0714399999999995\n",
      "Offset is : 0.00788000000000011\n",
      "Offset is : 0.017879999999999896\n",
      "Offset is : 0.0028800000000002157\n",
      "Offset is : 0.04287999999999981\n",
      "Offset is : 0.06288000000000027\n"
     ]
    }
   ],
   "source": [
    "# For each of the 9 run:\n",
    "\n",
    "for run in np.arange(1,10):\n",
    "\n",
    "    # Open the text grid\n",
    "    grid = textgrids.TextGrid(f'./stimuli/grid/text_french_run{run}.TextGrid')\n",
    "\n",
    "    dict_ts_to_phoneme = {} # Dict linking timestamp to phoneme\n",
    "\n",
    "    # Get the list of phonemes, their timestamp, and create a dict based off it\n",
    "    for ph in grid['MAU']:\n",
    "        dict_ts_to_phoneme[ph.mid] = ph.text\n",
    "\n",
    "    # Explore the TextGrid file, and add the voiced information\n",
    "    # Now that all the text grids are generated, it is needed to add to each Text Grid the \n",
    "    # Following information: is it a voiced or a non-voiced phoneme?\n",
    "\n",
    "    # Voiced phonemes are the following:\n",
    "    voiced = ['Z','z','v','R','D','d','b']\n",
    "\n",
    "    # All the other phonemecs are then non-voiced\n",
    "\n",
    "    dict_ts_to_voiced = {}\n",
    "\n",
    "    for key in dict_ts_to_phoneme:\n",
    "        if dict_ts_to_phoneme[key] in voiced:\n",
    "            dict_ts_to_voiced[key] = 'voiced'\n",
    "        else:\n",
    "            dict_ts_to_voiced[key] = 'non-voiced'\n",
    "\n",
    "    # Deleting the first phoneme as it is an error:\n",
    "    dict_ts_to_voiced.pop(list(dict_ts_to_voiced.keys())[0])\n",
    "\n",
    "\n",
    "    # Now load the MEG recording, and add this information as annotations\n",
    "    TASK = 'listen'\n",
    "\n",
    "    subject = '1'\n",
    "\n",
    "    # path_bids = '/home/co/workspace_LPP/data/MEG/LPP/LPP_bids'\n",
    "    path_bids = '/home/is153802/data/BIDS_final/'\n",
    "\n",
    "\n",
    "    bids_path = mne_bids.BIDSPath(\n",
    "                subject=subject,\n",
    "                session='01',\n",
    "                task=TASK,\n",
    "                datatype=\"meg\",\n",
    "                root=path_bids,\n",
    "                run='0'+str(run),\n",
    "            )\n",
    "\n",
    "    # raw = mne_bids.read_raw_bids(bids_path)\n",
    "\n",
    "    events_path = str(bids_path.fpath)[:-7] + 'events.tsv'\n",
    "\n",
    "    events = pd.read_csv(events_path,delimiter = '\\t')[['onset','duration','trial_type']]\n",
    "\n",
    "    # Now we have to:\n",
    "\n",
    "    # Align the phonemes timestamps with the current onset of words as in the events.tsv file\n",
    "\n",
    "    # Add them as a kind: phoneme\n",
    "\n",
    "    # There is an offset between the first word and the first phoneme: let's reajust\n",
    "\n",
    "    offset = events.onset[0] - list(dict_ts_to_voiced.keys())[0]\n",
    "\n",
    "    print(f'Offset is : {offset}')\n",
    "\n",
    "    for timestamp in dict_ts_to_voiced.keys():\n",
    "        timestamp_final = timestamp + offset \n",
    "        to_add = pd.DataFrame([timestamp_final, 0.02, {'kind':'phoneme','voice':dict_ts_to_voiced[timestamp]}])\n",
    "        to_add_t = to_add.transpose()\n",
    "        to_add_t.columns = events.columns\n",
    "        events = pd.concat([events, to_add_t])\n",
    "    \n",
    "    events.to_csv(str(bids_path.fpath)[:-7] + 'events.tsv', sep='\\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in np.arange(2,20):\n",
    "    # For each of the 9 run:\n",
    "\n",
    "    for run in np.arange(1,10):\n",
    "\n",
    "        # Open the text grid\n",
    "        grid = textgrids.TextGrid(f'./stimuli/grid/text_french_run{run}.TextGrid')\n",
    "\n",
    "        dict_ts_to_phoneme = {} # Dict linking timestamp to phoneme\n",
    "\n",
    "        # Get the list of phonemes, their timestamp, and create a dict based off it\n",
    "        for ph in grid['MAU']:\n",
    "            dict_ts_to_phoneme[ph.mid] = ph.text\n",
    "\n",
    "        # Explore the TextGrid file, and add the voiced information\n",
    "        # Now that all the text grids are generated, it is needed to add to each Text Grid the \n",
    "        # Following information: is it a voiced or a non-voiced phoneme?\n",
    "\n",
    "        # Voiced phonemes are the following:\n",
    "        voiced = ['Z','z','v','R','D','d','b']\n",
    "\n",
    "        # All the other phonemecs are then non-voiced\n",
    "\n",
    "        dict_ts_to_voiced = {}\n",
    "\n",
    "        for key in dict_ts_to_phoneme:\n",
    "            if dict_ts_to_phoneme[key] in voiced:\n",
    "                dict_ts_to_voiced[key] = 'voiced'\n",
    "            else:\n",
    "                dict_ts_to_voiced[key] = 'non-voiced'\n",
    "\n",
    "        # Deleting the first phoneme as it is an error:\n",
    "        dict_ts_to_voiced.pop(list(dict_ts_to_voiced.keys())[0])\n",
    "\n",
    "\n",
    "        # Now load the MEG recording, and add this information as annotations\n",
    "        TASK = 'listen'\n",
    "\n",
    "        subject = str(sub)\n",
    "\n",
    "        # path_bids = '/home/co/workspace_LPP/data/MEG/LPP/LPP_bids'\n",
    "        path_bids = '/home/is153802/data/BIDS_final/'\n",
    "\n",
    "\n",
    "        bids_path = mne_bids.BIDSPath(\n",
    "                    subject=subject,\n",
    "                    session='01',\n",
    "                    task=TASK,\n",
    "                    datatype=\"meg\",\n",
    "                    root=path_bids,\n",
    "                    run='0'+str(run),\n",
    "                )\n",
    "\n",
    "        # raw = mne_bids.read_raw_bids(bids_path)\n",
    "\n",
    "        events_path = str(bids_path.fpath)[:-7] + 'events.tsv'\n",
    "\n",
    "        events = pd.read_csv(events_path,delimiter = '\\t')[['onset','duration','trial_type']]\n",
    "\n",
    "        # Now we have to:\n",
    "\n",
    "        # Align the phonemes timestamps with the current onset of words as in the events.tsv file\n",
    "\n",
    "        # Add them as a kind: phoneme\n",
    "\n",
    "        # There is an offset between the first word and the first phoneme: let's reajust\n",
    "\n",
    "        offset = events.onset[0] - list(dict_ts_to_voiced.keys())[0]\n",
    "\n",
    "        print(f'Offset is : {offset}')\n",
    "\n",
    "        for timestamp in dict_ts_to_voiced.keys():\n",
    "            timestamp_final = timestamp + offset \n",
    "            to_add = pd.DataFrame([timestamp_final, 0.02, {'kind':'phoneme','voice':dict_ts_to_voiced[timestamp]}])\n",
    "            to_add_t = to_add.transpose()\n",
    "            to_add_t.columns = events.columns\n",
    "            events = pd.concat([events, to_add_t])\n",
    "        \n",
    "        events.to_csv(str(bids_path.fpath)[:-7] + 'events.tsv', sep='\\t',index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('meg-masc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1541b2a7ce6718fb850c7e31854a9840c1a445a51449da7ae9469c30a41f28d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
